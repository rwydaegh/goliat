{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GOLIAT Project Documentation","text":"<p>Welcome to the documentation for the GOLIAT project.</p>"},{"location":"COLORING_RULES/","title":"Coloring Rules for Terminal Output","text":"<p>This document outlines the rules for colorizing terminal output using the <code>colorama</code> library. The goal is to improve readability and draw the user's attention to the most critical information. All colors are defined in the <code>COLOR_MAP</code> dictionary in <code>src/colors.py</code> to ensure consistency and ease of maintenance.</p>"},{"location":"COLORING_RULES/#how-to-use","title":"How to Use","text":"<p>To apply a color, use the <code>_log</code> method from the <code>LoggingMixin</code> and specify the <code>log_type</code>.</p> <pre><code># Example usage:\nself._log(\"This is a warning message.\", log_type='warning')\nself._log(\"File saved successfully.\", level='progress', log_type='success')\n</code></pre> <p>Important: When adding a <code>log_type</code>, do not change the existing <code>level</code> parameter (e.g., <code>level='progress'</code>). The <code>level</code> controls which log file the message goes to, while <code>log_type</code> only controls the terminal color.</p>"},{"location":"COLORING_RULES/#color-to-type-mapping","title":"Color-to-Type Mapping","text":"<p>This table defines the intended use for each <code>log_type</code> and its corresponding color.</p> <code>log_type</code> Color Description &amp; Use Cases Example <code>default</code> White Standard, neutral output. Used for messages that don't fit any other category. <code>Running full simulation setup...</code> <code>header</code> Bright Magenta For major section headers that announce the start of a significant phase of the study. <code>--- Starting Far-Field Study: My Study ---</code> <code>progress</code> Green High-level progress updates that indicate a specific, positive step forward in the process. <code>--- Processing Frequency 1/5: 700MHz ---</code> <code>success</code> Bright Green Indicates the successful completion of a major operation or the entire study. <code>--- Study Finished ---</code> or <code>All required packages are already installed.</code> <code>info</code> Cyan Important, non-critical information that provides context, such as file paths or key configuration settings. <code>Project path set to: D:\\...</code> or <code>Solver kernel set to: Acceleware</code> <code>highlight</code> Bright Yellow Used to draw attention to a specific value or result within a block of text, such as a key performance metric. <code>Final Balance: 99.87%</code> <code>warning</code> Yellow For non-critical issues or potential problems that the user should be aware of, but that don't stop the process. <code>WARNING: Could not extract power balance.</code> or <code>GetPower() not available, falling back to manual extraction.</code> <code>error</code> Red For recoverable errors or failures within a specific part of the process. The overall study may continue. <code>ERROR: An error occurred during placement 'by_cheek': ...</code> <code>fatal</code> Magenta For critical, non-recoverable errors that will terminate the study. <code>FATAL ERROR: Could not find simulation bounding box.</code> <code>verbose</code> Blue Detailed, low-level debugging information intended for the <code>verbose</code> log stream. Not typically for progress updates. <code>- Activating line profiler for subtask: setup_simulation</code> <p>By following these rules, we can create a more intuitive and effective user experience.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page provides comprehensive API documentation for all modules in the GOLIAT simulation framework.</p>"},{"location":"api/#src","title":"<code>src</code>","text":""},{"location":"api/#src.analysis","title":"<code>analysis</code>","text":""},{"location":"api/#src.analysis.analyzer","title":"<code>analyzer</code>","text":""},{"location":"api/#src.analysis.analyzer.Analyzer","title":"<code>Analyzer</code>","text":"<p>Analyzes the results of simulation studies using a strategy pattern.</p> Source code in <code>src\\analysis\\analyzer.py</code> <pre><code>class Analyzer:\n    \"\"\"\n    Analyzes the results of simulation studies using a strategy pattern.\n    \"\"\"\n    def __init__(self, config: Config, phantom_name: str, strategy: BaseAnalysisStrategy):\n        self.config = config\n        self.base_dir = config.base_dir\n        self.phantom_name = phantom_name\n        self.strategy = strategy\n        self.results_base_dir = self.strategy.get_results_base_dir()\n        self.plotter = Plotter(self.strategy.get_plots_dir())\n        self.all_results = []\n        self.all_organ_results = []\n        self.tissue_group_definitions = {\n            \"eyes_group\": [\"eye\", \"cornea\", \"sclera\", \"lens\", \"vitreous\"],\n            \"skin_group\": [\"skin\"],\n            \"brain_group\": [\"brain\", \"commissura\", \"midbrain\", \"pineal\", \"hypophysis\", \"medulla\", \"pons\", \"thalamus\", \"hippocampus\", \"cerebellum\"]\n        }\n\n    def run_analysis(self):\n        \"\"\"\n        Main method to run the full analysis pipeline using the selected strategy.\n        \"\"\"\n        logging.getLogger('progress').info(f\"--- Starting Results Analysis for Phantom: {self.phantom_name} ---\", extra={'log_type': 'header'})\n        self.strategy.load_and_process_results(self)\n\n        if not self.all_results:\n            logging.getLogger('progress').info(\"--- No results found to analyze. ---\", extra={'log_type': 'warning'})\n            return\n\n        results_df = pd.DataFrame(self.all_results)\n        all_organ_results_df = pd.DataFrame(self.all_organ_results)\n\n        results_df = self._convert_units_and_cache(results_df, all_organ_results_df)\n        self._export_reports(results_df, all_organ_results_df)\n        self.strategy.generate_plots(self, self.plotter, results_df, all_organ_results_df)\n\n        logging.getLogger('progress').info(\"--- Analysis Finished ---\", extra={'log_type': 'success'})\n\n    def _process_single_result(self, frequency_mhz, scenario_name, pos_name, orient_name):\n        if self.strategy.__class__.__name__ == 'FarFieldAnalysisStrategy':\n            # For far-field, pos_name is the full placement directory name\n            detailed_placement_name = pos_name\n        else:\n            detailed_placement_name = f\"{scenario_name}_{pos_name}_{orient_name}\"\n\n        results_dir = os.path.join(self.results_base_dir, f\"{frequency_mhz}MHz\", detailed_placement_name)\n        pickle_path = os.path.join(results_dir, \"sar_stats_all_tissues.pkl\")\n        json_path = os.path.join(results_dir, \"sar_results.json\")\n\n        if not (os.path.exists(pickle_path) and os.path.exists(json_path)):\n            return\n\n        logging.getLogger('progress').info(f\"  - Processing: {frequency_mhz}MHz, {detailed_placement_name}\", extra={'log_type': 'progress'})\n        try:\n            with open(pickle_path, 'rb') as f:\n                pickle_data = pickle.load(f)\n            with open(json_path, 'r') as f:\n                sar_results = json.load(f)\n\n            simulated_power_w = sar_results.get('input_power_W', float('nan'))\n            normalization_factor = self.strategy.get_normalization_factor(frequency_mhz, simulated_power_w)\n            result_entry, organ_entries = self.strategy.extract_data(\n                pickle_data, frequency_mhz, detailed_placement_name, scenario_name,\n                simulated_power_w, normalization_factor\n            )\n            result_entry = self.strategy.apply_bug_fixes(result_entry)\n            self.all_results.append(result_entry)\n            self.all_organ_results.extend(organ_entries)\n        except Exception as e:\n            logging.getLogger('progress').error(f\"    - ERROR: Could not process data for {detailed_placement_name} at {frequency_mhz}MHz: {e}\", extra={'log_type': 'error'})\n\n    def _convert_units_and_cache(self, results_df, organ_results_df):\n        \"\"\"Converts SAR units to mW/kg and caches both summary and organ-level results.\"\"\"\n        sar_columns = [col for col in results_df.columns if 'SAR' in col]\n        results_df[sar_columns] = results_df[sar_columns] * 1000\n\n        output_pickle_path = os.path.join(self.results_base_dir, \"aggregated_results.pkl\")\n        os.makedirs(os.path.dirname(output_pickle_path), exist_ok=True)\n\n        cached_data = {\n            'summary_results': results_df,\n            'organ_results': organ_results_df\n        }\n        with open(output_pickle_path, 'wb') as f:\n            pickle.dump(cached_data, f)\n\n        logging.getLogger('progress').info(f\"\\n--- Aggregated summary and organ results (in mW/kg) cached to: {output_pickle_path} ---\", extra={'log_type': 'success'})\n        return results_df\n\n    def _export_reports(self, results_df, all_organ_results_df):\n        results_for_export = results_df.drop(columns=['input_power_w', 'scenario'])\n        logging.getLogger('progress').info(\"\\n--- Full Normalized Results per Simulation (in mW/kg) ---\", extra={'log_type': 'header'})\n        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n            logging.getLogger('progress').info(results_for_export.sort_values(by=['frequency_mhz', 'placement']))\n        summary_stats = self.strategy.calculate_summary_stats(results_df)\n        logging.getLogger('progress').info(\"\\n--- Summary Statistics (Mean) of Normalized SAR per Scenario and Frequency (in mW/kg) ---\", extra={'log_type': 'header'})\n        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n            logging.getLogger('progress').info(summary_stats)\n        detailed_csv_path = os.path.join(self.results_base_dir, 'normalized_results_detailed.csv')\n        summary_csv_path = os.path.join(self.results_base_dir, 'normalized_results_summary.csv')\n        organ_csv_path = os.path.join(self.results_base_dir, 'normalized_results_organs.csv')\n        results_for_export.to_csv(detailed_csv_path, index=False)\n        summary_stats.to_csv(summary_csv_path)\n        all_organ_results_df.to_csv(organ_csv_path, index=False)\n        logging.getLogger('progress').info(f\"\\n--- Detailed results saved to: {detailed_csv_path} ---\", extra={'log_type': 'success'})\n        logging.getLogger('progress').info(f\"--- Summary statistics saved to: {summary_csv_path} ---\", extra={'log_type': 'success'})\n        logging.getLogger('progress').info(f\"--- Organ-level results saved to: {organ_csv_path} ---\", extra={'log_type': 'success'})\n\n    def _generate_plots(self, results_df, all_organ_results_df):\n        \"\"\"Generates all plots using the Plotter class.\"\"\"\n        # This method is now delegated to the strategy\n        self.strategy.generate_plots(self.plotter, results_df, all_organ_results_df)\n</code></pre>"},{"location":"api/#src.analysis.analyzer.Analyzer.run_analysis","title":"<code>run_analysis()</code>","text":"<p>Main method to run the full analysis pipeline using the selected strategy.</p> Source code in <code>src\\analysis\\analyzer.py</code> <pre><code>def run_analysis(self):\n    \"\"\"\n    Main method to run the full analysis pipeline using the selected strategy.\n    \"\"\"\n    logging.getLogger('progress').info(f\"--- Starting Results Analysis for Phantom: {self.phantom_name} ---\", extra={'log_type': 'header'})\n    self.strategy.load_and_process_results(self)\n\n    if not self.all_results:\n        logging.getLogger('progress').info(\"--- No results found to analyze. ---\", extra={'log_type': 'warning'})\n        return\n\n    results_df = pd.DataFrame(self.all_results)\n    all_organ_results_df = pd.DataFrame(self.all_organ_results)\n\n    results_df = self._convert_units_and_cache(results_df, all_organ_results_df)\n    self._export_reports(results_df, all_organ_results_df)\n    self.strategy.generate_plots(self, self.plotter, results_df, all_organ_results_df)\n\n    logging.getLogger('progress').info(\"--- Analysis Finished ---\", extra={'log_type': 'success'})\n</code></pre>"},{"location":"api/#src.analysis.plotter","title":"<code>plotter</code>","text":""},{"location":"api/#src.analysis.plotter.Plotter","title":"<code>Plotter</code>","text":"Source code in <code>src\\analysis\\plotter.py</code> <pre><code>class Plotter:\n    def __init__(self, plots_dir):\n        self.plots_dir = plots_dir\n        os.makedirs(self.plots_dir, exist_ok=True)\n        logging.getLogger('progress').info(f\"--- Plots will be saved to '{self.plots_dir}' directory. ---\", extra={'log_type': 'info'})\n\n    def plot_average_sar_bar(self, scenario_name, avg_results, progress_info):\n        fig, ax = plt.subplots(figsize=(12, 7))\n        avg_results[['SAR_head', 'SAR_trunk']].plot(kind='bar', ax=ax, colormap='viridis')\n        progress_labels = [f\"{freq} MHz\\n({progress_info.get(freq, '0/0')})\" for freq in avg_results.index]\n        ax.set_xticklabels(progress_labels, rotation=0)\n        ax.set_title(f'Average Normalized SAR for Scenario: {scenario_name}')\n        ax.set_xlabel('Frequency (MHz) and Completion Progress')\n        ax.set_ylabel('Normalized SAR (mW/kg)')\n        ax.legend(['Head SAR', 'Trunk SAR'])\n        plt.tight_layout()\n        fig.savefig(os.path.join(self.plots_dir, f'average_sar_bar_{scenario_name}.png'))\n        plt.close(fig)\n\n    def plot_whole_body_sar_bar(self, avg_results):\n        fig, ax = plt.subplots(figsize=(12, 7))\n        avg_results['SAR_whole_body'].plot(kind='bar', ax=ax, color='skyblue')\n        ax.set_xticklabels(avg_results.index.get_level_values('frequency_mhz'), rotation=0)\n        ax.set_title('Average Whole-Body SAR')\n        ax.set_xlabel('Frequency (MHz)')\n        ax.set_ylabel('Normalized Whole-Body SAR (mW/kg)')\n        plt.tight_layout()\n        fig.savefig(os.path.join(self.plots_dir, 'average_whole_body_sar_bar.png'))\n        plt.close(fig)\n\n    def plot_peak_sar_line(self, summary_stats):\n        \"\"\"Plots the peak SAR across frequencies for far-field.\"\"\"\n        fig, ax = plt.subplots(figsize=(12, 7))\n        if 'peak_sar' in summary_stats.columns:\n            summary_stats['peak_sar'].plot(kind='line', marker='o', ax=ax, color='purple')\n            ax.set_title('Average Peak SAR (10g) Across All Tissues')\n            ax.set_xlabel('Frequency (MHz)')\n            ax.set_ylabel('Normalized Peak SAR (mW/kg)')\n            ax.set_xticks(summary_stats.index)\n            ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n        else:\n            ax.text(0.5, 0.5, 'No Peak SAR data found', ha='center', va='center')\n            ax.set_title('Average Peak SAR (10g) Across All Tissues')\n        plt.tight_layout()\n        fig.savefig(os.path.join(self.plots_dir, 'line_peak_sar_summary.png'))\n        plt.close(fig)\n\n    def plot_pssar_line(self, scenario_name, avg_results):\n        fig, ax = plt.subplots(figsize=(12, 7))\n        pssar_columns = [col for col in avg_results.columns if col.startswith('psSAR10g')]\n        if pssar_columns:\n            avg_results[pssar_columns].plot(kind='line', marker='o', ax=ax, colormap='viridis')\n            ax.set_title(f'Average Normalized psSAR10g for Scenario: {scenario_name}')\n            ax.set_xlabel('Frequency (MHz)')\n            ax.set_ylabel('Normalized psSAR10g (mW/kg)')\n            ax.legend([LEGEND_LABELS.get(col, col) for col in pssar_columns])\n            ax.set_xticks(avg_results.index)\n        else:\n            ax.text(0.5, 0.5, 'No psSAR10g data found', ha='center', va='center')\n            ax.set_title(f'Average Normalized psSAR10g for Scenario: {scenario_name}')\n        plt.tight_layout()\n        fig.savefig(os.path.join(self.plots_dir, f'pssar10g_line_{scenario_name}.png'))\n        plt.close(fig)\n\n    def plot_sar_distribution_boxplots(self, scenario_name, scenario_results_df):\n        pssar_columns = [col for col in scenario_results_df.columns if col.startswith('psSAR10g')]\n        sar_metrics_for_boxplot = ['SAR_head', 'SAR_trunk'] + pssar_columns\n        for metric in sar_metrics_for_boxplot:\n            if not scenario_results_df[metric].dropna().empty:\n                fig, ax = plt.subplots(figsize=(12, 7))\n                sns.boxplot(data=scenario_results_df, x='frequency_mhz', y=metric, ax=ax, hue='frequency_mhz', palette='viridis', legend=False)\n                ax.set_title(f'Distribution of Normalized {METRIC_LABELS.get(metric, metric)} for Scenario: {scenario_name}')\n                ax.set_xlabel('Frequency (MHz)')\n                ax.set_ylabel('Normalized SAR (mW/kg)')\n                plt.tight_layout()\n                fig.savefig(os.path.join(self.plots_dir, f'boxplot_{metric}_{scenario_name}.png'))\n                plt.close(fig)\n\n    def plot_far_field_distribution_boxplot(self, results_df, metric='SAR_whole_body'):\n        \"\"\"Generates a boxplot for the distribution of a given metric in far-field results.\"\"\"\n        if metric not in results_df.columns or results_df[metric].dropna().empty:\n            logging.getLogger('progress').warning(f\"  - WARNING: No data for metric '{metric}' to generate boxplot.\", extra={'log_type': 'warning'})\n            return\n\n        fig, ax = plt.subplots(figsize=(12, 7))\n        sns.boxplot(data=results_df, x='frequency_mhz', y=metric, ax=ax, hue='frequency_mhz', palette='viridis', legend=False)\n        ax.set_title(f'Distribution of Normalized {METRIC_LABELS.get(metric, metric)}')\n        ax.set_xlabel('Frequency (MHz)')\n        ax.set_ylabel('Normalized SAR (mW/kg)')\n        plt.tight_layout()\n        fig.savefig(os.path.join(self.plots_dir, f'boxplot_{metric}_distribution.png'))\n        plt.close(fig)\n\n    def _plot_heatmap(self, fig, ax, data, title, cbar=True, cbar_ax=None):\n        \"\"\"Helper function to plot a single heatmap.\"\"\"\n        sns.heatmap(data, ax=ax, annot=True, fmt=\".2f\", cmap='viridis', linewidths=.5,\n                    norm=LogNorm(vmin=data[data &gt; 0].min().min(), vmax=data.max().max()),\n                    cbar=cbar, cbar_ax=cbar_ax if cbar else None)\n        ax.set_title(title, pad=20)\n        return ax\n\n    def plot_sar_heatmap(self, organ_df, group_df, tissue_groups):\n        \"\"\"Generates the combined heatmap for Min, Avg, and Max SAR.\"\"\"\n        organ_pivot = organ_df.pivot_table(index='tissue', columns='frequency_mhz', values=['min_sar', 'avg_sar', 'max_sar'])\n        organ_pivot = organ_pivot.loc[(organ_pivot &gt; 0.01).any(axis=1)]\n        mean_organ_sar = organ_pivot.mean(axis=1).sort_values(ascending=False)\n        organ_pivot = organ_pivot.reindex(mean_organ_sar.index)\n        organ_pivot = organ_pivot.reorder_levels([1, 0], axis=1)\n        metric_order = ['min_sar', 'avg_sar', 'max_sar']\n        sorted_columns = sorted(organ_pivot.columns, key=lambda x: (x[0], metric_order.index(x[1])))\n        organ_pivot = organ_pivot[sorted_columns]\n\n        group_pivot = group_df.pivot_table(index='group', columns='frequency_mhz', values='avg_sar')\n        mean_group_sar = group_pivot.mean(axis=1).sort_values(ascending=False)\n        group_pivot = group_pivot.reindex(mean_group_sar.index)\n\n        if organ_pivot.empty: return\n\n        fig = plt.figure(figsize=(24, 12 + len(organ_pivot) * 0.4))\n        gs = gridspec.GridSpec(2, 2, height_ratios=[len(organ_pivot), len(group_pivot) + 1],\n                                 width_ratios=[0.95, 0.05], hspace=0.1)\n        ax_organ = fig.add_subplot(gs[0, 0])\n        ax_group = fig.add_subplot(gs[1, 0])\n        cbar_ax = fig.add_subplot(gs[:, 1])\n\n        ax_organ = self._plot_heatmap(fig, ax_organ, organ_pivot, 'Min, Avg, and Max SAR (mW/kg) per Tissue', cbar=True, cbar_ax=cbar_ax)\n        ax_organ.set_xlabel('')\n        x_labels = [metric.replace('_sar', '') for freq, metric in organ_pivot.columns]\n        ax_organ.set_xticks(np.arange(len(x_labels)) + 0.5)\n        ax_organ.set_xticklabels(x_labels, rotation=0)\n        ax_organ.set_ylabel('Tissue')\n\n        group_colors = {'eyes_group': 'r', 'skin_group': 'g', 'brain_group': 'b'}\n        tissue_to_group = {tissue: group for group, tissues in tissue_groups.items() for tissue in tissues}\n        for tick_label in ax_organ.get_yticklabels():\n            group = tissue_to_group.get(tick_label.get_text())\n            if group in group_colors:\n                tick_label.set_color(group_colors[group])\n\n        ax_group = self._plot_heatmap(fig, ax_group, group_pivot, 'Organ Group Summary (Avg SAR)', cbar=False)\n        ax_group.set_xlabel('Frequency (MHz)')\n        ax_group.set_ylabel('')\n        for tick_label in ax_group.get_yticklabels():\n            tick_label.set_rotation(0)\n            tick_label.set_color(group_colors.get(f\"{tick_label.get_text().lower()}_group\", 'black'))\n\n        plt.tight_layout(rect=[0, 0, 0.95, 0.98])\n        fig.savefig(os.path.join(self.plots_dir, 'heatmap_sar_summary.png'))\n        plt.close(fig)\n\n    def plot_peak_sar_heatmap(self, organ_df, group_df, tissue_groups, value_col='peak_sar_10g_mw_kg', title='Peak SAR'):\n        \"\"\"Generates a combined heatmap for a given peak SAR metric.\"\"\"\n        organ_pivot = organ_df.pivot_table(index='tissue', columns='frequency_mhz', values=value_col)\n        organ_pivot = organ_pivot.loc[(organ_pivot &gt; 0.01).any(axis=1)]\n        mean_organ_sar = organ_pivot.mean(axis=1).sort_values(ascending=False)\n        organ_pivot = organ_pivot.reindex(mean_organ_sar.index)\n\n        group_pivot = group_df.pivot_table(index='group', columns='frequency_mhz', values=value_col)\n        mean_group_sar = group_pivot.mean(axis=1).sort_values(ascending=False)\n        group_pivot = group_pivot.reindex(mean_group_sar.index)\n\n        if organ_pivot.empty: return\n\n        fig = plt.figure(figsize=(18, 10 + len(organ_pivot) * 0.3))\n        gs = gridspec.GridSpec(2, 2, height_ratios=[len(organ_pivot), len(group_pivot) + 1],\n                                 width_ratios=[0.95, 0.05], hspace=0.1)\n        ax_organ = fig.add_subplot(gs[0, 0])\n        ax_group = fig.add_subplot(gs[1, 0])\n        cbar_ax = fig.add_subplot(gs[:, 1])\n\n        ax_organ = self._plot_heatmap(fig, ax_organ, organ_pivot, f'{title} (mW/kg) per Tissue', cbar=True, cbar_ax=cbar_ax)\n        ax_organ.set_xlabel('')\n        ax_organ.set_xticklabels([])\n        ax_organ.set_ylabel('Tissue')\n\n        group_colors = {'eyes_group': 'r', 'skin_group': 'g', 'brain_group': 'b'}\n        tissue_to_group = {tissue: group for group, tissues in tissue_groups.items() for tissue in tissues}\n        for tick_label in ax_organ.get_yticklabels():\n            group = tissue_to_group.get(tick_label.get_text())\n            if group in group_colors:\n                tick_label.set_color(group_colors[group])\n\n        ax_group = self._plot_heatmap(fig, ax_group, group_pivot, f'Organ Group Summary ({title})', cbar=False)\n        ax_group.set_xlabel('Frequency (MHz)')\n        ax_group.set_ylabel('')\n        for tick_label in ax_group.get_yticklabels():\n            tick_label.set_rotation(0)\n            tick_label.set_color(group_colors.get(f\"{tick_label.get_text().lower()}_group\", 'black'))\n\n        plt.tight_layout(rect=[0, 0, 0.95, 0.98])\n        fig.savefig(os.path.join(self.plots_dir, f'heatmap_{value_col}_summary.png'))\n        plt.close(fig)\n</code></pre>"},{"location":"api/#src.analysis.plotter.Plotter.plot_peak_sar_line","title":"<code>plot_peak_sar_line(summary_stats)</code>","text":"<p>Plots the peak SAR across frequencies for far-field.</p> Source code in <code>src\\analysis\\plotter.py</code> <pre><code>def plot_peak_sar_line(self, summary_stats):\n    \"\"\"Plots the peak SAR across frequencies for far-field.\"\"\"\n    fig, ax = plt.subplots(figsize=(12, 7))\n    if 'peak_sar' in summary_stats.columns:\n        summary_stats['peak_sar'].plot(kind='line', marker='o', ax=ax, color='purple')\n        ax.set_title('Average Peak SAR (10g) Across All Tissues')\n        ax.set_xlabel('Frequency (MHz)')\n        ax.set_ylabel('Normalized Peak SAR (mW/kg)')\n        ax.set_xticks(summary_stats.index)\n        ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n    else:\n        ax.text(0.5, 0.5, 'No Peak SAR data found', ha='center', va='center')\n        ax.set_title('Average Peak SAR (10g) Across All Tissues')\n    plt.tight_layout()\n    fig.savefig(os.path.join(self.plots_dir, 'line_peak_sar_summary.png'))\n    plt.close(fig)\n</code></pre>"},{"location":"api/#src.analysis.plotter.Plotter.plot_far_field_distribution_boxplot","title":"<code>plot_far_field_distribution_boxplot(results_df, metric='SAR_whole_body')</code>","text":"<p>Generates a boxplot for the distribution of a given metric in far-field results.</p> Source code in <code>src\\analysis\\plotter.py</code> <pre><code>def plot_far_field_distribution_boxplot(self, results_df, metric='SAR_whole_body'):\n    \"\"\"Generates a boxplot for the distribution of a given metric in far-field results.\"\"\"\n    if metric not in results_df.columns or results_df[metric].dropna().empty:\n        logging.getLogger('progress').warning(f\"  - WARNING: No data for metric '{metric}' to generate boxplot.\", extra={'log_type': 'warning'})\n        return\n\n    fig, ax = plt.subplots(figsize=(12, 7))\n    sns.boxplot(data=results_df, x='frequency_mhz', y=metric, ax=ax, hue='frequency_mhz', palette='viridis', legend=False)\n    ax.set_title(f'Distribution of Normalized {METRIC_LABELS.get(metric, metric)}')\n    ax.set_xlabel('Frequency (MHz)')\n    ax.set_ylabel('Normalized SAR (mW/kg)')\n    plt.tight_layout()\n    fig.savefig(os.path.join(self.plots_dir, f'boxplot_{metric}_distribution.png'))\n    plt.close(fig)\n</code></pre>"},{"location":"api/#src.analysis.plotter.Plotter.plot_sar_heatmap","title":"<code>plot_sar_heatmap(organ_df, group_df, tissue_groups)</code>","text":"<p>Generates the combined heatmap for Min, Avg, and Max SAR.</p> Source code in <code>src\\analysis\\plotter.py</code> <pre><code>def plot_sar_heatmap(self, organ_df, group_df, tissue_groups):\n    \"\"\"Generates the combined heatmap for Min, Avg, and Max SAR.\"\"\"\n    organ_pivot = organ_df.pivot_table(index='tissue', columns='frequency_mhz', values=['min_sar', 'avg_sar', 'max_sar'])\n    organ_pivot = organ_pivot.loc[(organ_pivot &gt; 0.01).any(axis=1)]\n    mean_organ_sar = organ_pivot.mean(axis=1).sort_values(ascending=False)\n    organ_pivot = organ_pivot.reindex(mean_organ_sar.index)\n    organ_pivot = organ_pivot.reorder_levels([1, 0], axis=1)\n    metric_order = ['min_sar', 'avg_sar', 'max_sar']\n    sorted_columns = sorted(organ_pivot.columns, key=lambda x: (x[0], metric_order.index(x[1])))\n    organ_pivot = organ_pivot[sorted_columns]\n\n    group_pivot = group_df.pivot_table(index='group', columns='frequency_mhz', values='avg_sar')\n    mean_group_sar = group_pivot.mean(axis=1).sort_values(ascending=False)\n    group_pivot = group_pivot.reindex(mean_group_sar.index)\n\n    if organ_pivot.empty: return\n\n    fig = plt.figure(figsize=(24, 12 + len(organ_pivot) * 0.4))\n    gs = gridspec.GridSpec(2, 2, height_ratios=[len(organ_pivot), len(group_pivot) + 1],\n                             width_ratios=[0.95, 0.05], hspace=0.1)\n    ax_organ = fig.add_subplot(gs[0, 0])\n    ax_group = fig.add_subplot(gs[1, 0])\n    cbar_ax = fig.add_subplot(gs[:, 1])\n\n    ax_organ = self._plot_heatmap(fig, ax_organ, organ_pivot, 'Min, Avg, and Max SAR (mW/kg) per Tissue', cbar=True, cbar_ax=cbar_ax)\n    ax_organ.set_xlabel('')\n    x_labels = [metric.replace('_sar', '') for freq, metric in organ_pivot.columns]\n    ax_organ.set_xticks(np.arange(len(x_labels)) + 0.5)\n    ax_organ.set_xticklabels(x_labels, rotation=0)\n    ax_organ.set_ylabel('Tissue')\n\n    group_colors = {'eyes_group': 'r', 'skin_group': 'g', 'brain_group': 'b'}\n    tissue_to_group = {tissue: group for group, tissues in tissue_groups.items() for tissue in tissues}\n    for tick_label in ax_organ.get_yticklabels():\n        group = tissue_to_group.get(tick_label.get_text())\n        if group in group_colors:\n            tick_label.set_color(group_colors[group])\n\n    ax_group = self._plot_heatmap(fig, ax_group, group_pivot, 'Organ Group Summary (Avg SAR)', cbar=False)\n    ax_group.set_xlabel('Frequency (MHz)')\n    ax_group.set_ylabel('')\n    for tick_label in ax_group.get_yticklabels():\n        tick_label.set_rotation(0)\n        tick_label.set_color(group_colors.get(f\"{tick_label.get_text().lower()}_group\", 'black'))\n\n    plt.tight_layout(rect=[0, 0, 0.95, 0.98])\n    fig.savefig(os.path.join(self.plots_dir, 'heatmap_sar_summary.png'))\n    plt.close(fig)\n</code></pre>"},{"location":"api/#src.analysis.plotter.Plotter.plot_peak_sar_heatmap","title":"<code>plot_peak_sar_heatmap(organ_df, group_df, tissue_groups, value_col='peak_sar_10g_mw_kg', title='Peak SAR')</code>","text":"<p>Generates a combined heatmap for a given peak SAR metric.</p> Source code in <code>src\\analysis\\plotter.py</code> <pre><code>def plot_peak_sar_heatmap(self, organ_df, group_df, tissue_groups, value_col='peak_sar_10g_mw_kg', title='Peak SAR'):\n    \"\"\"Generates a combined heatmap for a given peak SAR metric.\"\"\"\n    organ_pivot = organ_df.pivot_table(index='tissue', columns='frequency_mhz', values=value_col)\n    organ_pivot = organ_pivot.loc[(organ_pivot &gt; 0.01).any(axis=1)]\n    mean_organ_sar = organ_pivot.mean(axis=1).sort_values(ascending=False)\n    organ_pivot = organ_pivot.reindex(mean_organ_sar.index)\n\n    group_pivot = group_df.pivot_table(index='group', columns='frequency_mhz', values=value_col)\n    mean_group_sar = group_pivot.mean(axis=1).sort_values(ascending=False)\n    group_pivot = group_pivot.reindex(mean_group_sar.index)\n\n    if organ_pivot.empty: return\n\n    fig = plt.figure(figsize=(18, 10 + len(organ_pivot) * 0.3))\n    gs = gridspec.GridSpec(2, 2, height_ratios=[len(organ_pivot), len(group_pivot) + 1],\n                             width_ratios=[0.95, 0.05], hspace=0.1)\n    ax_organ = fig.add_subplot(gs[0, 0])\n    ax_group = fig.add_subplot(gs[1, 0])\n    cbar_ax = fig.add_subplot(gs[:, 1])\n\n    ax_organ = self._plot_heatmap(fig, ax_organ, organ_pivot, f'{title} (mW/kg) per Tissue', cbar=True, cbar_ax=cbar_ax)\n    ax_organ.set_xlabel('')\n    ax_organ.set_xticklabels([])\n    ax_organ.set_ylabel('Tissue')\n\n    group_colors = {'eyes_group': 'r', 'skin_group': 'g', 'brain_group': 'b'}\n    tissue_to_group = {tissue: group for group, tissues in tissue_groups.items() for tissue in tissues}\n    for tick_label in ax_organ.get_yticklabels():\n        group = tissue_to_group.get(tick_label.get_text())\n        if group in group_colors:\n            tick_label.set_color(group_colors[group])\n\n    ax_group = self._plot_heatmap(fig, ax_group, group_pivot, f'Organ Group Summary ({title})', cbar=False)\n    ax_group.set_xlabel('Frequency (MHz)')\n    ax_group.set_ylabel('')\n    for tick_label in ax_group.get_yticklabels():\n        tick_label.set_rotation(0)\n        tick_label.set_color(group_colors.get(f\"{tick_label.get_text().lower()}_group\", 'black'))\n\n    plt.tight_layout(rect=[0, 0, 0.95, 0.98])\n    fig.savefig(os.path.join(self.plots_dir, f'heatmap_{value_col}_summary.png'))\n    plt.close(fig)\n</code></pre>"},{"location":"api/#src.analysis.strategies","title":"<code>strategies</code>","text":""},{"location":"api/#src.analysis.strategies.BaseAnalysisStrategy","title":"<code>BaseAnalysisStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for analysis strategies.</p> Source code in <code>src\\analysis\\strategies.py</code> <pre><code>class BaseAnalysisStrategy(ABC):\n    \"\"\"\n    Abstract base class for analysis strategies.\n    \"\"\"\n    def __init__(self, config, phantom_name):\n        self.config = config\n        self.phantom_name = phantom_name\n        self.base_dir = config.base_dir\n\n    @abstractmethod\n    def get_results_base_dir(self):\n        pass\n\n    @abstractmethod\n    def get_plots_dir(self):\n        pass\n\n    @abstractmethod\n    def load_and_process_results(self, analyzer):\n        pass\n\n    @abstractmethod\n    def get_normalization_factor(self, frequency_mhz, simulated_power_w):\n        pass\n\n    @abstractmethod\n    def extract_data(self, pickle_data, frequency_mhz, detailed_name, scenario_name, sim_power, norm_factor):\n        pass\n\n    @abstractmethod\n    def apply_bug_fixes(self, result_entry):\n        return result_entry\n\n    @abstractmethod\n    def calculate_summary_stats(self, results_df):\n        pass\n\n    @abstractmethod\n    def generate_plots(self, analyzer, plotter, results_df, all_organ_results_df):\n        pass\n</code></pre>"},{"location":"api/#src.analysis.strategies.NearFieldAnalysisStrategy","title":"<code>NearFieldAnalysisStrategy</code>","text":"<p>               Bases: <code>BaseAnalysisStrategy</code></p> <p>Analysis strategy for near-field simulations.</p> Source code in <code>src\\analysis\\strategies.py</code> <pre><code>class NearFieldAnalysisStrategy(BaseAnalysisStrategy):\n    \"\"\"\n    Analysis strategy for near-field simulations.\n    \"\"\"\n    def get_results_base_dir(self):\n        return os.path.join(self.base_dir, \"results\", \"near_field\", self.phantom_name)\n\n    def get_plots_dir(self):\n        return os.path.join(self.base_dir, 'plots', 'near_field', self.phantom_name)\n\n    def load_and_process_results(self, analyzer):\n        frequencies = self.config.get_antenna_config().keys()\n        placement_scenarios = self.config.get_setting(\"placement_scenarios\", {})\n\n        for freq in frequencies:\n            frequency_mhz = int(freq)\n            for scenario_name, scenario_def in placement_scenarios.items():\n                positions = scenario_def.get(\"positions\", {})\n                orientations = scenario_def.get(\"orientations\", {})\n                for pos_name in positions.keys():\n                    for orient_name in orientations.keys():\n                        analyzer._process_single_result(frequency_mhz, scenario_name, pos_name, orient_name)\n\n    def get_normalization_factor(self, frequency_mhz, simulated_power_w):\n        antenna_configs = self.config.get_antenna_config()\n        freq_config = antenna_configs.get(str(frequency_mhz), {})\n        target_power_mw = freq_config.get('target_power_mW')\n        if target_power_mw is not None and pd.notna(simulated_power_w) and simulated_power_w &gt; 0:\n            target_power_w = target_power_mw / 1000.0\n            return target_power_w / simulated_power_w\n        return 1.0\n\n    def extract_data(self, pickle_data, frequency_mhz, placement_name, scenario_name, sim_power, norm_factor):\n        summary_results = pickle_data.get('summary_results', {})\n        grouped_stats = pickle_data.get('grouped_sar_stats', {})\n        detailed_df = pickle_data.get('detailed_sar_stats')\n        result_entry = {\n            'frequency_mhz': frequency_mhz, 'placement': placement_name, 'scenario': scenario_name,\n            'input_power_w': sim_power,\n            'SAR_head': summary_results.get('head_SAR', pd.NA) * norm_factor,\n            'SAR_trunk': summary_results.get('trunk_SAR', pd.NA) * norm_factor,\n        }\n        for group_name, stats in grouped_stats.items():\n            key = f\"psSAR10g_{group_name.replace('_group', '')}\"\n            result_entry[key] = stats.get('peak_sar', pd.NA) * norm_factor\n\n        organ_entries = []\n        if detailed_df is not None:\n            peak_sar_col = 'Peak Spatial-Average SAR[IEEE/IEC62704-1] (10g)'\n            for _, row in detailed_df.iterrows():\n                organ_entries.append({\n                    'frequency_mhz': frequency_mhz, 'placement': placement_name, 'tissue': row['Tissue'],\n                    'mass_avg_sar_mw_kg': row['Mass-Averaged SAR'] * norm_factor * 1000,\n                    'peak_sar_10g_mw_kg': row.get(peak_sar_col, pd.NA) * norm_factor * 1000,\n                    'min_local_sar_mw_kg': row.get('Min. local SAR', pd.NA) * norm_factor * 1000,\n                    'max_local_sar_mw_kg': row.get('Max. local SAR', pd.NA) * norm_factor * 1000,\n                })\n        return result_entry, organ_entries\n\n    def apply_bug_fixes(self, result_entry):\n        placement = result_entry['placement'].lower()\n        if placement.startswith('front_of_eyes') or placement.startswith('by_cheek'):\n            if pd.isna(result_entry.get('SAR_head')) and pd.notna(result_entry.get('SAR_trunk')):\n                result_entry['SAR_head'] = result_entry['SAR_trunk']\n                result_entry['SAR_trunk'] = pd.NA\n        return result_entry\n\n    def calculate_summary_stats(self, results_df):\n        placement_scenarios = self.config.get_setting(\"placement_scenarios\", {})\n        placements_per_scenario = {}\n        logging.getLogger('progress').info(\"\\n--- Calculating Total Possible Placements per Scenario ---\", extra={'log_type': 'header'})\n        for name, definition in placement_scenarios.items():\n            total = len(definition.get(\"positions\", {})) * len(definition.get(\"orientations\", {}))\n            placements_per_scenario[name] = total\n            logging.getLogger('progress').info(f\"- Scenario '{name}': {total} placements\", extra={'log_type': 'info'})\n        summary_stats = results_df.groupby(['scenario', 'frequency_mhz']).mean(numeric_only=True)\n        completion_counts = results_df.groupby(['scenario', 'frequency_mhz']).size()\n        summary_stats['progress'] = summary_stats.index.map(\n            lambda idx: f\"{completion_counts.get(idx, 0)}/{placements_per_scenario.get(idx[0], 0)}\"\n        )\n        return summary_stats\n\n    def generate_plots(self, analyzer, plotter, results_df, all_organ_results_df):\n        scenarios_with_results = results_df['scenario'].unique()\n        summary_stats = self.calculate_summary_stats(results_df)\n\n        for scenario_name in scenarios_with_results:\n            logging.getLogger('progress').info(f\"\\n--- Generating plots for scenario: {scenario_name} ---\", extra={'log_type': 'header'})\n            scenario_results_df = results_df[results_df['scenario'] == scenario_name]\n            scenario_summary_stats = summary_stats.loc[scenario_name]\n            avg_results = scenario_summary_stats.drop(columns=['progress'])\n            progress_info = scenario_summary_stats['progress']\n            plotter.plot_average_sar_bar(scenario_name, avg_results, progress_info)\n            plotter.plot_pssar_line(scenario_name, avg_results)\n            plotter.plot_sar_distribution_boxplots(scenario_name, scenario_results_df)\n</code></pre>"},{"location":"api/#src.analysis.strategies.FarFieldAnalysisStrategy","title":"<code>FarFieldAnalysisStrategy</code>","text":"<p>               Bases: <code>BaseAnalysisStrategy</code></p> <p>Analysis strategy for far-field simulations.</p> Source code in <code>src\\analysis\\strategies.py</code> <pre><code>class FarFieldAnalysisStrategy(BaseAnalysisStrategy):\n    \"\"\"\n    Analysis strategy for far-field simulations.\n    \"\"\"\n    def get_results_base_dir(self):\n        return os.path.join(self.base_dir, \"results\", \"far_field\", self.phantom_name)\n\n    def get_plots_dir(self):\n        return os.path.join(self.base_dir, 'plots', 'far_field', self.phantom_name)\n\n    def load_and_process_results(self, analyzer):\n        frequencies = self.config.get_setting('frequencies_mhz', [])\n        far_field_params = self.config.get_setting('far_field_setup/environmental', {})\n        incident_directions = far_field_params.get('incident_directions', [])\n        polarizations = far_field_params.get('polarizations', [])\n\n        for freq in frequencies:\n            for direction_name in incident_directions:\n                for polarization_name in polarizations:\n                    placement_name = f\"environmental_{direction_name}_{polarization_name}\"\n                    analyzer._process_single_result(freq, \"environmental\", placement_name, \"\")\n\n    def get_normalization_factor(self, frequency_mhz, simulated_power_w):\n        # For far-field, we normalize to a power density of 1 W/m^2\n        # This should be handled in the simulation results, so factor is 1.0 here.\n        return 1.0\n\n    def extract_data(self, pickle_data, frequency_mhz, placement_name, scenario_name, sim_power, norm_factor):\n        summary_results = pickle_data.get('summary_results', {})\n        detailed_df = pickle_data.get('detailed_sar_stats')\n\n        result_entry = {\n            'frequency_mhz': frequency_mhz,\n            'placement': placement_name,\n            'scenario': scenario_name,\n            'input_power_w': sim_power,\n            'SAR_whole_body': summary_results.get('whole_body_sar', pd.NA),\n            'peak_sar': summary_results.get('peak_sar_10g_W_kg', pd.NA),\n        }\n\n        organ_entries = []\n        if detailed_df is not None:\n            peak_sar_col = 'Peak Spatial-Average SAR[IEEE/IEC62704-1] (10g)'\n            for _, row in detailed_df.iterrows():\n                organ_entries.append({\n                    'frequency_mhz': frequency_mhz,\n                    'placement': placement_name,\n                    'tissue': row['Tissue'],\n                    'mass_avg_sar_mw_kg': row['Mass-Averaged SAR'] * 1000, # Already normalized in extractor\n                    'peak_sar_10g_mw_kg': row.get(peak_sar_col, pd.NA) * 1000, # Already normalized\n                })\n        return result_entry, organ_entries\n\n    def apply_bug_fixes(self, result_entry):\n        return result_entry\n\n    def calculate_summary_stats(self, results_df):\n        return results_df.groupby('frequency_mhz').mean(numeric_only=True)\n\n    def generate_plots(self, analyzer, plotter, results_df, all_organ_results_df):\n        logging.getLogger('progress').info(\"\\n--- Generating plots for far-field analysis ---\", extra={'log_type': 'header'})\n        summary_stats = self.calculate_summary_stats(results_df)\n        plotter.plot_whole_body_sar_bar(summary_stats)\n        plotter.plot_peak_sar_line(summary_stats)\n        plotter.plot_far_field_distribution_boxplot(results_df, metric='SAR_whole_body')\n        plotter.plot_far_field_distribution_boxplot(results_df, metric='peak_sar')\n\n        # Prepare data for heatmaps\n        organ_sar_df = all_organ_results_df.groupby(['tissue', 'frequency_mhz']).agg(\n            avg_sar=('mass_avg_sar_mw_kg', 'mean')\n        ).reset_index()\n\n        organ_pssar_df = all_organ_results_df.groupby(['tissue', 'frequency_mhz'])['peak_sar_10g_mw_kg'].mean().reset_index()\n\n        group_summary_data = []\n        # tissue_groups defined in analyzer\n        for group_name, tissues in analyzer.tissue_group_definitions.items():\n            if not tissues: continue\n            # Create a case-insensitive regex pattern to match any of the tissue keywords\n            pattern = '|'.join(tissues)\n            group_df = all_organ_results_df[all_organ_results_df['tissue'].str.contains(pattern, case=False, na=False)]\n\n            if not group_df.empty:\n                summary = group_df.groupby('frequency_mhz').agg(\n                    avg_sar=('mass_avg_sar_mw_kg', 'mean'),\n                    peak_sar_10g_mw_kg=('peak_sar_10g_mw_kg', 'mean')\n                ).reset_index()\n                summary['group'] = group_name.replace('_group', '').capitalize()\n                group_summary_data.append(summary)\n\n        group_summary_df = pd.concat(group_summary_data, ignore_index=True) if group_summary_data else pd.DataFrame()\n\n        if not group_summary_df.empty:\n            group_sar_summary = group_summary_df[['group', 'frequency_mhz', 'avg_sar']]\n            group_pssar_summary = group_summary_df[['group', 'frequency_mhz', 'peak_sar_10g_mw_kg']]\n\n            plotter.plot_peak_sar_heatmap(organ_sar_df, group_sar_summary, analyzer.tissue_group_definitions, value_col='avg_sar', title='Average SAR')\n            plotter.plot_peak_sar_heatmap(organ_pssar_df, group_pssar_summary, analyzer.tissue_group_definitions, value_col='peak_sar_10g_mw_kg', title='Peak SAR 10g')\n        else:\n            logging.getLogger('progress').warning(\"  - WARNING: No data found for tissue groups, skipping heatmaps.\", extra={'log_type': 'warning'})\n</code></pre>"},{"location":"api/#src.antenna","title":"<code>antenna</code>","text":""},{"location":"api/#src.antenna.Antenna","title":"<code>Antenna</code>","text":"<p>Helper class for antenna properties.</p> Source code in <code>src\\antenna.py</code> <pre><code>class Antenna:\n    \"\"\"\n    Helper class for antenna properties.\n    \"\"\"\n    def __init__(self, config, frequency_mhz):\n        self.config = config\n        self.frequency_mhz = frequency_mhz\n        self.antenna_config = self.config.get_antenna_config()\n\n    def get_config_for_frequency(self):\n        \"\"\"\n        Returns the specific antenna configuration for the given frequency.\n        \"\"\"\n        freq_str = str(self.frequency_mhz)\n        if freq_str not in self.antenna_config:\n            raise ValueError(f\"Antenna configuration not defined for frequency: {self.frequency_mhz} MHz\")\n        return self.antenna_config[freq_str]\n\n    def get_model_type(self):\n        \"\"\"\n        Returns the antenna model type (e.g., 'PIFA', 'IFA') for the current frequency.\n        \"\"\"\n        return self.get_config_for_frequency().get(\"model_type\")\n\n    def get_source_entity_name(self):\n        \"\"\"\n        Returns the source entity name for the current frequency.\n        \"\"\"\n        return self.get_config_for_frequency().get(\"source_name\")\n\n    def get_centered_antenna_path(self, centered_antennas_dir):\n        \"\"\"\n        Returns the full path to the centered .sab file for the current frequency.\n        \"\"\"\n        antenna_filename = f\"{self.frequency_mhz}MHz_centered.sab\"\n        return os.path.join(centered_antennas_dir, antenna_filename)\n</code></pre>"},{"location":"api/#src.antenna.Antenna.get_config_for_frequency","title":"<code>get_config_for_frequency()</code>","text":"<p>Returns the specific antenna configuration for the given frequency.</p> Source code in <code>src\\antenna.py</code> <pre><code>def get_config_for_frequency(self):\n    \"\"\"\n    Returns the specific antenna configuration for the given frequency.\n    \"\"\"\n    freq_str = str(self.frequency_mhz)\n    if freq_str not in self.antenna_config:\n        raise ValueError(f\"Antenna configuration not defined for frequency: {self.frequency_mhz} MHz\")\n    return self.antenna_config[freq_str]\n</code></pre>"},{"location":"api/#src.antenna.Antenna.get_model_type","title":"<code>get_model_type()</code>","text":"<p>Returns the antenna model type (e.g., 'PIFA', 'IFA') for the current frequency.</p> Source code in <code>src\\antenna.py</code> <pre><code>def get_model_type(self):\n    \"\"\"\n    Returns the antenna model type (e.g., 'PIFA', 'IFA') for the current frequency.\n    \"\"\"\n    return self.get_config_for_frequency().get(\"model_type\")\n</code></pre>"},{"location":"api/#src.antenna.Antenna.get_source_entity_name","title":"<code>get_source_entity_name()</code>","text":"<p>Returns the source entity name for the current frequency.</p> Source code in <code>src\\antenna.py</code> <pre><code>def get_source_entity_name(self):\n    \"\"\"\n    Returns the source entity name for the current frequency.\n    \"\"\"\n    return self.get_config_for_frequency().get(\"source_name\")\n</code></pre>"},{"location":"api/#src.antenna.Antenna.get_centered_antenna_path","title":"<code>get_centered_antenna_path(centered_antennas_dir)</code>","text":"<p>Returns the full path to the centered .sab file for the current frequency.</p> Source code in <code>src\\antenna.py</code> <pre><code>def get_centered_antenna_path(self, centered_antennas_dir):\n    \"\"\"\n    Returns the full path to the centered .sab file for the current frequency.\n    \"\"\"\n    antenna_filename = f\"{self.frequency_mhz}MHz_centered.sab\"\n    return os.path.join(centered_antennas_dir, antenna_filename)\n</code></pre>"},{"location":"api/#src.colors","title":"<code>colors</code>","text":""},{"location":"api/#src.colors.get_color","title":"<code>get_color(log_type)</code>","text":"<p>Returns the color for a given log type. Defaults to white if the type is not found.</p> Source code in <code>src\\colors.py</code> <pre><code>def get_color(log_type):\n    \"\"\"\n    Returns the color for a given log type.\n    Defaults to white if the type is not found.\n    \"\"\"\n    return COLOR_MAP.get(log_type, Fore.WHITE)\n</code></pre>"},{"location":"api/#src.config","title":"<code>config</code>","text":""},{"location":"api/#src.config.Config","title":"<code>Config</code>","text":"<p>Handles loading and validation of configuration files with inheritance.</p> Source code in <code>src\\config.py</code> <pre><code>class Config:\n    \"\"\"\n    Handles loading and validation of configuration files with inheritance.\n    \"\"\"\n    def __init__(self, base_dir, config_filename=\"near_field_config.json\"):\n        self.base_dir = base_dir\n        self.config_path = self._resolve_config_path(config_filename, self.base_dir)\n        self.material_mapping_path = os.path.join(self.base_dir, 'data', 'material_name_mapping.json')\n        self.profiling_config_path = os.path.join(self.base_dir, 'configs', 'profiling_config.json')\n\n        self.config = self._load_config_with_inheritance(self.config_path)\n        self.material_mapping = self._load_json(self.material_mapping_path)\n        self.profiling_config = self._load_json(self.profiling_config_path)\n\n    def _resolve_config_path(self, config_filename, base_path):\n        \"\"\"\n        Resolves the absolute path to the configuration file.\n        - If a full path is given, it's used directly.\n        - If a filename without '.json' is given, it's assumed to be in 'configs/'.\n        \"\"\"\n        # If a full path is given, use it as is.\n        if os.path.isabs(config_filename) or os.path.dirname(config_filename):\n            return os.path.join(self.base_dir, config_filename)\n\n        # If just a name is given, assume it's in the 'configs' directory.\n        if not config_filename.endswith('.json'):\n            config_filename += '.json'\n\n        return os.path.join(self.base_dir, 'configs', config_filename)\n\n    def get_setting(self, path, default=None):\n        \"\"\"\n        Retrieves a setting from the configuration using a path-like string.\n        Example path: \"simulation_parameters.number_of_point_sensors\"\n        \"\"\"\n        keys = path.split('.')\n        current_config = self.config\n        for key in keys:\n            if isinstance(current_config, dict) and key in current_config:\n                current_config = current_config[key]\n            else:\n                return default\n        return current_config\n\n    def _load_config_with_inheritance(self, path):\n        \"\"\"\n        Loads a JSON configuration file and handles 'extends' for inheritance.\n        \"\"\"\n        config = self._load_json(path)\n\n        if \"extends\" in config:\n            # Pass the directory of the current file as the base for resolution\n            base_config_path = self._resolve_config_path(config[\"extends\"], base_path=os.path.dirname(path))\n            base_config = self._load_config_with_inheritance(base_config_path)\n\n            # Merge the current configuration into the base, with the current overriding the base.\n            config = deep_merge(config, base_config)\n\n        return config\n\n    def _load_json(self, path):\n        \"\"\"Loads a JSON file.\"\"\"\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"Configuration file not found at: {path}\")\n        with open(path, 'r') as f:\n            return json.load(f)\n\n    def get_simulation_parameters(self):\n        \"\"\"Returns the simulation parameters.\"\"\"\n        return self.config.get(\"simulation_parameters\", {})\n\n    def get_antenna_config(self):\n        \"\"\"Returns the antenna configuration.\"\"\"\n        return self.config.get(\"antenna_config\", {})\n\n    def get_gridding_parameters(self):\n        \"\"\"Returns the gridding parameters.\"\"\"\n        return self.config.get(\"gridding_parameters\", {})\n\n    def get_phantom_config(self, phantom_name):\n        \"\"\"Returns the configuration for a specific phantom.\"\"\"\n        return self.config.get(\"phantom_definitions\", {}).get(phantom_name)\n\n    def get_phantom_placements(self, phantom_name):\n        \"\"\"Returns the placement configuration for a specific phantom.\"\"\"\n        phantom_config = self.get_phantom_config(phantom_name)\n        if phantom_config:\n            return phantom_config.get(\"placements\", {})\n        return {}\n\n    def get_material_mapping(self, phantom_name):\n        \"\"\"Returns the material name mapping for a specific phantom.\"\"\"\n        if phantom_name in self.material_mapping:\n            return self.material_mapping[phantom_name]\n        else:\n            # Fallback to the old format or a default\n            return self.material_mapping\n\n    def get_solver_settings(self):\n        \"\"\"Returns the solver settings.\"\"\"\n        return self.config.get(\"solver_settings\", {})\n\n    def get_antenna_component_names(self, antenna_model_type):\n        \"\"\"Returns the component names for a specific antenna model.\"\"\"\n        return self.config.get(\"antenna_config\", {}).get(\"components\", {}).get(antenna_model_type)\n\n    def get_manual_isolve(self):\n        \"\"\"Returns the manual_isolve flag.\"\"\"\n        return self.config.get(\"manual_isolve\", False)\n\n    def get_freespace_expansion(self):\n        \"\"\"Returns the freespace antenna bounding box expansion in mm.\"\"\"\n        return self.get_simulation_parameters().get(\"freespace_antenna_bbox_expansion_mm\", [10, 10, 10])\n\n    def get_excitation_type(self):\n        \"\"\"Returns the excitation type.\"\"\"\n        return self.get_simulation_parameters().get(\"excitation_type\", \"Harmonic\")\n\n    def get_bandwidth(self):\n        \"\"\"Returns the bandwidth in MHz.\"\"\"\n        return self.get_simulation_parameters().get(\"bandwidth_mhz\", 50.0)\n\n    def get_placement_scenario(self, scenario_name):\n        \"\"\"Returns the definition for a specific placement scenario.\"\"\"\n        return self.config.get(\"placement_scenarios\", {}).get(scenario_name)\n\n    def get_profiling_config(self, study_type):\n        \"\"\"\n        Returns the specific profiling configuration for the given study type.\n        \"\"\"\n        if study_type not in self.profiling_config:\n            raise ValueError(f\"Profiling configuration not defined for study type: {study_type}\")\n        return self.profiling_config[study_type]\n\n    def get_line_profiling_config(self):\n        \"\"\"\n        Returns the line_profiling settings from the main config.\n        \"\"\"\n        return self.get_setting('line_profiling', {})\n\n    def get_server(self):\n        \"\"\"Returns the server name.\"\"\"\n        return self.config.get(\"server\", None)\n\n    def get_osparc_credentials(self):\n        \"\"\"Returns the osparc credentials from environment variables.\"\"\"\n        credentials = {\n            \"api_key\": os.getenv(\"OSPARC_API_KEY\"),\n            \"api_secret\": os.getenv(\"OSPARC_API_SECRET\"),\n            \"api_server\": os.getenv(\"OSPARC_API_SERVER\"),\n            \"api_version\": os.getenv(\"OSPARC_API_VERSION\", \"v0\")\n        }\n\n        # Check for missing credentials and provide helpful error message\n        missing = [key for key, value in credentials.items() if value is None and key != \"api_version\"]\n        if missing:\n            raise ValueError(\n                f\"Missing oSPARC credentials: {', '.join(missing)}. \"\n                \"Please create a .env file in the project root with your oSPARC API credentials. \"\n                \"See README.md for setup instructions.\"\n            )\n\n        return credentials\n\n    def get_only_write_input_file(self):\n        \"\"\"\n        Returns the 'only_write_input_file' flag from execution_control.\n        \"\"\"\n        return self.get_setting('execution_control.only_write_input_file', False)\n</code></pre>"},{"location":"api/#src.config.Config.get_setting","title":"<code>get_setting(path, default=None)</code>","text":"<p>Retrieves a setting from the configuration using a path-like string. Example path: \"simulation_parameters.number_of_point_sensors\"</p> Source code in <code>src\\config.py</code> <pre><code>def get_setting(self, path, default=None):\n    \"\"\"\n    Retrieves a setting from the configuration using a path-like string.\n    Example path: \"simulation_parameters.number_of_point_sensors\"\n    \"\"\"\n    keys = path.split('.')\n    current_config = self.config\n    for key in keys:\n        if isinstance(current_config, dict) and key in current_config:\n            current_config = current_config[key]\n        else:\n            return default\n    return current_config\n</code></pre>"},{"location":"api/#src.config.Config.get_simulation_parameters","title":"<code>get_simulation_parameters()</code>","text":"<p>Returns the simulation parameters.</p> Source code in <code>src\\config.py</code> <pre><code>def get_simulation_parameters(self):\n    \"\"\"Returns the simulation parameters.\"\"\"\n    return self.config.get(\"simulation_parameters\", {})\n</code></pre>"},{"location":"api/#src.config.Config.get_antenna_config","title":"<code>get_antenna_config()</code>","text":"<p>Returns the antenna configuration.</p> Source code in <code>src\\config.py</code> <pre><code>def get_antenna_config(self):\n    \"\"\"Returns the antenna configuration.\"\"\"\n    return self.config.get(\"antenna_config\", {})\n</code></pre>"},{"location":"api/#src.config.Config.get_gridding_parameters","title":"<code>get_gridding_parameters()</code>","text":"<p>Returns the gridding parameters.</p> Source code in <code>src\\config.py</code> <pre><code>def get_gridding_parameters(self):\n    \"\"\"Returns the gridding parameters.\"\"\"\n    return self.config.get(\"gridding_parameters\", {})\n</code></pre>"},{"location":"api/#src.config.Config.get_phantom_config","title":"<code>get_phantom_config(phantom_name)</code>","text":"<p>Returns the configuration for a specific phantom.</p> Source code in <code>src\\config.py</code> <pre><code>def get_phantom_config(self, phantom_name):\n    \"\"\"Returns the configuration for a specific phantom.\"\"\"\n    return self.config.get(\"phantom_definitions\", {}).get(phantom_name)\n</code></pre>"},{"location":"api/#src.config.Config.get_phantom_placements","title":"<code>get_phantom_placements(phantom_name)</code>","text":"<p>Returns the placement configuration for a specific phantom.</p> Source code in <code>src\\config.py</code> <pre><code>def get_phantom_placements(self, phantom_name):\n    \"\"\"Returns the placement configuration for a specific phantom.\"\"\"\n    phantom_config = self.get_phantom_config(phantom_name)\n    if phantom_config:\n        return phantom_config.get(\"placements\", {})\n    return {}\n</code></pre>"},{"location":"api/#src.config.Config.get_material_mapping","title":"<code>get_material_mapping(phantom_name)</code>","text":"<p>Returns the material name mapping for a specific phantom.</p> Source code in <code>src\\config.py</code> <pre><code>def get_material_mapping(self, phantom_name):\n    \"\"\"Returns the material name mapping for a specific phantom.\"\"\"\n    if phantom_name in self.material_mapping:\n        return self.material_mapping[phantom_name]\n    else:\n        # Fallback to the old format or a default\n        return self.material_mapping\n</code></pre>"},{"location":"api/#src.config.Config.get_solver_settings","title":"<code>get_solver_settings()</code>","text":"<p>Returns the solver settings.</p> Source code in <code>src\\config.py</code> <pre><code>def get_solver_settings(self):\n    \"\"\"Returns the solver settings.\"\"\"\n    return self.config.get(\"solver_settings\", {})\n</code></pre>"},{"location":"api/#src.config.Config.get_antenna_component_names","title":"<code>get_antenna_component_names(antenna_model_type)</code>","text":"<p>Returns the component names for a specific antenna model.</p> Source code in <code>src\\config.py</code> <pre><code>def get_antenna_component_names(self, antenna_model_type):\n    \"\"\"Returns the component names for a specific antenna model.\"\"\"\n    return self.config.get(\"antenna_config\", {}).get(\"components\", {}).get(antenna_model_type)\n</code></pre>"},{"location":"api/#src.config.Config.get_manual_isolve","title":"<code>get_manual_isolve()</code>","text":"<p>Returns the manual_isolve flag.</p> Source code in <code>src\\config.py</code> <pre><code>def get_manual_isolve(self):\n    \"\"\"Returns the manual_isolve flag.\"\"\"\n    return self.config.get(\"manual_isolve\", False)\n</code></pre>"},{"location":"api/#src.config.Config.get_freespace_expansion","title":"<code>get_freespace_expansion()</code>","text":"<p>Returns the freespace antenna bounding box expansion in mm.</p> Source code in <code>src\\config.py</code> <pre><code>def get_freespace_expansion(self):\n    \"\"\"Returns the freespace antenna bounding box expansion in mm.\"\"\"\n    return self.get_simulation_parameters().get(\"freespace_antenna_bbox_expansion_mm\", [10, 10, 10])\n</code></pre>"},{"location":"api/#src.config.Config.get_excitation_type","title":"<code>get_excitation_type()</code>","text":"<p>Returns the excitation type.</p> Source code in <code>src\\config.py</code> <pre><code>def get_excitation_type(self):\n    \"\"\"Returns the excitation type.\"\"\"\n    return self.get_simulation_parameters().get(\"excitation_type\", \"Harmonic\")\n</code></pre>"},{"location":"api/#src.config.Config.get_bandwidth","title":"<code>get_bandwidth()</code>","text":"<p>Returns the bandwidth in MHz.</p> Source code in <code>src\\config.py</code> <pre><code>def get_bandwidth(self):\n    \"\"\"Returns the bandwidth in MHz.\"\"\"\n    return self.get_simulation_parameters().get(\"bandwidth_mhz\", 50.0)\n</code></pre>"},{"location":"api/#src.config.Config.get_placement_scenario","title":"<code>get_placement_scenario(scenario_name)</code>","text":"<p>Returns the definition for a specific placement scenario.</p> Source code in <code>src\\config.py</code> <pre><code>def get_placement_scenario(self, scenario_name):\n    \"\"\"Returns the definition for a specific placement scenario.\"\"\"\n    return self.config.get(\"placement_scenarios\", {}).get(scenario_name)\n</code></pre>"},{"location":"api/#src.config.Config.get_profiling_config","title":"<code>get_profiling_config(study_type)</code>","text":"<p>Returns the specific profiling configuration for the given study type.</p> Source code in <code>src\\config.py</code> <pre><code>def get_profiling_config(self, study_type):\n    \"\"\"\n    Returns the specific profiling configuration for the given study type.\n    \"\"\"\n    if study_type not in self.profiling_config:\n        raise ValueError(f\"Profiling configuration not defined for study type: {study_type}\")\n    return self.profiling_config[study_type]\n</code></pre>"},{"location":"api/#src.config.Config.get_line_profiling_config","title":"<code>get_line_profiling_config()</code>","text":"<p>Returns the line_profiling settings from the main config.</p> Source code in <code>src\\config.py</code> <pre><code>def get_line_profiling_config(self):\n    \"\"\"\n    Returns the line_profiling settings from the main config.\n    \"\"\"\n    return self.get_setting('line_profiling', {})\n</code></pre>"},{"location":"api/#src.config.Config.get_server","title":"<code>get_server()</code>","text":"<p>Returns the server name.</p> Source code in <code>src\\config.py</code> <pre><code>def get_server(self):\n    \"\"\"Returns the server name.\"\"\"\n    return self.config.get(\"server\", None)\n</code></pre>"},{"location":"api/#src.config.Config.get_osparc_credentials","title":"<code>get_osparc_credentials()</code>","text":"<p>Returns the osparc credentials from environment variables.</p> Source code in <code>src\\config.py</code> <pre><code>def get_osparc_credentials(self):\n    \"\"\"Returns the osparc credentials from environment variables.\"\"\"\n    credentials = {\n        \"api_key\": os.getenv(\"OSPARC_API_KEY\"),\n        \"api_secret\": os.getenv(\"OSPARC_API_SECRET\"),\n        \"api_server\": os.getenv(\"OSPARC_API_SERVER\"),\n        \"api_version\": os.getenv(\"OSPARC_API_VERSION\", \"v0\")\n    }\n\n    # Check for missing credentials and provide helpful error message\n    missing = [key for key, value in credentials.items() if value is None and key != \"api_version\"]\n    if missing:\n        raise ValueError(\n            f\"Missing oSPARC credentials: {', '.join(missing)}. \"\n            \"Please create a .env file in the project root with your oSPARC API credentials. \"\n            \"See README.md for setup instructions.\"\n        )\n\n    return credentials\n</code></pre>"},{"location":"api/#src.config.Config.get_only_write_input_file","title":"<code>get_only_write_input_file()</code>","text":"<p>Returns the 'only_write_input_file' flag from execution_control.</p> Source code in <code>src\\config.py</code> <pre><code>def get_only_write_input_file(self):\n    \"\"\"\n    Returns the 'only_write_input_file' flag from execution_control.\n    \"\"\"\n    return self.get_setting('execution_control.only_write_input_file', False)\n</code></pre>"},{"location":"api/#src.config.deep_merge","title":"<code>deep_merge(source, destination)</code>","text":"<p>Recursively merges two dictionaries, with values from the source overriding those in the destination.</p> Source code in <code>src\\config.py</code> <pre><code>def deep_merge(source, destination):\n    \"\"\"\n    Recursively merges two dictionaries, with values from the source overriding\n    those in the destination.\n    \"\"\"\n    for key, value in source.items():\n        if isinstance(value, dict) and key in destination and isinstance(destination[key], dict):\n            # If both are dicts, recurse\n            deep_merge(value, destination[key])\n        else:\n            # Otherwise, source overrides destination\n            destination[key] = value\n    return destination\n</code></pre>"},{"location":"api/#src.data_extractor","title":"<code>data_extractor</code>","text":""},{"location":"api/#src.data_extractor.get_parameter_from_json","title":"<code>get_parameter_from_json(file_path, json_path)</code>","text":"<p>Extracts a parameter from a JSON file using a dot-separated path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the JSON file.</p> required <code>json_path</code> <code>str</code> <p>The dot-separated path to the desired value.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The extracted value, or None if not found.</p> Source code in <code>src\\data_extractor.py</code> <pre><code>def get_parameter_from_json(file_path: str, json_path: str) -&gt; Any:\n    \"\"\"\n    Extracts a parameter from a JSON file using a dot-separated path.\n\n    Args:\n        file_path (str): The path to the JSON file.\n        json_path (str): The dot-separated path to the desired value.\n\n    Returns:\n        Any: The extracted value, or None if not found.\n    \"\"\"\n    if not os.path.exists(file_path):\n        return None\n\n    with open(file_path, 'r') as f:\n        data = json.load(f)\n\n    keys = json_path.split('.')\n    value = data\n    for key in keys:\n        if isinstance(value, dict) and key in value:\n            value = value[key]\n        else:\n            return None\n    return value\n</code></pre>"},{"location":"api/#src.data_extractor.get_parameter","title":"<code>get_parameter(source_config, context)</code>","text":"<p>Generic function to retrieve a parameter from a specified source.</p> <p>Parameters:</p> Name Type Description Default <code>source_config</code> <code>Dict[str, Any]</code> <p>Configuration for the data source.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>Contextual information like frequency, phantom_name, etc.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The retrieved parameter value, or None if an error occurs.</p> Source code in <code>src\\data_extractor.py</code> <pre><code>def get_parameter(source_config: Dict[str, Any], context: Dict[str, Any]) -&gt; Any:\n    \"\"\"\n    Generic function to retrieve a parameter from a specified source.\n\n    Args:\n        source_config (Dict[str, Any]): Configuration for the data source.\n        context (Dict[str, Any]): Contextual information like frequency, phantom_name, etc.\n\n    Returns:\n        Any: The retrieved parameter value, or None if an error occurs.\n    \"\"\"\n    source_type = source_config.get('source_type')\n\n    if source_type == 'json':\n        file_path_template = source_config.get('file_path_template')\n        if not file_path_template:\n            return None\n\n        # Replace placeholders in the file path template with values from the context\n        try:\n            file_path = file_path_template.format(**context)\n        except KeyError as e:\n            logging.getLogger('verbose').error(f\"Error: Missing context for placeholder in file_path_template: {e}\")\n            return None\n\n        json_path = source_config.get('json_path')\n        if not json_path:\n            return None\n\n        project_root = context.get('project_root', '')\n        full_path = os.path.join(project_root, file_path)\n\n        return get_parameter_from_json(full_path, json_path)\n\n    # Add other source types here in the future (e.g., 'simulation')\n    # elif source_type == 'simulation':\n    #     # ... implementation for extracting from simulation ...\n    #     pass\n\n    else:\n        logging.getLogger('verbose').error(f\"Error: Unsupported source type '{source_type}'\")\n        return None\n</code></pre>"},{"location":"api/#src.gui_manager","title":"<code>gui_manager</code>","text":""},{"location":"api/#src.gui_manager.QueueGUI","title":"<code>QueueGUI</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>A GUI proxy that sends messages to a multiprocessing.Queue. This allows a separate process to communicate with the main GUI process. It mimics the interface of the original GUI signal system.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>class QueueGUI(LoggingMixin):\n    \"\"\"\n    A GUI proxy that sends messages to a multiprocessing.Queue.\n    This allows a separate process to communicate with the main GUI process.\n    It mimics the interface of the original GUI signal system.\n    \"\"\"\n    def __init__(self, queue, stop_event, profiler, progress_logger, verbose_logger):\n        self.queue = queue\n        self.stop_event = stop_event\n        self.profiler = profiler\n        self.progress_logger = progress_logger\n        self.verbose_logger = verbose_logger\n\n    def log(self, message, level='verbose', log_type='default'):\n        \"\"\"Puts a log message on the queue.\"\"\"\n        if level == 'progress':\n            self.queue.put({'type': 'status', 'message': message, 'log_type': log_type})\n\n    def update_overall_progress(self, current_step, total_steps):\n        \"\"\"Puts an overall progress update on the queue.\"\"\"\n        self.queue.put({'type': 'overall_progress', 'current': current_step, 'total': total_steps})\n\n    def update_stage_progress(self, stage_name, current_step, total_steps):\n        \"\"\"Puts a stage progress update on the queue.\"\"\"\n        self.queue.put({'type': 'stage_progress', 'name': stage_name, 'current': current_step, 'total': total_steps})\n\n    def start_stage_animation(self, task_name, end_value):\n        \"\"\"Puts a start animation message on the queue with the estimated duration.\"\"\"\n        estimate = self.profiler.get_subtask_estimate(task_name)\n        self.queue.put({'type': 'start_animation', 'estimate': estimate, 'end_value': end_value})\n\n    def end_stage_animation(self):\n        \"\"\"Puts an end animation message on the queue.\"\"\"\n        self.queue.put({'type': 'end_animation'})\n\n    def update_profiler(self):\n        \"\"\"Puts the updated profiler object on the queue.\"\"\"\n        # We now send the entire profiler object to the GUI\n        self.queue.put({'type': 'profiler_update', 'profiler': self.profiler})\n\n    def process_events(self):\n        \"\"\"This is a no-op in the process-based model, but kept for interface compatibility.\"\"\"\n        pass\n\n    def is_stopped(self):\n        \"\"\"Checks if the stop event has been set by the main process.\"\"\"\n        return self.stop_event.is_set()\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.log","title":"<code>log(message, level='verbose', log_type='default')</code>","text":"<p>Puts a log message on the queue.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def log(self, message, level='verbose', log_type='default'):\n    \"\"\"Puts a log message on the queue.\"\"\"\n    if level == 'progress':\n        self.queue.put({'type': 'status', 'message': message, 'log_type': log_type})\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.update_overall_progress","title":"<code>update_overall_progress(current_step, total_steps)</code>","text":"<p>Puts an overall progress update on the queue.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def update_overall_progress(self, current_step, total_steps):\n    \"\"\"Puts an overall progress update on the queue.\"\"\"\n    self.queue.put({'type': 'overall_progress', 'current': current_step, 'total': total_steps})\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.update_stage_progress","title":"<code>update_stage_progress(stage_name, current_step, total_steps)</code>","text":"<p>Puts a stage progress update on the queue.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def update_stage_progress(self, stage_name, current_step, total_steps):\n    \"\"\"Puts a stage progress update on the queue.\"\"\"\n    self.queue.put({'type': 'stage_progress', 'name': stage_name, 'current': current_step, 'total': total_steps})\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.start_stage_animation","title":"<code>start_stage_animation(task_name, end_value)</code>","text":"<p>Puts a start animation message on the queue with the estimated duration.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def start_stage_animation(self, task_name, end_value):\n    \"\"\"Puts a start animation message on the queue with the estimated duration.\"\"\"\n    estimate = self.profiler.get_subtask_estimate(task_name)\n    self.queue.put({'type': 'start_animation', 'estimate': estimate, 'end_value': end_value})\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.end_stage_animation","title":"<code>end_stage_animation()</code>","text":"<p>Puts an end animation message on the queue.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def end_stage_animation(self):\n    \"\"\"Puts an end animation message on the queue.\"\"\"\n    self.queue.put({'type': 'end_animation'})\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.update_profiler","title":"<code>update_profiler()</code>","text":"<p>Puts the updated profiler object on the queue.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def update_profiler(self):\n    \"\"\"Puts the updated profiler object on the queue.\"\"\"\n    # We now send the entire profiler object to the GUI\n    self.queue.put({'type': 'profiler_update', 'profiler': self.profiler})\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.process_events","title":"<code>process_events()</code>","text":"<p>This is a no-op in the process-based model, but kept for interface compatibility.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def process_events(self):\n    \"\"\"This is a no-op in the process-based model, but kept for interface compatibility.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#src.gui_manager.QueueGUI.is_stopped","title":"<code>is_stopped()</code>","text":"<p>Checks if the stop event has been set by the main process.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def is_stopped(self):\n    \"\"\"Checks if the stop event has been set by the main process.\"\"\"\n    return self.stop_event.is_set()\n</code></pre>"},{"location":"api/#src.gui_manager.ProgressGUI","title":"<code>ProgressGUI</code>","text":"<p>               Bases: <code>QWidget</code></p> Source code in <code>src\\gui_manager.py</code> <pre><code>class ProgressGUI(QWidget):\n    def __init__(self, queue, stop_event, process, window_title=\"Simulation Progress\"):\n        super().__init__()\n        self.queue = queue\n        self.stop_event = stop_event\n        self.process = process\n        self.start_time = time.monotonic()\n        self.progress_logger = logging.getLogger('progress')\n        self.verbose_logger = logging.getLogger('verbose')\n        self.window_title = window_title\n        self.init_ui()\n\n        self.phase_name_map = {\n            \"Setup\": \"setup\",\n            \"Running Simulation\": \"run\",\n            \"Extracting Results\": \"extract\"\n        }\n\n        self.animation_timer = QTimer(self)\n        self.animation_timer.timeout.connect(self.update_animation)\n        self.animation_active = False\n        self.animation_start_time = 0\n        self.animation_duration = 0\n        self.animation_start_value = 0\n        self.animation_end_value = 0\n        self.total_steps_for_stage = 0\n\n        self.profiler_phase = None\n\n        self.queue_timer = QTimer(self)\n        self.queue_timer.timeout.connect(self.process_queue)\n        self.queue_timer.start(100) # Poll the queue every 100ms\n\n        self.clock_timer = QTimer(self)\n        self.clock_timer.timeout.connect(self.update_clock)\n        self.clock_timer.start(1000)\n\n    def init_ui(self):\n        self.setWindowTitle(self.window_title)\n        self.layout = QVBoxLayout()\n        self.grid_layout = QGridLayout()\n\n        self.overall_progress_label = QLabel(\"Overall Progress:\")\n        self.layout.addWidget(self.overall_progress_label)\n        self.overall_progress_bar = QProgressBar(self)\n        self.overall_progress_bar.setRange(0, 1000)\n        self.layout.addWidget(self.overall_progress_bar)\n\n        self.stage_label = QLabel(\"Current Stage:\")\n        self.layout.addWidget(self.stage_label)\n        self.stage_progress_bar = QProgressBar(self)\n        self.stage_progress_bar.setRange(0, 1000)\n        self.layout.addWidget(self.stage_progress_bar)\n\n        self.elapsed_label = QLabel(\"Elapsed: N/A\")\n        self.eta_label = QLabel(\"Time Remaining: N/A\")\n        self.grid_layout.addWidget(self.elapsed_label, 0, 0)\n        self.grid_layout.addWidget(self.eta_label, 0, 1)\n        self.layout.addLayout(self.grid_layout)\n\n        self.status_log_label = QLabel(\"Status Log:\")\n        self.layout.addWidget(self.status_log_label)\n        self.status_text = QTextEdit(self)\n        self.status_text.setReadOnly(True)\n        self.layout.addWidget(self.status_text)\n\n        # --- Buttons ---\n        self.button_layout = QHBoxLayout()\n        self.stop_button = QPushButton(\"Stop\")\n        self.stop_button.clicked.connect(self.stop_study)\n        self.tray_button = QPushButton(\"Run in Background\")\n        self.tray_button.clicked.connect(self.hide_to_tray)\n        self.button_layout.addWidget(self.stop_button)\n        self.button_layout.addWidget(self.tray_button)\n        self.layout.addLayout(self.button_layout)\n        # --- End Buttons ---\n\n        self.setLayout(self.layout)\n\n        # --- System Tray Icon ---\n        self.tray_icon = QSystemTrayIcon(self)\n        style = self.style()\n        icon = style.standardIcon(style.StandardPixmap.SP_ComputerIcon)\n        self.tray_icon.setIcon(icon)\n        self.tray_icon.setToolTip(\"Simulation is running...\")\n\n        tray_menu = QMenu(self)\n        show_action = QAction(\"Show\", self)\n        show_action.triggered.connect(self.show_from_tray)\n        tray_menu.addAction(show_action)\n\n        exit_action = QAction(\"Exit\", self)\n        exit_action.triggered.connect(self.close)\n        tray_menu.addAction(exit_action)\n\n        self.tray_icon.setContextMenu(tray_menu)\n        self.tray_icon.activated.connect(self.tray_icon_activated)\n        # --- End System Tray Icon ---\n\n    def process_queue(self):\n        \"\"\"Processes messages from the study process queue.\"\"\"\n        from queue import Empty\n        while not self.queue.empty():\n            try:\n                msg = self.queue.get_nowait()\n                msg_type = msg.get('type')\n\n                if msg_type == 'status':\n                    self.update_status(msg['message'], msg.get('log_type', 'default'))\n                elif msg_type == 'overall_progress':\n                    self.update_overall_progress(msg['current'], msg['total'])\n                elif msg_type == 'stage_progress':\n                    self.update_stage_progress(msg['name'], msg['current'], msg['total'])\n                elif msg_type == 'start_animation':\n                    self.start_stage_animation(msg['estimate'], msg['end_value'])\n                elif msg_type == 'end_animation':\n                    self.end_stage_animation()\n                elif msg_type == 'profiler_update':\n                    # This message now passes the entire profiler object\n                    # to ensure the GUI has the latest estimates.\n                    self.profiler = msg.get('profiler')\n                    if self.profiler:\n                        self.profiler_phase = self.profiler.current_phase\n                elif msg_type == 'finished':\n                    self.study_finished()\n                elif msg_type == 'fatal_error':\n                    self.update_status(f\"FATAL ERROR: {msg['message']}\", log_type='fatal')\n                    self.study_finished(error=True)\n\n            except Empty:\n                break # Queue is empty\n            except Exception as e:\n                self.verbose_logger.error(f\"Error processing GUI queue: {e}\\n{traceback.format_exc()}\")\n\n\n    def tray_icon_activated(self, reason):\n        if reason == QSystemTrayIcon.ActivationReason.Trigger:\n            self.show_from_tray()\n\n    def hide_to_tray(self):\n        self.hide()\n        self.tray_icon.show()\n\n    def show_from_tray(self):\n        self.show()\n        self.tray_icon.hide()\n\n    def stop_study(self):\n        message = \"--- Sending stop signal to study process ---\"\n        self.progress_logger.info(message, extra={'log_type': 'warning'})\n        self.verbose_logger.info(message, extra={'log_type': 'warning'})\n        self.update_status(message, log_type='warning')\n        self.stop_button.setEnabled(False)\n        self.tray_button.setEnabled(False)\n\n        # Signal the study process to stop using the event\n        self.stop_event.set()\n\n    def update_overall_progress(self, current_step, total_steps):\n        if total_steps &gt; 0:\n            progress_percent = (current_step / total_steps) * 100\n            self.overall_progress_bar.setValue(int(progress_percent * 10))\n            self.overall_progress_bar.setFormat(f\"{progress_percent:.1f}%\")\n\n    def update_stage_progress(self, stage_name, current_step, total_steps):\n        self.stage_label.setText(f\"Current Stage: {stage_name}\")\n        self.total_steps_for_stage = total_steps\n\n        self.end_stage_animation()\n\n        progress_percent = (current_step / total_steps) if total_steps &gt; 0 else 0\n        final_value = int(progress_percent * 1000)\n\n        self.stage_progress_bar.setValue(final_value)\n        self.stage_progress_bar.setFormat(f\"{progress_percent * 100:.0f}%\")\n\n        # Note: Overall progress is now driven by messages, not calculated here.\n\n    def start_stage_animation(self, estimated_duration, end_step):\n        self.animation_start_time = time.monotonic()\n        self.animation_duration = estimated_duration\n        self.animation_start_value = self.stage_progress_bar.value()\n\n        if self.total_steps_for_stage &gt; 0:\n            self.animation_end_value = int((end_step / self.total_steps_for_stage) * 1000)\n        else:\n            self.animation_end_value = 0\n\n        if self.animation_start_value &gt;= self.animation_end_value:\n            return\n\n        self.animation_active = True\n        if not self.animation_timer.isActive():\n            self.animation_timer.start(50)\n\n    def end_stage_animation(self):\n        self.animation_active = False\n        if self.animation_timer.isActive():\n            self.animation_timer.stop()\n\n    def update_animation(self):\n        if not self.animation_active:\n            return\n\n        elapsed = time.monotonic() - self.animation_start_time\n\n        if self.animation_duration &gt; 0:\n            progress_ratio = min(elapsed / self.animation_duration, 1.0)\n        else:\n            progress_ratio = 1.0\n\n        value_range = self.animation_end_value - self.animation_start_value\n        current_value = self.animation_start_value + int(value_range * progress_ratio)\n\n        current_value = min(current_value, self.animation_end_value)\n\n        self.stage_progress_bar.setValue(current_value)\n        percent = (current_value / 1000) * 100\n        self.stage_progress_bar.setFormat(f\"{percent:.0f}%\")\n\n    def update_status(self, message, log_type='default'):\n        # This is a simplified version. A real implementation might use HTML with colors.\n        self.status_text.append(message)\n\n    def update_clock(self):\n        elapsed_sec = time.monotonic() - self.start_time\n        self.elapsed_label.setText(f\"Elapsed: {format_time(elapsed_sec)}\")\n\n        # Calculate ETA here using the pull model\n        if hasattr(self, 'profiler') and self.profiler and self.profiler.current_phase:\n            current_stage_progress_ratio = self.stage_progress_bar.value() / 1000.0\n            eta_sec = self.profiler.get_time_remaining(current_stage_progress=current_stage_progress_ratio)\n\n            if eta_sec is not None:\n                self.eta_label.setText(f\"Time Remaining: {format_time(eta_sec)}\")\n            else:\n                self.eta_label.setText(\"Time Remaining: N/A\")\n        else:\n            self.eta_label.setText(\"Time Remaining: N/A\")\n\n    def study_finished(self, error=False):\n        self.clock_timer.stop()\n        self.queue_timer.stop()\n        self.end_stage_animation()\n        if not error:\n            self.update_status(\"--- Study Finished ---\", log_type='success')\n            self.overall_progress_bar.setValue(self.overall_progress_bar.maximum())\n            self.stage_label.setText(\"Finished\")\n        else:\n            self.update_status(\"--- Study Finished with Errors ---\", log_type='fatal')\n            self.stage_label.setText(\"Error\")\n\n        self.stop_button.setEnabled(False)\n        self.tray_button.setEnabled(False)\n        QTimer.singleShot(3000, self.close)\n\n    def closeEvent(self, event):\n        if self.tray_icon.isVisible():\n            self.tray_icon.hide()\n\n        # Terminate the study process if it's still alive\n        if self.process.is_alive():\n            self.progress_logger.info(\"Terminating study process...\", extra={'log_type': 'warning'})\n            self.process.terminate()\n            self.process.join(timeout=5) # Wait for termination\n\n        shutdown_loggers()\n        event.accept()\n</code></pre>"},{"location":"api/#src.gui_manager.ProgressGUI.process_queue","title":"<code>process_queue()</code>","text":"<p>Processes messages from the study process queue.</p> Source code in <code>src\\gui_manager.py</code> <pre><code>def process_queue(self):\n    \"\"\"Processes messages from the study process queue.\"\"\"\n    from queue import Empty\n    while not self.queue.empty():\n        try:\n            msg = self.queue.get_nowait()\n            msg_type = msg.get('type')\n\n            if msg_type == 'status':\n                self.update_status(msg['message'], msg.get('log_type', 'default'))\n            elif msg_type == 'overall_progress':\n                self.update_overall_progress(msg['current'], msg['total'])\n            elif msg_type == 'stage_progress':\n                self.update_stage_progress(msg['name'], msg['current'], msg['total'])\n            elif msg_type == 'start_animation':\n                self.start_stage_animation(msg['estimate'], msg['end_value'])\n            elif msg_type == 'end_animation':\n                self.end_stage_animation()\n            elif msg_type == 'profiler_update':\n                # This message now passes the entire profiler object\n                # to ensure the GUI has the latest estimates.\n                self.profiler = msg.get('profiler')\n                if self.profiler:\n                    self.profiler_phase = self.profiler.current_phase\n            elif msg_type == 'finished':\n                self.study_finished()\n            elif msg_type == 'fatal_error':\n                self.update_status(f\"FATAL ERROR: {msg['message']}\", log_type='fatal')\n                self.study_finished(error=True)\n\n        except Empty:\n            break # Queue is empty\n        except Exception as e:\n            self.verbose_logger.error(f\"Error processing GUI queue: {e}\\n{traceback.format_exc()}\")\n</code></pre>"},{"location":"api/#src.logging_manager","title":"<code>logging_manager</code>","text":""},{"location":"api/#src.logging_manager.ColorFormatter","title":"<code>ColorFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>A custom formatter that applies colors based on the log_type attribute of a log record, and resets the style after each message.</p> Source code in <code>src\\logging_manager.py</code> <pre><code>class ColorFormatter(logging.Formatter):\n    \"\"\"\n    A custom formatter that applies colors based on the log_type attribute\n    of a log record, and resets the style after each message.\n    \"\"\"\n    def format(self, record):\n        log_type = getattr(record, 'log_type', 'default')\n        color = get_color(log_type)\n        message = record.getMessage()\n        return f\"{color}{message}{Style.RESET_ALL}\"\n</code></pre>"},{"location":"api/#src.logging_manager.LoggingMixin","title":"<code>LoggingMixin</code>","text":"<p>A mixin class to provide a standardized _log method. It expects the inheriting class to have 'verbose_logger', 'progress_logger', and an optional 'gui' attribute.</p> Source code in <code>src\\logging_manager.py</code> <pre><code>class LoggingMixin:\n    \"\"\"\n    A mixin class to provide a standardized _log method.\n    It expects the inheriting class to have 'verbose_logger', 'progress_logger',\n    and an optional 'gui' attribute.\n    \"\"\"\n    def _log(self, message, level='verbose', log_type='default'):\n        \"\"\"\n        Logs a message to the appropriate logger with a specified type for color-coding.\n\n        Args:\n            message (str): The message to log.\n            level (str): The logging level ('progress' or 'verbose').\n            log_type (str): The type of log message for color-coding (e.g., 'info', 'warning', 'error').\n        \"\"\"\n        extra = {'log_type': log_type}\n\n        if level == 'progress':\n            self.progress_logger.info(message, extra=extra)\n            if hasattr(self, 'gui') and self.gui:\n                self.gui.log(message, level='progress')\n        else:\n            self.verbose_logger.info(message, extra=extra)\n            if hasattr(self, 'gui') and self.gui and level != 'progress':\n                self.gui.log(message, level='verbose')\n</code></pre>"},{"location":"api/#src.logging_manager.setup_loggers","title":"<code>setup_loggers(process_id=None)</code>","text":"<p>Sets up two loggers: 1. 'progress': For high-level progress updates. Logs to console, a .progress.log file, and the main .log file. 2. 'verbose': For detailed, verbose output. Logs to console and the main .log file.</p> Source code in <code>src\\logging_manager.py</code> <pre><code>def setup_loggers(process_id=None):\n    \"\"\"\n    Sets up two loggers:\n    1. 'progress': For high-level progress updates. Logs to console, a .progress.log file, and the main .log file.\n    2. 'verbose': For detailed, verbose output. Logs to console and the main .log file.\n    \"\"\"\n    init(autoreset=True) # Initialize colorama\n    log_dir = 'logs'\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n\n    # Create a timestamp for the session. This ensures all log files in a\n    # single run share the same timestamp.\n    session_timestamp = datetime.now().strftime('%d-%m_%H-%M-%S')\n    if process_id:\n        session_timestamp = f\"{session_timestamp}_{process_id}\"\n\n    # --- Log Rotation ---\n    lock_file_path = os.path.join(log_dir, 'log_rotation.lock')\n\n    # Acquire lock for log rotation\n    while True:\n        try:\n            with open(lock_file_path, 'x'):\n                break  # Lock acquired\n        except FileExistsError:\n            time.sleep(0.1) # Wait for the lock to be released\n\n    try:\n        # Now managing two types of log files. The total limit is 30 (15 pairs).\n        log_files = [os.path.join(log_dir, f) for f in os.listdir(log_dir) if f.endswith('.log')]\n        log_files.sort(key=os.path.getctime)\n        # We check against 10 because each run creates a pair of files.\n        while len(log_files) &gt;= 30:\n            try:\n                old_log = log_files.pop(0)\n                base, _ = os.path.splitext(old_log)\n                progress_log = base + '.progress.log'\n\n                if os.path.exists(old_log):\n                    os.remove(old_log)\n                if os.path.exists(progress_log):\n                    os.remove(progress_log)\n            except OSError:\n                # File might be locked, skip.\n                pass\n    finally:\n        # Release lock\n        if os.path.exists(lock_file_path):\n            os.remove(lock_file_path)\n\n    # --- Filename Setup ---\n    progress_log_filename = os.path.join(log_dir, f'{session_timestamp}.progress.log')\n    main_log_filename = os.path.join(log_dir, f'{session_timestamp}.log')\n\n    # --- Formatters ---\n    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    console_formatter = ColorFormatter()\n\n    # --- Main Log File Handler (for both loggers) ---\n    main_file_handler = logging.FileHandler(main_log_filename, mode='a')\n    main_file_handler.setFormatter(file_formatter)\n\n    # --- Progress Logger ---\n    progress_logger = logging.getLogger('progress')\n    progress_logger.setLevel(logging.INFO)\n    # Remove existing handlers to prevent duplicates\n    for handler in progress_logger.handlers[:]:\n        progress_logger.removeHandler(handler)\n\n    # File handler for progress file\n    progress_file_handler = logging.FileHandler(progress_log_filename, mode='a')\n    progress_file_handler.setFormatter(file_formatter)\n    progress_logger.addHandler(progress_file_handler)\n\n    # Add main file handler\n    progress_logger.addHandler(main_file_handler)\n\n    # Stream handler for progress (console output)\n    progress_stream_handler = logging.StreamHandler()\n    progress_stream_handler.setFormatter(console_formatter)\n    progress_logger.addHandler(progress_stream_handler)\n    progress_logger.propagate = False\n\n    # --- Verbose Logger ---\n    verbose_logger = logging.getLogger('verbose')\n    verbose_logger.setLevel(logging.INFO)\n    # Remove existing handlers\n    for handler in verbose_logger.handlers[:]:\n        verbose_logger.removeHandler(handler)\n\n    # Add main file handler\n    verbose_logger.addHandler(main_file_handler)\n\n    # Stream handler for verbose (console output)\n    verbose_stream_handler = logging.StreamHandler()\n    verbose_stream_handler.setFormatter(console_formatter)\n    verbose_logger.addHandler(verbose_stream_handler)\n    verbose_logger.propagate = False\n\n    progress_logger.info(f\"--- Progress logging started for file: {os.path.abspath(progress_log_filename)} ---\")\n    verbose_logger.info(f\"--- Main logging started for file: {os.path.abspath(main_log_filename)} ---\")\n\n    return progress_logger, verbose_logger, session_timestamp\n</code></pre>"},{"location":"api/#src.logging_manager.shutdown_loggers","title":"<code>shutdown_loggers()</code>","text":"<p>Shuts down all logging handlers to release file locks.</p> Source code in <code>src\\logging_manager.py</code> <pre><code>def shutdown_loggers():\n    \"\"\"\n    Shuts down all logging handlers to release file locks.\n    \"\"\"\n    for name in ['progress', 'verbose']:\n        logger = logging.getLogger(name)\n        logger.info(\"--- Logging shutdown ---\")\n        for handler in logger.handlers[:]:\n            handler.close()\n            logger.removeHandler(handler)\n</code></pre>"},{"location":"api/#src.osparc_batch","title":"<code>osparc_batch</code>","text":""},{"location":"api/#src.osparc_batch.gui","title":"<code>gui</code>","text":""},{"location":"api/#src.osparc_batch.gui.BatchGUI","title":"<code>BatchGUI</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>A simple GUI for the oSPARC batch run.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>class BatchGUI(QWidget):\n    \"\"\"A simple GUI for the oSPARC batch run.\"\"\"\n    print_progress_requested = Signal()\n    stop_run_requested = Signal()\n    cancel_jobs_requested = Signal()\n\n    def __init__(self):\n        super().__init__()\n        self.init_ui()\n\n        logger.info(\"Initializing BatchGUI UI.\")\n    def init_ui(self):\n        \"\"\"Initializes the user interface components.\"\"\"\n        self.setWindowTitle(\"oSPARC Batch Runner\")\n        self.layout = QVBoxLayout()\n\n        self.button_layout = QHBoxLayout()\n        self.progress_button = QPushButton(\"Print Progress\")\n        self.progress_button.clicked.connect(self.print_progress_requested.emit)\n\n        self.force_stop_button = QPushButton(\"Force Stop\")\n        self.force_stop_button.clicked.connect(self.force_stop_run)\n\n        self.stop_and_cancel_button = QPushButton(\"Stop and Cancel Jobs\")\n        self.stop_and_cancel_button.clicked.connect(self.stop_and_cancel_jobs)\n\n        self.tray_button = QPushButton(\"Move to Tray\")\n        self.tray_button.clicked.connect(self.hide_to_tray)\n\n        self.button_layout.addWidget(self.progress_button)\n        self.button_layout.addWidget(self.force_stop_button)\n        self.button_layout.addWidget(self.stop_and_cancel_button)\n        self.button_layout.addWidget(self.tray_button)\n        self.layout.addLayout(self.button_layout)\n\n        self.setLayout(self.layout)\n\n        self.tray_icon = QSystemTrayIcon(self)\n        style = self.style()\n        icon = style.standardIcon(style.StandardPixmap.SP_ComputerIcon)\n        self.tray_icon.setIcon(icon)\n        self.tray_icon.setToolTip(\"oSPARC batch run is in progress...\")\n\n        tray_menu = QMenu(self)\n        show_action = QAction(\"Show\", self)\n        show_action.triggered.connect(self.show_from_tray)\n        tray_menu.addAction(show_action)\n\n        exit_action = QAction(\"Exit\", self)\n        exit_action.triggered.connect(self.close)\n        tray_menu.addAction(exit_action)\n\n        self.tray_icon.setContextMenu(tray_menu)\n        self.tray_icon.activated.connect(self.tray_icon_activated)\n\n    def force_stop_run(self):\n        \"\"\"Stops the main batch process immediately.\"\"\"\n        logger.info(\"Force stop button clicked.\")\n        self.force_stop_button.setEnabled(False)\n        self.stop_and_cancel_button.setEnabled(False)\n        self.stop_run_requested.emit()\n        QApplication.instance().quit()\n\n    def stop_and_cancel_jobs(self):\n        \"\"\"Stops the main batch process and cancels all running jobs.\"\"\"\n        logger.info(\"Stop and cancel jobs button clicked.\")\n        self.force_stop_button.setEnabled(False)\n        self.stop_and_cancel_button.setEnabled(False)\n        self.cancel_jobs_requested.emit()\n        # The worker will handle the rest, including quitting the app\n\n    def hide_to_tray(self):\n        \"\"\"Hide the main window and show the tray icon.\"\"\"\n        logger.info(\"Hiding window to system tray.\")\n        self.hide()\n        self.tray_icon.show()\n\n    def show_from_tray(self):\n        \"\"\"Show the main window and hide the tray icon.\"\"\"\n        logger.info(\"Showing window from system tray.\")\n        self.show()\n        self.tray_icon.hide()\n\n    def tray_icon_activated(self, reason):\n        \"\"\"Handle tray icon activation.\"\"\"\n        if reason == QSystemTrayIcon.ActivationReason.Trigger:\n            logger.debug(\"Tray icon clicked, showing window.\")\n            self.show_from_tray()\n\n    def closeEvent(self, event):\n        \"\"\"Handle the window close event.\"\"\"\n        logger.info(\"Window close event triggered.\")\n        self.force_stop_run()\n        event.accept()\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.init_ui","title":"<code>init_ui()</code>","text":"<p>Initializes the user interface components.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def init_ui(self):\n    \"\"\"Initializes the user interface components.\"\"\"\n    self.setWindowTitle(\"oSPARC Batch Runner\")\n    self.layout = QVBoxLayout()\n\n    self.button_layout = QHBoxLayout()\n    self.progress_button = QPushButton(\"Print Progress\")\n    self.progress_button.clicked.connect(self.print_progress_requested.emit)\n\n    self.force_stop_button = QPushButton(\"Force Stop\")\n    self.force_stop_button.clicked.connect(self.force_stop_run)\n\n    self.stop_and_cancel_button = QPushButton(\"Stop and Cancel Jobs\")\n    self.stop_and_cancel_button.clicked.connect(self.stop_and_cancel_jobs)\n\n    self.tray_button = QPushButton(\"Move to Tray\")\n    self.tray_button.clicked.connect(self.hide_to_tray)\n\n    self.button_layout.addWidget(self.progress_button)\n    self.button_layout.addWidget(self.force_stop_button)\n    self.button_layout.addWidget(self.stop_and_cancel_button)\n    self.button_layout.addWidget(self.tray_button)\n    self.layout.addLayout(self.button_layout)\n\n    self.setLayout(self.layout)\n\n    self.tray_icon = QSystemTrayIcon(self)\n    style = self.style()\n    icon = style.standardIcon(style.StandardPixmap.SP_ComputerIcon)\n    self.tray_icon.setIcon(icon)\n    self.tray_icon.setToolTip(\"oSPARC batch run is in progress...\")\n\n    tray_menu = QMenu(self)\n    show_action = QAction(\"Show\", self)\n    show_action.triggered.connect(self.show_from_tray)\n    tray_menu.addAction(show_action)\n\n    exit_action = QAction(\"Exit\", self)\n    exit_action.triggered.connect(self.close)\n    tray_menu.addAction(exit_action)\n\n    self.tray_icon.setContextMenu(tray_menu)\n    self.tray_icon.activated.connect(self.tray_icon_activated)\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.force_stop_run","title":"<code>force_stop_run()</code>","text":"<p>Stops the main batch process immediately.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def force_stop_run(self):\n    \"\"\"Stops the main batch process immediately.\"\"\"\n    logger.info(\"Force stop button clicked.\")\n    self.force_stop_button.setEnabled(False)\n    self.stop_and_cancel_button.setEnabled(False)\n    self.stop_run_requested.emit()\n    QApplication.instance().quit()\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.stop_and_cancel_jobs","title":"<code>stop_and_cancel_jobs()</code>","text":"<p>Stops the main batch process and cancels all running jobs.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def stop_and_cancel_jobs(self):\n    \"\"\"Stops the main batch process and cancels all running jobs.\"\"\"\n    logger.info(\"Stop and cancel jobs button clicked.\")\n    self.force_stop_button.setEnabled(False)\n    self.stop_and_cancel_button.setEnabled(False)\n    self.cancel_jobs_requested.emit()\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.hide_to_tray","title":"<code>hide_to_tray()</code>","text":"<p>Hide the main window and show the tray icon.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def hide_to_tray(self):\n    \"\"\"Hide the main window and show the tray icon.\"\"\"\n    logger.info(\"Hiding window to system tray.\")\n    self.hide()\n    self.tray_icon.show()\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.show_from_tray","title":"<code>show_from_tray()</code>","text":"<p>Show the main window and hide the tray icon.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def show_from_tray(self):\n    \"\"\"Show the main window and hide the tray icon.\"\"\"\n    logger.info(\"Showing window from system tray.\")\n    self.show()\n    self.tray_icon.hide()\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.tray_icon_activated","title":"<code>tray_icon_activated(reason)</code>","text":"<p>Handle tray icon activation.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def tray_icon_activated(self, reason):\n    \"\"\"Handle tray icon activation.\"\"\"\n    if reason == QSystemTrayIcon.ActivationReason.Trigger:\n        logger.debug(\"Tray icon clicked, showing window.\")\n        self.show_from_tray()\n</code></pre>"},{"location":"api/#src.osparc_batch.gui.BatchGUI.closeEvent","title":"<code>closeEvent(event)</code>","text":"<p>Handle the window close event.</p> Source code in <code>src\\osparc_batch\\gui.py</code> <pre><code>def closeEvent(self, event):\n    \"\"\"Handle the window close event.\"\"\"\n    logger.info(\"Window close event triggered.\")\n    self.force_stop_run()\n    event.accept()\n</code></pre>"},{"location":"api/#src.osparc_batch.runner","title":"<code>runner</code>","text":""},{"location":"api/#src.osparc_batch.runner.setup_console_logging","title":"<code>setup_console_logging()</code>","text":"<p>Sets up a basic console logger with color.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def setup_console_logging():\n    \"\"\"Sets up a basic console logger with color.\"\"\"\n    colorama.init(autoreset=True)\n    logger = logging.getLogger('osparc_batch')\n    logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter('%(message)s'))\n    logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.setup_job_logging","title":"<code>setup_job_logging(base_dir, job_id)</code>","text":"<p>Set up a unique log file for each job in a specific subdirectory.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def setup_job_logging(base_dir: str, job_id: str):\n    \"\"\"Set up a unique log file for each job in a specific subdirectory.\"\"\"\n    log_dir = Path(base_dir) / \"logs\" / \"osparc_submission_logs\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    log_file_path = log_dir / f\"job_{job_id}.log\"\n\n    job_logger = logging.getLogger(f'job_{job_id}')\n    job_logger.setLevel(logging.INFO)\n    job_logger.propagate = False\n\n    if job_logger.hasHandlers():\n        job_logger.handlers.clear()\n\n    file_handler = logging.FileHandler(log_file_path, mode='w')\n    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n    job_logger.addHandler(file_handler)\n\n    return job_logger\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.find_input_files","title":"<code>find_input_files(config)</code>","text":"<p>Finds solver input files (.h5), identifies the latest group based on creation time and configuration, and cleans up older, unselected files.</p> <p>Supports both far-field and near-field study types: - Far-field: Uses frequencies_mhz and finds files per phantom/frequency - Near-field: Uses antenna_config keys and finds files per phantom/frequency/placement</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def find_input_files(config) -&gt; list[Path]:\n    \"\"\"\n    Finds solver input files (.h5), identifies the latest group based on creation time and configuration,\n    and cleans up older, unselected files.\n\n    Supports both far-field and near-field study types:\n    - Far-field: Uses frequencies_mhz and finds files per phantom/frequency\n    - Near-field: Uses antenna_config keys and finds files per phantom/frequency/placement\n    \"\"\"\n    main_logger.info(f\"{colorama.Fore.MAGENTA}--- Searching for input files based on configuration ---\")\n    results_base_dir = Path(config.base_dir) / \"results\"\n    study_type = config.get_setting('study_type')\n    phantoms = config.get_setting('phantoms', [])\n\n    # Get frequencies based on study type\n    if study_type == 'far_field':\n        frequencies = config.get_setting('frequencies_mhz', [])\n        if not frequencies:\n            raise ValueError(\"Far-field config must specify 'frequencies_mhz'.\")\n    elif study_type == 'near_field':\n        # Near-field uses antenna_config keys as frequencies\n        antenna_config = config.get_setting('antenna_config', {})\n        if not antenna_config:\n            raise ValueError(\"Near-field config must specify 'antenna_config'.\")\n        frequencies = [int(freq_str) for freq_str in antenna_config.keys()]\n    else:\n        raise ValueError(f\"Unknown study_type: {study_type}\")\n\n    if not all([study_type, phantoms]):\n        raise ValueError(\"Config must specify 'study_type' and 'phantoms'.\")\n\n    all_input_files = []\n    for phantom in phantoms:\n        for freq in frequencies:\n            if study_type == 'far_field':\n                all_input_files.extend(_find_far_field_input_files(\n                    config, results_base_dir, phantom, freq\n                ))\n            elif study_type == 'near_field':\n                all_input_files.extend(_find_near_field_input_files(\n                    config, results_base_dir, phantom, freq\n                ))\n\n    if not all_input_files:\n        main_logger.error(f\"{colorama.Fore.RED}ERROR: Could not find any input files to process.\")\n        sys.exit(1)\n\n    main_logger.info(f\"{colorama.Fore.GREEN}--- Found a total of {len(all_input_files)} input files to process. ---\")\n    return all_input_files\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.get_osparc_client_config","title":"<code>get_osparc_client_config(config, osparc_module)</code>","text":"<p>Initializes and returns the oSPARC client configuration.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def get_osparc_client_config(config, osparc_module):\n    \"\"\"Initializes and returns the oSPARC client configuration.\"\"\"\n    creds = config.get_osparc_credentials()\n    if not all(k in creds for k in ['api_key', 'api_secret', 'api_server']):\n        raise ValueError(\"Missing oSPARC credentials in configuration.\")\n\n    temp_dir = Path(config.base_dir) / \"tmp_download\"\n    temp_dir.mkdir(exist_ok=True)\n\n    client_config = osparc_module.Configuration(\n        host=creds['api_server'],\n        username=creds['api_key'],\n        password=creds['api_secret'],\n    )\n    client_config.temp_folder_path = str(temp_dir)\n    return client_config\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.submit_job","title":"<code>submit_job(input_file_path, client_cfg, solver_key, solver_version, osparc_module)</code>","text":"<p>Submits a single job to oSPARC and returns the job and solver objects.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def submit_job(input_file_path: Path, client_cfg, solver_key: str, solver_version: str, osparc_module):\n    \"\"\"Submits a single job to oSPARC and returns the job and solver objects.\"\"\"\n    with osparc_module.ApiClient(client_cfg) as api_client:\n        files_api = osparc_module.FilesApi(api_client)\n        solvers_api = osparc_module.SolversApi(api_client)\n\n        input_file_osparc = files_api.upload_file(file=str(input_file_path))\n        solver = solvers_api.get_solver_release(solver_key, solver_version)\n\n        job = solvers_api.create_job(\n            solver.id,\n            solver.version,\n            job_inputs=osparc_module.JobInputs({\"input_1\": input_file_osparc})\n        )\n\n        if not job.id:\n            raise RuntimeError(\"oSPARC API did not return a job ID after creation.\")\n\n        solvers_api.start_job(solver.id, solver.version, job.id)\n        return job, solver\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.download_and_process_results","title":"<code>download_and_process_results(job, solver, client_cfg, input_file_path, osparc_module, status_callback=None)</code>","text":"<p>Downloads and processes the results for a single job.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def download_and_process_results(job, solver, client_cfg, input_file_path, osparc_module, status_callback=None):\n    \"\"\"Downloads and processes the results for a single job.\"\"\"\n    job_logger = logging.getLogger(f'job_{job.id}')\n    try:\n        with osparc_module.ApiClient(client_cfg) as api_client:\n            files_api = osparc_module.FilesApi(api_client)\n            solvers_api = osparc_module.SolversApi(api_client)\n\n            job_logger.info(f\"--- Downloading results for job {job.id} ---\")\n            if status_callback:\n                status_callback.emit(job.id, \"DOWNLOADING\")\n            outputs = solvers_api.get_job_outputs(solver.id, solver.version, job.id)\n\n            output_dir = input_file_path.parent\n\n            for output_name, result_file in outputs.results.items():\n                job_logger.info(f\"Downloading {output_name} for job {job.id}...\")\n\n                download_path = files_api.download_file(file_id=result_file.id)\n\n                if result_file.filename.endswith('.zip'):\n                    job_logger.info(f\"Extracting {result_file.filename} to {output_dir}\")\n                    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n                        zip_ref.extractall(output_dir)\n\n                        # --- Enhanced Log File Handling ---\n                        uuid = input_file_path.stem.replace('_Input', '')\n                        extracted_files = zip_ref.namelist()\n\n                        for filename in extracted_files:\n                            if filename.endswith('.log'):\n                                extracted_path = output_dir / filename\n                                if \"input.log\" in filename:\n                                    new_name = f\"iSolve-output-{uuid}.log\"\n                                else:\n                                    new_name = f\"{uuid}_AxLog.log\"\n\n                                final_log_path = output_dir / new_name\n                                shutil.move(extracted_path, final_log_path)\n                                job_logger.info(f\"Renamed and moved log file to {final_log_path}\")\n\n                    os.remove(download_path)\n                else:\n                    if \"output.h5\" in result_file.filename:\n                        output_filename = input_file_path.stem.replace('_Input', '_Output') + \".h5\"\n                    else:\n                        output_filename = result_file.filename\n\n                    final_path = output_dir / output_filename\n                    shutil.move(download_path, final_path)\n                    job_logger.info(f\"Saved {output_name} to {final_path}\")\n\n                if status_callback:\n                    status_callback.emit(job.id, \"COMPLETED\")\n\n    except Exception as e:\n        job_logger.error(f\"Could not retrieve results for job {job.id}: {e}\\n{traceback.format_exc()}\")\n        if status_callback:\n            status_callback.emit(job.id, \"FAILED\")\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.get_progress_report","title":"<code>get_progress_report(input_files, job_statuses, file_to_job_id)</code>","text":"<p>Generates a status summary and a colored file tree string.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def get_progress_report(input_files, job_statuses, file_to_job_id) -&gt; str:\n    \"\"\"Generates a status summary and a colored file tree string.\"\"\"\n    report_lines = []\n\n    status_counts = defaultdict(int)\n    for status_tuple in job_statuses.values():\n        status_str = status_tuple[0] if isinstance(status_tuple, tuple) else status_tuple\n        state = status_str.split(\" \")[0]\n        status_counts[state] += 1\n    summary = \" | \".join(f\"{state}: {count}\" for state, count in sorted(status_counts.items()))\n    report_lines.append(f\"\\n{colorama.Fore.BLUE}--- Progress Summary ---\\n{summary}\\n{colorama.Style.RESET_ALL}\")\n\n    tree = {}\n    if not input_files:\n        return \"\\n\".join(report_lines)\n\n    # --- Optimized Path Handling ---\n    try:\n        first_path_parts = input_files[0].parts\n        results_index = first_path_parts.index('results')\n        base_path = Path(*first_path_parts[:results_index+1])\n    except (ValueError, IndexError):\n        # Fallback for safety, though not expected with the current structure\n        common_path_str = os.path.commonpath([str(p.parent) for p in input_files])\n        base_path = Path(common_path_str)\n\n    for file_path in input_files:\n        try:\n            relative_path = file_path.relative_to(base_path)\n            parts = list(relative_path.parts)\n            if not parts:\n                continue\n\n            current_level = tree\n            for part in parts[:-1]:\n                current_level = current_level.setdefault(part, {})\n\n            filename = parts[-1]\n            current_level[filename] = file_to_job_id.get(file_path)\n\n        except (IndexError, ValueError) as e:\n            report_lines.append(f\"{colorama.Fore.RED}Could not process path {file_path}: {e}{colorama.Style.RESET_ALL}\")\n\n    def build_tree_recursive(node, prefix=\"\"):\n        def sort_key(item):\n            match = re.match(r'(\\d+)', item)\n            if match:\n                return (1, int(match.group(1)))\n            return (0, item)\n\n        items = sorted(node.keys(), key=sort_key)\n        for i, item in enumerate(items):\n            is_last = i == len(items) - 1\n            connector = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n\n            if isinstance(node[item], dict):\n                report_lines.append(f\"{prefix}{connector}{colorama.Fore.WHITE}{item}\")\n                new_prefix = prefix + (\"    \" if is_last else \"\u2502   \")\n                build_tree_recursive(node[item], new_prefix)\n            else:\n                job_id = node[item]\n                status_tuple = job_statuses.get(job_id, (\"UNKNOWN\", time.time()))\n                status_str, start_time = status_tuple if isinstance(status_tuple, tuple) else (status_tuple, time.time())\n\n                elapsed_time = time.time() - start_time\n                timer_str = f\" ({elapsed_time:.0f}s)\"\n\n                status = status_str.split(\" \")[0]\n                color = STATUS_COLORS.get(status, colorama.Fore.WHITE)\n                colored_text = f\"{color}{item} (oSPARC Job: {job_id}, Status: {status_str}{timer_str}){colorama.Style.RESET_ALL}\"\n                report_lines.append(f\"{prefix}{connector}{colored_text}\")\n\n    report_lines.append(f\"{colorama.Fore.BLUE}--- File Status Tree ---{colorama.Style.RESET_ALL}\")\n    build_tree_recursive(tree)\n    report_lines.append(f\"{colorama.Fore.BLUE}------------------------{colorama.Style.RESET_ALL}\\n\")\n\n    return \"\\n\".join(report_lines)\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.main_process_logic","title":"<code>main_process_logic(worker)</code>","text":"<p>The main logic of the batch run, now running in a QThread.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def main_process_logic(worker: 'Worker'):\n    \"\"\"The main logic of the batch run, now running in a QThread.\"\"\"\n    import osparc as osparc_module\n\n    try:\n        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n        worker.client_cfg = get_osparc_client_config(worker.config, osparc_module)\n\n        solver_key = \"simcore/services/comp/isolve-gpu\"\n        solver_version = \"2.2.212\"\n\n        main_logger.info(f\"{colorama.Fore.MAGENTA}--- Submitting Jobs to oSPARC in Parallel ---\")\n        worker.running_jobs = {}\n        with ProcessPoolExecutor(max_workers=min(len(worker.input_files), 61) or 1) as executor:\n            future_to_file = {\n                executor.submit(_submit_job_in_process, fp, worker.client_cfg, solver_key, solver_version): fp\n                for fp in worker.input_files\n            }\n            for future in as_completed(future_to_file):\n                file_path = future_to_file[future]\n                try:\n                    result = future.result()\n                    if result:\n                        job, solver = result\n                        worker.running_jobs[file_path] = (job, solver)\n                        setup_job_logging(base_dir, job.id)\n                        job_logger = logging.getLogger(f'job_{job.id}')\n                        job_logger.info(f\"Job {job.id} submitted for input file {file_path.name} at path {file_path}.\")\n                except Exception as exc:\n                    main_logger.error(f'ERROR: Submitting job for {file_path.name} generated an exception: {exc}\\n{traceback.format_exc()}')\n\n        if not worker.running_jobs:\n            main_logger.error(\"ERROR: No jobs were successfully submitted. Exiting.\")\n            worker.finished.emit()\n            return\n\n        main_logger.info(f\"{colorama.Fore.MAGENTA}--- Polling for Job Completion and Downloading Results ---\")\n        worker.job_statuses = {job.id: (\"PENDING\", time.time()) for _, (job, _) in worker.running_jobs.items()}\n        worker.file_to_job_id = {fp: j.id for fp, (j, s) in worker.running_jobs.items()}\n        worker.downloaded_jobs = set()\n\n        worker.timer.start(5000)\n\n    except Exception as e:\n        main_logger.error(f\"\\nCRITICAL ERROR in main process: {e}\\n{traceback.format_exc()}\")\n        worker.finished.emit()\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.clear_log_directory","title":"<code>clear_log_directory(base_dir)</code>","text":"<p>Deletes all files in the osparc_submission_logs directory.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def clear_log_directory(base_dir: str):\n    \"\"\"Deletes all files in the osparc_submission_logs directory.\"\"\"\n    log_dir = Path(base_dir) / \"logs\" / \"osparc_submission_logs\"\n    if log_dir.exists():\n        main_logger.info(f\"--- Clearing log directory: {log_dir} ---\")\n        for item in log_dir.iterdir():\n            if item.is_file():\n                try:\n                    item.unlink()\n                except OSError as e:\n                    main_logger.error(f\"Error deleting file {item}: {e}\")\n            elif item.is_dir():\n                try:\n                    shutil.rmtree(item)\n                except OSError as e:\n                    main_logger.error(f\"Error deleting directory {item}: {e}\")\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.clear_temp_download_directory","title":"<code>clear_temp_download_directory(base_dir)</code>","text":"<p>Deletes the temporary download directory.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def clear_temp_download_directory(base_dir: str):\n    \"\"\"Deletes the temporary download directory.\"\"\"\n    temp_dir = Path(base_dir) / \"tmp_download\"\n    if temp_dir.exists():\n        main_logger.info(f\"--- Clearing temporary download directory: {temp_dir} ---\")\n        try:\n            shutil.rmtree(temp_dir)\n        except OSError as e:\n            main_logger.error(f\"Error deleting directory {temp_dir}: {e}\")\n</code></pre>"},{"location":"api/#src.osparc_batch.runner.main","title":"<code>main(config_path)</code>","text":"<p>Main entry point: sets up and starts the GUI and the main logic process.</p> Source code in <code>src\\osparc_batch\\runner.py</code> <pre><code>def main(config_path: str):\n    \"\"\"Main entry point: sets up and starts the GUI and the main logic process.\"\"\"\n    import sys\n    from PySide6.QtWidgets import QApplication\n    from src.config import Config\n\n    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n    clear_log_directory(base_dir)\n\n    config = Config(base_dir, config_path)\n    input_files = find_input_files(config)\n\n    app = QApplication.instance() or QApplication(sys.argv)\n\n    thread = QThread()\n    worker = Worker(\n        config_path=config_path,\n        logger=main_logger,\n        get_osparc_client_config_func=get_osparc_client_config,\n        download_and_process_results_func=download_and_process_results,\n        get_progress_report_func=get_progress_report,\n        main_process_logic_func=main_process_logic,\n    )\n    worker.config = config\n    worker.input_files = input_files\n    worker.moveToThread(thread)\n\n    gui = BatchGUI()\n\n    # Connect signals and slots\n    thread.started.connect(worker.run)\n    worker.finished.connect(thread.quit)\n    worker.finished.connect(worker.deleteLater)\n    thread.finished.connect(thread.deleteLater)\n\n    worker.progress.connect(main_logger.info)\n    gui.print_progress_requested.connect(worker.request_progress_report)\n    gui.stop_run_requested.connect(worker.stop)\n    gui.cancel_jobs_requested.connect(worker.cancel_jobs)\n\n    worker.finished.connect(app.quit)\n\n    thread.start()\n    gui.show()\n    app.exec()\n\n    if thread.isRunning():\n        thread.quit()\n        thread.wait()\n\n    clear_temp_download_directory(base_dir)\n</code></pre>"},{"location":"api/#src.osparc_batch.worker","title":"<code>worker</code>","text":""},{"location":"api/#src.osparc_batch.worker.Worker","title":"<code>Worker</code>","text":"<p>               Bases: <code>QObject</code></p> <p>Worker thread to run the oSPARC batch main logic and manage polling/downloads.</p> Source code in <code>src\\osparc_batch\\worker.py</code> <pre><code>class Worker(QObject):\n    \"\"\"Worker thread to run the oSPARC batch main logic and manage polling/downloads.\"\"\"\n\n    finished = Signal()\n    progress = Signal(str)\n    status_update_requested = Signal(str, str)\n\n    def __init__(\n        self,\n        config_path: str,\n        logger: logging.Logger,\n        get_osparc_client_config_func: Callable[..., Any],\n        download_and_process_results_func: Callable[..., Any],\n        get_progress_report_func: Callable[..., str],\n        main_process_logic_func: Callable[..., Any],\n    ):\n        super().__init__()\n        # Inputs / injected dependencies\n        self.config_path = config_path\n        self.logger = logger\n        self.get_osparc_client_config = get_osparc_client_config_func\n        self.download_and_process_results = download_and_process_results_func\n        self.get_progress_report = get_progress_report_func\n        self.main_process_logic = main_process_logic_func\n\n        # Runtime state\n        self.config = None\n        self.stop_requested = False\n        self.input_files = []\n        self.job_statuses = {}\n        self.file_to_job_id = {}\n        self.running_jobs = {}\n        self.downloaded_jobs = set()\n        self.jobs_being_downloaded = set()\n        self.file_retries = {} # Correct: Associate retries with the file path\n        self.client_cfg = None\n\n        # Executors and timers\n        self.download_executor = ThreadPoolExecutor(max_workers=4)\n        self.timer = QTimer(self)\n        self.timer.timeout.connect(self._check_jobs_status)\n        self.status_update_requested.connect(self._update_job_status)\n\n    def run(self):\n        \"\"\"Starts the long-running task.\"\"\"\n        self.main_process_logic(self)\n\n    def _download_job_in_thread(self, job, solver, file_path):\n        \"\"\"Helper to run a single download in a thread.\"\"\"\n        import osparc as osparc_module\n        try:\n            client_cfg = self.get_osparc_client_config(self.config, osparc_module)\n            self.download_and_process_results(\n                job, solver, client_cfg, file_path, osparc_module, self.status_update_requested\n            )\n        except Exception as e:\n            job_logger = logging.getLogger(f'job_{job.id}')\n            job_logger.error(f\"Error during download for job {job.id}: {e}\\n{traceback.format_exc()}\")\n            self.status_update_requested.emit(job.id, \"FAILED\")\n        finally:\n            if job.id in self.jobs_being_downloaded:\n                self.jobs_being_downloaded.remove(job.id)\n            self.downloaded_jobs.add(job.id)\n\n    def _check_jobs_status(self):\n        \"\"\"Periodically checks the status of running jobs.\"\"\"\n        if self.stop_requested or len(self.downloaded_jobs) &gt;= len(self.running_jobs):\n            if self.timer.isActive():\n                self.timer.stop()\n            self.download_executor.shutdown()\n            self.logger.info(\"\\n--- All Jobs Finished or Stopped ---\")\n            final_report = self.get_progress_report(self.input_files, self.job_statuses, self.file_to_job_id)\n            self.logger.info(final_report)\n            self.finished.emit()\n            return\n\n        import osparc as osparc_module\n        with osparc_module.ApiClient(self.client_cfg) as api_client:\n            solvers_api = osparc_module.SolversApi(api_client)\n            # Iterate over a copy of the items, as the dictionary may be modified during the loop\n            for file_path, (job, solver) in list(self.running_jobs.items()):\n                if job.id in self.downloaded_jobs or job.id in self.jobs_being_downloaded:\n                    continue\n\n                try:\n                    status = solvers_api.inspect_job(solver.id, solver.version, job.id)\n                    job_logger = logging.getLogger(f'job_{job.id}')\n                    new_status_str = f\"{status.state} ({status.progress}%)\"\n\n                    current_status_str, _ = self.job_statuses.get(job.id, (\"UNKNOWN\", time.time()))\n\n                    if status.state == current_status_str.split(\" \")[0]:\n                        self.job_statuses[job.id] = (new_status_str, self.job_statuses[job.id][1])\n                    else:\n                        self.job_statuses[job.id] = (new_status_str, time.time())\n                        job_logger.info(f\"Status update: {new_status_str}\")\n\n                    if status.state == \"SUCCESS\":\n                        self.logger.info(f\"\\nJob {job.id} for {file_path.name} finished. Starting download...\")\n                        self.jobs_being_downloaded.add(job.id)\n                        self.job_statuses[job.id] = (\"DOWNLOADING\", time.time())\n                        self.download_executor.submit(self._download_job_in_thread, job, solver, file_path)\n\n                    elif status.state == \"FAILED\":\n                        job_logger.error(f\"Job {job.id} for {file_path.name} has failed.\")\n\n                        retries = self.file_retries.get(file_path, 0)\n                        if retries &lt; 3:\n                            new_retry_count = retries + 1\n                            self.file_retries[file_path] = new_retry_count\n                            job_logger.warning(f\"Retrying job for {file_path.name} (attempt {new_retry_count}/3)...\")\n                            self.status_update_requested.emit(job.id, f\"RETRYING ({new_retry_count}/3)\")\n                            self._resubmit_job(file_path)\n                        else:\n                            job_logger.error(f\"Job for {file_path.name} has failed after {retries} retries. Giving up.\")\n                            self.downloaded_jobs.add(job.id)\n                            if self.job_statuses.get(job.id, (\"dummy\", 0))[0] != \"FAILED\":\n                                self.job_statuses[job.id] = (\"FAILED\", time.time())\n\n                except Exception as exc:\n                    job_logger = logging.getLogger(f'job_{job.id}')\n                    job_logger.error(f\"Error inspecting job {job.id}: {exc}\\n{traceback.format_exc()}\")\n                    self.job_statuses[job.id] = (\"FAILED\", time.time())\n                    self.downloaded_jobs.add(job.id)\n\n    def _resubmit_job(self, file_path):\n        \"\"\"Resubmits a failed job.\"\"\"\n        import osparc as osparc_module\n        from src.osparc_batch.runner import _submit_job_in_process, setup_job_logging\n\n        try:\n            base_dir = self.config.base_dir\n            solver_key = \"simcore/services/comp/isolve-gpu\"\n            solver_version = \"2.2.212\"\n\n            # --- State Cleanup for the Old Job ---\n            old_job_id = self.file_to_job_id.pop(file_path, None)\n            if old_job_id:\n                self.running_jobs.pop(file_path, None)\n                self.job_statuses.pop(old_job_id, None)\n                self.downloaded_jobs.discard(old_job_id)\n\n            # --- Submit New Job ---\n            job, solver = _submit_job_in_process(file_path, self.client_cfg, solver_key, solver_version)\n            if job:\n                setup_job_logging(base_dir, job.id)\n                job_logger = logging.getLogger(f'job_{job.id}')\n                job_logger.info(f\"Resubmitted as new job {job.id} for input file {file_path.name}.\")\n\n                # --- State Update for the New Job ---\n                self.running_jobs[file_path] = (job, solver)\n                self.file_to_job_id[file_path] = job.id\n                self.job_statuses[job.id] = (\"PENDING\", time.time())\n                # The retry count is already updated in _check_jobs_status using self.file_retries\n            else:\n                self.logger.error(f\"Failed to resubmit job for {file_path.name}. The job will be marked as FAILED.\")\n                if old_job_id:\n                    self.file_to_job_id[file_path] = old_job_id \n                    self.job_statuses[old_job_id] = (\"FAILED\", time.time())\n                    self.downloaded_jobs.add(old_job_id)\n\n        except Exception as e:\n            self.logger.error(f\"Critical error during job resubmission for {file_path.name}: {e}\\n{traceback.format_exc()}\")\n\n\n    @Slot(str, str)\n    def _update_job_status(self, job_id, status):\n        \"\"\"Thread-safe method to update job status.\"\"\"\n        self.job_statuses[job_id] = (status, time.time())\n\n    @Slot()\n    def request_progress_report(self):\n        \"\"\"Handles the request for a progress report.\"\"\"\n        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        self.logger.info(f\"--- Progress report requested by user at {timestamp} ---\")\n        if not self.input_files:\n            self.logger.info(\"No input files found yet. The process may still be initializing.\")\n            return\n        report = self.get_progress_report(self.input_files, self.job_statuses, self.file_to_job_id)\n        self.logger.info(report)\n\n    @Slot()\n    def stop(self):\n        \"\"\"Requests the worker to stop.\"\"\"\n        self.logger.info(\"--- Stop requested by user ---\")\n        self.stop_requested = True\n        if self.timer.isActive():\n            self.timer.stop()\n        # The cancel_futures parameter is available in Python 3.9+\n        # For older versions, this will gracefully shut down.\n        self.download_executor.shutdown(wait=False, cancel_futures=True)\n        self.finished.emit()\n        if self.thread():\n            self.thread().quit()\n            self.thread().wait()\n\n    @Slot()\n    def cancel_jobs(self):\n        \"\"\"Cancels all running jobs and then stops the worker.\"\"\"\n        self.logger.info(\"--- Cancellation of all jobs requested by user ---\")\n        self.stop_requested = True\n        if self.timer.isActive():\n            self.timer.stop()\n\n        # Run the cancel_all_jobs.py script\n        try:\n            script_path = \"scripts/cancel_all_jobs.py\"\n            config_path = self.config_path\n            max_jobs = len(self.running_jobs)\n            self.logger.info(f\"Running cancellation script for {max_jobs} jobs...\")\n            subprocess.run(\n                [\n                    \"python\",\n                    script_path,\n                    \"--config\",\n                    config_path,\n                    \"--max-jobs\",\n                    str(max_jobs),\n                ],\n                check=True,\n            )\n            self.logger.info(\"--- Job cancellation script finished ---\")\n        except FileNotFoundError:\n            self.logger.error(f\"Error: The script '{script_path}' was not found.\")\n        except subprocess.CalledProcessError as e:\n            self.logger.error(f\"Error running cancellation script: {e}\")\n        except Exception as e:\n            self.logger.error(f\"An unexpected error occurred during job cancellation: {e}\")\n        finally:\n            self.download_executor.shutdown(wait=False, cancel_futures=True)\n            self.finished.emit()\n            if self.thread():\n                self.thread().quit()\n                self.thread().wait()\n</code></pre>"},{"location":"api/#src.osparc_batch.worker.Worker.run","title":"<code>run()</code>","text":"<p>Starts the long-running task.</p> Source code in <code>src\\osparc_batch\\worker.py</code> <pre><code>def run(self):\n    \"\"\"Starts the long-running task.\"\"\"\n    self.main_process_logic(self)\n</code></pre>"},{"location":"api/#src.osparc_batch.worker.Worker.request_progress_report","title":"<code>request_progress_report()</code>","text":"<p>Handles the request for a progress report.</p> Source code in <code>src\\osparc_batch\\worker.py</code> <pre><code>@Slot()\ndef request_progress_report(self):\n    \"\"\"Handles the request for a progress report.\"\"\"\n    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n    self.logger.info(f\"--- Progress report requested by user at {timestamp} ---\")\n    if not self.input_files:\n        self.logger.info(\"No input files found yet. The process may still be initializing.\")\n        return\n    report = self.get_progress_report(self.input_files, self.job_statuses, self.file_to_job_id)\n    self.logger.info(report)\n</code></pre>"},{"location":"api/#src.osparc_batch.worker.Worker.stop","title":"<code>stop()</code>","text":"<p>Requests the worker to stop.</p> Source code in <code>src\\osparc_batch\\worker.py</code> <pre><code>@Slot()\ndef stop(self):\n    \"\"\"Requests the worker to stop.\"\"\"\n    self.logger.info(\"--- Stop requested by user ---\")\n    self.stop_requested = True\n    if self.timer.isActive():\n        self.timer.stop()\n    # The cancel_futures parameter is available in Python 3.9+\n    # For older versions, this will gracefully shut down.\n    self.download_executor.shutdown(wait=False, cancel_futures=True)\n    self.finished.emit()\n    if self.thread():\n        self.thread().quit()\n        self.thread().wait()\n</code></pre>"},{"location":"api/#src.osparc_batch.worker.Worker.cancel_jobs","title":"<code>cancel_jobs()</code>","text":"<p>Cancels all running jobs and then stops the worker.</p> Source code in <code>src\\osparc_batch\\worker.py</code> <pre><code>@Slot()\ndef cancel_jobs(self):\n    \"\"\"Cancels all running jobs and then stops the worker.\"\"\"\n    self.logger.info(\"--- Cancellation of all jobs requested by user ---\")\n    self.stop_requested = True\n    if self.timer.isActive():\n        self.timer.stop()\n\n    # Run the cancel_all_jobs.py script\n    try:\n        script_path = \"scripts/cancel_all_jobs.py\"\n        config_path = self.config_path\n        max_jobs = len(self.running_jobs)\n        self.logger.info(f\"Running cancellation script for {max_jobs} jobs...\")\n        subprocess.run(\n            [\n                \"python\",\n                script_path,\n                \"--config\",\n                config_path,\n                \"--max-jobs\",\n                str(max_jobs),\n            ],\n            check=True,\n        )\n        self.logger.info(\"--- Job cancellation script finished ---\")\n    except FileNotFoundError:\n        self.logger.error(f\"Error: The script '{script_path}' was not found.\")\n    except subprocess.CalledProcessError as e:\n        self.logger.error(f\"Error running cancellation script: {e}\")\n    except Exception as e:\n        self.logger.error(f\"An unexpected error occurred during job cancellation: {e}\")\n    finally:\n        self.download_executor.shutdown(wait=False, cancel_futures=True)\n        self.finished.emit()\n        if self.thread():\n            self.thread().quit()\n            self.thread().wait()\n</code></pre>"},{"location":"api/#src.profiler","title":"<code>profiler</code>","text":""},{"location":"api/#src.profiler.Profiler","title":"<code>Profiler</code>","text":"<p>A profiler to track execution time, estimate remaining time, and manage study phases.</p> Source code in <code>src\\profiler.py</code> <pre><code>class Profiler:\n    \"\"\"\n    A profiler to track execution time, estimate remaining time, and manage study phases.\n    \"\"\"\n    def __init__(self, execution_control, profiling_config, study_type, config_path):\n        self.execution_control = execution_control\n        self.profiling_config = profiling_config\n        self.study_type = study_type\n        self.config_path = config_path\n\n        self.phase_weights = self._calculate_phase_weights()\n        self.subtask_times = defaultdict(list)\n        self.subtask_stack = []\n\n        self.total_simulations = 0\n        self.completed_simulations = 0\n        self.total_projects = 0\n        self.current_project = 0\n        self.completed_phases = set()\n\n        self.start_time = time.monotonic()\n        self.current_phase = None\n        self.phase_start_time = None\n        self.run_phase_total_duration = 0\n\n    def _calculate_phase_weights(self):\n        \"\"\"Calculate the relative weight of each enabled study phase.\"\"\"\n        weights = {}\n        total_weight = 0\n        for phase in ['setup', 'run', 'extract']:\n            if self.execution_control.get(f'do_{phase}', False):\n                weight = self.profiling_config.get(f'avg_{phase}_time', 1)\n                weights[phase] = weight\n                total_weight += weight\n\n        if total_weight &gt; 0:\n            for phase in weights:\n                weights[phase] /= total_weight\n        return weights\n\n    def set_total_simulations(self, total):\n        self.total_simulations = total\n\n    def set_project_scope(self, total_projects):\n        self.total_projects = total_projects\n\n    def set_current_project(self, project_index):\n        self.current_project = project_index\n\n    def start_stage(self, phase_name, total_stages=1):\n        self.current_phase = phase_name\n        self.phase_start_time = time.monotonic()\n        self.completed_stages_in_phase = 0\n        self.total_stages_in_phase = total_stages\n\n    def end_stage(self):\n        if self.phase_start_time:\n            elapsed = time.monotonic() - self.phase_start_time\n            self.profiling_config[f'avg_{self.current_phase}_time'] = elapsed\n        self.current_phase = None\n\n    def complete_run_phase(self):\n        self.run_phase_total_duration = sum(self.subtask_times.get('run_simulation_total', [0]))\n\n    def get_weighted_progress(self, phase_name, phase_progress_ratio):\n        \"\"\"Calculates the overall progress based on the current phase's weight and progress.\"\"\"\n        progress = 0\n        # Add the weight of all completed phases\n        for p, w in self.phase_weights.items():\n            if p == phase_name:\n                progress += w * phase_progress_ratio\n                break\n            progress += w\n        return progress * 100\n\n    def get_subtask_estimate(self, task_name):\n        \"\"\"\n        Gets the estimated time for a specific subtask from the profiling config.\n        Returns a default value if not found.\n        \"\"\"\n        return self.profiling_config.get(f'avg_{task_name}', 1.0) # Default to 1 second\n\n    def get_time_remaining(self, current_stage_progress=0.0):\n        \"\"\"\n        Estimates the total time remaining for the study by considering completed phases,\n        the progress of the current phase, and the estimated time for future phases.\n        \"\"\"\n        if not self.current_phase:\n            return 0\n\n        # Find the index of the current phase in the execution order\n        ordered_phases = [p for p in ['setup', 'run', 'extract'] if self.execution_control.get(f'do_{p}', False)]\n        try:\n            current_phase_index = ordered_phases.index(self.current_phase)\n        except ValueError:\n            return 0 # Current phase not in the expected list\n\n        # 1. Time remaining in the current phase\n        current_phase_total_time = self.profiling_config.get(f'avg_{self.current_phase}_time', 60)\n        time_in_current_phase = current_phase_total_time * (1 - current_stage_progress)\n\n        # 2. Time for all future phases\n        time_for_future_phases = 0\n        for i in range(current_phase_index + 1, len(ordered_phases)):\n            future_phase = ordered_phases[i]\n            time_for_future_phases += self.profiling_config.get(f'avg_{future_phase}_time', 60)\n\n        # 3. Total ETA\n        eta = time_in_current_phase + time_for_future_phases\n        return max(0, eta)\n\n    def update_and_save_estimates(self):\n        \"\"\"\n        Updates the profiling config with the latest averages and saves it to file.\n        This should be called after a subtask or stage has finished.\n        \"\"\"\n        try:\n            with open(self.config_path, 'r') as f:\n                full_config = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            full_config = {}\n\n        if self.study_type not in full_config:\n            full_config[self.study_type] = {}\n\n        # Update phase averages from the current profiling session\n        for key, value in self.profiling_config.items():\n            if key.startswith('avg_'):\n                full_config[self.study_type][key] = value\n\n        # Update subtask averages from the current profiling session\n        for task_name, times in self.subtask_times.items():\n            if times:\n                # Here, we can decide on the averaging strategy.\n                # For now, let's use a simple moving average for responsiveness.\n                # A more robust approach could be a weighted average.\n                # For simplicity, we'll just average all recorded times for now.\n                avg_task_time = sum(times) / len(times)\n                full_config[self.study_type][f'avg_{task_name}'] = avg_task_time\n\n        with open(self.config_path, 'w') as f:\n            json.dump(full_config, f, indent=4)\n\n    def save_estimates(self):\n        \"\"\"\n        Ensures the final estimates are saved. This can be called at the end of the study.\n        \"\"\"\n        self.update_and_save_estimates()\n</code></pre>"},{"location":"api/#src.profiler.Profiler.get_weighted_progress","title":"<code>get_weighted_progress(phase_name, phase_progress_ratio)</code>","text":"<p>Calculates the overall progress based on the current phase's weight and progress.</p> Source code in <code>src\\profiler.py</code> <pre><code>def get_weighted_progress(self, phase_name, phase_progress_ratio):\n    \"\"\"Calculates the overall progress based on the current phase's weight and progress.\"\"\"\n    progress = 0\n    # Add the weight of all completed phases\n    for p, w in self.phase_weights.items():\n        if p == phase_name:\n            progress += w * phase_progress_ratio\n            break\n        progress += w\n    return progress * 100\n</code></pre>"},{"location":"api/#src.profiler.Profiler.get_subtask_estimate","title":"<code>get_subtask_estimate(task_name)</code>","text":"<p>Gets the estimated time for a specific subtask from the profiling config. Returns a default value if not found.</p> Source code in <code>src\\profiler.py</code> <pre><code>def get_subtask_estimate(self, task_name):\n    \"\"\"\n    Gets the estimated time for a specific subtask from the profiling config.\n    Returns a default value if not found.\n    \"\"\"\n    return self.profiling_config.get(f'avg_{task_name}', 1.0) # Default to 1 second\n</code></pre>"},{"location":"api/#src.profiler.Profiler.get_time_remaining","title":"<code>get_time_remaining(current_stage_progress=0.0)</code>","text":"<p>Estimates the total time remaining for the study by considering completed phases, the progress of the current phase, and the estimated time for future phases.</p> Source code in <code>src\\profiler.py</code> <pre><code>def get_time_remaining(self, current_stage_progress=0.0):\n    \"\"\"\n    Estimates the total time remaining for the study by considering completed phases,\n    the progress of the current phase, and the estimated time for future phases.\n    \"\"\"\n    if not self.current_phase:\n        return 0\n\n    # Find the index of the current phase in the execution order\n    ordered_phases = [p for p in ['setup', 'run', 'extract'] if self.execution_control.get(f'do_{p}', False)]\n    try:\n        current_phase_index = ordered_phases.index(self.current_phase)\n    except ValueError:\n        return 0 # Current phase not in the expected list\n\n    # 1. Time remaining in the current phase\n    current_phase_total_time = self.profiling_config.get(f'avg_{self.current_phase}_time', 60)\n    time_in_current_phase = current_phase_total_time * (1 - current_stage_progress)\n\n    # 2. Time for all future phases\n    time_for_future_phases = 0\n    for i in range(current_phase_index + 1, len(ordered_phases)):\n        future_phase = ordered_phases[i]\n        time_for_future_phases += self.profiling_config.get(f'avg_{future_phase}_time', 60)\n\n    # 3. Total ETA\n    eta = time_in_current_phase + time_for_future_phases\n    return max(0, eta)\n</code></pre>"},{"location":"api/#src.profiler.Profiler.update_and_save_estimates","title":"<code>update_and_save_estimates()</code>","text":"<p>Updates the profiling config with the latest averages and saves it to file. This should be called after a subtask or stage has finished.</p> Source code in <code>src\\profiler.py</code> <pre><code>def update_and_save_estimates(self):\n    \"\"\"\n    Updates the profiling config with the latest averages and saves it to file.\n    This should be called after a subtask or stage has finished.\n    \"\"\"\n    try:\n        with open(self.config_path, 'r') as f:\n            full_config = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        full_config = {}\n\n    if self.study_type not in full_config:\n        full_config[self.study_type] = {}\n\n    # Update phase averages from the current profiling session\n    for key, value in self.profiling_config.items():\n        if key.startswith('avg_'):\n            full_config[self.study_type][key] = value\n\n    # Update subtask averages from the current profiling session\n    for task_name, times in self.subtask_times.items():\n        if times:\n            # Here, we can decide on the averaging strategy.\n            # For now, let's use a simple moving average for responsiveness.\n            # A more robust approach could be a weighted average.\n            # For simplicity, we'll just average all recorded times for now.\n            avg_task_time = sum(times) / len(times)\n            full_config[self.study_type][f'avg_{task_name}'] = avg_task_time\n\n    with open(self.config_path, 'w') as f:\n        json.dump(full_config, f, indent=4)\n</code></pre>"},{"location":"api/#src.profiler.Profiler.save_estimates","title":"<code>save_estimates()</code>","text":"<p>Ensures the final estimates are saved. This can be called at the end of the study.</p> Source code in <code>src\\profiler.py</code> <pre><code>def save_estimates(self):\n    \"\"\"\n    Ensures the final estimates are saved. This can be called at the end of the study.\n    \"\"\"\n    self.update_and_save_estimates()\n</code></pre>"},{"location":"api/#src.project_manager","title":"<code>project_manager</code>","text":""},{"location":"api/#src.project_manager.ProjectCorruptionError","title":"<code>ProjectCorruptionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for corrupted project files.</p> Source code in <code>src\\project_manager.py</code> <pre><code>class ProjectCorruptionError(Exception):\n    \"\"\"Custom exception for corrupted project files.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager","title":"<code>ProjectManager</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Handles the lifecycle of the .smash project file.</p> Source code in <code>src\\project_manager.py</code> <pre><code>class ProjectManager(LoggingMixin):\n    \"\"\"\n    Handles the lifecycle of the .smash project file.\n    \"\"\"\n    def __init__(self, config, verbose_logger, progress_logger, gui=None):\n        self.config = config\n        self.verbose_logger = verbose_logger\n        self.progress_logger = progress_logger\n        self.gui = gui\n        import s4l_v1.document\n        self.document = s4l_v1.document\n        self.project_path = None\n        self.execution_control = self.config.get_setting('execution_control', {'do_setup': True, 'do_run': True, 'do_extract': True})\n\n\n    def _is_valid_smash_file(self):\n        \"\"\"\n        Checks if the project file is valid and not locked.\n        It first tries to rename the file to itself to check for a lock,\n        then uses h5py to check for basic HDF5 format corruption.\n        \"\"\"\n        if not self.project_path:\n            return False\n        # Lock files can exist, preventing a check. If a lock file for the project exists,\n        # we can reasonably assume the file is locked and avoid the check.\n        lock_file_path = os.path.join(os.path.dirname(self.project_path), f\".{os.path.basename(self.project_path)}.s4l_lock\")\n        if os.path.exists(lock_file_path):\n            self._log(f\"  - Lock file detected: {lock_file_path}. Assuming project is in use.\", log_type='warning')\n            return False\n\n        try:\n            # 1. Check for file lock by trying to rename. This is a common Windows trick.\n            os.rename(self.project_path, self.project_path)\n        except OSError as e:\n            self._log(f\"  - File lock detected on {self.project_path}: {e}\", log_type='warning')\n            self._log(f\"  - The file is likely being used by another process. Skipping.\", log_type='warning')\n            return False\n\n        try:\n            # 2. Check for HDF5 format corruption.\n            with h5py.File(self.project_path, 'r') as f:\n                pass\n            return True\n        except OSError as e:\n            self._log(f\"  - HDF5 format error in {self.project_path}: {e}\", log_type='error')\n            return False\n\n    def create_or_open_project(self, phantom_name, frequency_mhz, placement_name=None):\n        \"\"\"\n        Creates or opens a project based on the 'do_setup' execution control flag.\n        \"\"\"\n        study_type = self.config.get_setting('study_type')\n        if not study_type:\n            raise ValueError(\"'study_type' not found in the configuration file.\")\n\n        if study_type == 'near_field':\n            if not all([phantom_name, frequency_mhz, placement_name]):\n                raise ValueError(\"For near-field studies, phantom_name, frequency_mhz, and placement_name are required.\")\n\n            project_dir = os.path.join(self.config.base_dir, 'results', 'near_field', phantom_name.lower(), f\"{frequency_mhz}MHz\", placement_name)\n            project_filename = f\"near_field_{phantom_name.lower()}_{frequency_mhz}MHz_{placement_name}.smash\"\n\n        elif study_type == 'far_field':\n            project_dir = os.path.join(self.config.base_dir, 'results', 'far_field', phantom_name.lower(), f\"{frequency_mhz}MHz\")\n            project_filename = f\"far_field_{phantom_name.lower()}_{frequency_mhz}MHz.smash\"\n\n        else:\n            raise ValueError(f\"Unknown study_type '{study_type}' in config.\")\n\n        os.makedirs(project_dir, exist_ok=True)\n        # Ensure path uses forward slashes for Sim4Life compatibility\n        self.project_path = os.path.join(project_dir, project_filename).replace('\\\\', '/')\n        self._log(f\"Project path set to: {self.project_path}\", log_type='info')\n\n        do_setup = self.execution_control.get('do_setup', True)\n\n        if do_setup:\n            self._log(\"Execution control: 'do_setup' is true. Creating a new project.\", log_type='info')\n            self.create_new()\n            # NOTE: Do not save immediately. The project is unstable until content is added.\n            # The calling study is responsible for the first save.\n        else:\n            self._log(\"Execution control: 'do_setup' is false. Attempting to open existing project.\", log_type='info')\n            if not os.path.exists(self.project_path):\n                # If the primary path doesn't exist, check for the old \"thelonius\" naming convention.\n                old_project_path = self.project_path.replace(\"thelonious\", \"thelonius\")\n                if os.path.exists(old_project_path):\n                    self._log(f\"Project not found at primary path, but found with old 'thelonius' naming: {old_project_path}\", log_type='warning')\n                    self.project_path = old_project_path\n                else:\n                    error_msg = f\"ERROR: 'do_setup' is false, but project file not found at {self.project_path} or with old naming. Cannot proceed.\"\n                    self._log(error_msg, log_type='fatal')\n                    raise FileNotFoundError(error_msg)\n\n            try:\n                self.open()\n            except ProjectCorruptionError:\n                if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n                    self.document.Close()\n                raise  # Re-raise the exception to halt the study\n\n    def create_new(self):\n        \"\"\"\n        Creates a new empty project in memory, deleting any existing file.\n        The project is not saved to disk until save() is explicitly called.\n        \"\"\"\n        # Close any currently open document to release file locks before deleting\n        if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n            self._log(\"Closing existing document before creating a new one to release file lock.\", log_type='info')\n            self.document.Close()\n\n        if self.project_path and os.path.exists(self.project_path):\n            self._log(f\"Deleting existing project file at {self.project_path}\", log_type='warning')\n            os.remove(self.project_path)\n\n            # Also delete associated cache files\n            project_dir = os.path.dirname(self.project_path)\n            project_filename_base = os.path.basename(self.project_path)\n            for item in os.listdir(project_dir):\n                # Cache files start with the project name (or a dot) and are not the project itself.\n                # e.g. &lt;project&gt;.smash.s4l_cache or .&lt;project&gt;.smash.s4l_lock\n                is_cache_file = item.startswith(f\".{project_filename_base}\") or \\\n                                (item.startswith(project_filename_base) and item != project_filename_base)\n\n                if is_cache_file:\n                    item_path = os.path.join(project_dir, item)\n                    if os.path.isfile(item_path):\n                        self._log(f\"Deleting cache file: {item_path}\", log_type='info')\n                        try:\n                            os.remove(item_path)\n                        except OSError as e:\n                            self._log(f\"Error deleting cache file {item_path}: {e}\", log_type='error')\n\n        self._log(\"Creating a new empty project in memory.\", log_type='info')\n        self.document.New()\n\n        # In S4L v9.0, the modeling environment must be initialized by creating a\n        # geometric entity after creating a new document. The project should not\n        # be saved until it is populated by the setup script.\n        self._log(\"Initializing model by creating and deleting a dummy block...\", log_type='verbose')\n        dummy_block = s4l_model.CreateSolidBlock(s4l_model.Vec3(0,0,0), s4l_model.Vec3(1,1,1))\n        dummy_block.Delete()\n        self._log(\"Model initialized, ready for population.\", log_type='verbose')\n\n    def open(self):\n        \"\"\"\n        Opens an existing project after validating it.\n        \"\"\"\n        self._log(f\"Validating project file: {self.project_path}\", log_type='info')\n        if not self._is_valid_smash_file():\n            self._log(f\"ERROR: Project file {self.project_path} is corrupted or locked.\", log_type='fatal')\n            raise ProjectCorruptionError(f\"File is not a valid or accessible HDF5 file: {self.project_path}\")\n\n        self._log(f\"Opening project with Sim4Life: {self.project_path}\", log_type='info')\n        try:\n            # The open_project utility calls s4l_v1.document.Open()\n            open_project(self.project_path)\n        except Exception as e:\n            # Catching a broad exception because the underlying Sim4Life error is not specific\n            self._log(f\"ERROR: Sim4Life failed to open project file, it is likely corrupted: {e}\", log_type='fatal')\n            # Close the document if it was partially opened, to release locks\n            if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n                self.document.Close()\n            raise ProjectCorruptionError(f\"Sim4Life could not open corrupted file: {self.project_path}\")\n\n    def save(self):\n        \"\"\"\n        Saves the project to its file path using SaveAs.\n        \"\"\"\n        if not self.project_path:\n            raise ValueError(\"Project path is not set. Cannot save.\")\n\n        self._log(f\"Saving project to {self.project_path}...\", log_type='info')\n        self.document.SaveAs(self.project_path)\n        self._log(\"Project saved.\", log_type='success')\n\n    def close(self):\n        \"\"\"\n        Closes the Sim4Life document.\n        \"\"\"\n        self._log(\"Closing project document...\", log_type='info')\n        self.document.Close()\n\n    def cleanup(self):\n        \"\"\"\n        Placeholder for any additional cleanup tasks.\n        Currently just closes the project.\n        \"\"\"\n        if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n            self.close()\n\n    def reload_project(self):\n        \"\"\"Saves, closes, and re-opens the project to ensure results are loaded.\"\"\"\n        if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n            self._log(\"Saving and reloading project to load results...\", log_type='info')\n            self.save()\n            self.close()\n\n        self.open()\n        self._log(\"Project reloaded.\", log_type='success')\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.create_or_open_project","title":"<code>create_or_open_project(phantom_name, frequency_mhz, placement_name=None)</code>","text":"<p>Creates or opens a project based on the 'do_setup' execution control flag.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def create_or_open_project(self, phantom_name, frequency_mhz, placement_name=None):\n    \"\"\"\n    Creates or opens a project based on the 'do_setup' execution control flag.\n    \"\"\"\n    study_type = self.config.get_setting('study_type')\n    if not study_type:\n        raise ValueError(\"'study_type' not found in the configuration file.\")\n\n    if study_type == 'near_field':\n        if not all([phantom_name, frequency_mhz, placement_name]):\n            raise ValueError(\"For near-field studies, phantom_name, frequency_mhz, and placement_name are required.\")\n\n        project_dir = os.path.join(self.config.base_dir, 'results', 'near_field', phantom_name.lower(), f\"{frequency_mhz}MHz\", placement_name)\n        project_filename = f\"near_field_{phantom_name.lower()}_{frequency_mhz}MHz_{placement_name}.smash\"\n\n    elif study_type == 'far_field':\n        project_dir = os.path.join(self.config.base_dir, 'results', 'far_field', phantom_name.lower(), f\"{frequency_mhz}MHz\")\n        project_filename = f\"far_field_{phantom_name.lower()}_{frequency_mhz}MHz.smash\"\n\n    else:\n        raise ValueError(f\"Unknown study_type '{study_type}' in config.\")\n\n    os.makedirs(project_dir, exist_ok=True)\n    # Ensure path uses forward slashes for Sim4Life compatibility\n    self.project_path = os.path.join(project_dir, project_filename).replace('\\\\', '/')\n    self._log(f\"Project path set to: {self.project_path}\", log_type='info')\n\n    do_setup = self.execution_control.get('do_setup', True)\n\n    if do_setup:\n        self._log(\"Execution control: 'do_setup' is true. Creating a new project.\", log_type='info')\n        self.create_new()\n        # NOTE: Do not save immediately. The project is unstable until content is added.\n        # The calling study is responsible for the first save.\n    else:\n        self._log(\"Execution control: 'do_setup' is false. Attempting to open existing project.\", log_type='info')\n        if not os.path.exists(self.project_path):\n            # If the primary path doesn't exist, check for the old \"thelonius\" naming convention.\n            old_project_path = self.project_path.replace(\"thelonious\", \"thelonius\")\n            if os.path.exists(old_project_path):\n                self._log(f\"Project not found at primary path, but found with old 'thelonius' naming: {old_project_path}\", log_type='warning')\n                self.project_path = old_project_path\n            else:\n                error_msg = f\"ERROR: 'do_setup' is false, but project file not found at {self.project_path} or with old naming. Cannot proceed.\"\n                self._log(error_msg, log_type='fatal')\n                raise FileNotFoundError(error_msg)\n\n        try:\n            self.open()\n        except ProjectCorruptionError:\n            if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n                self.document.Close()\n            raise  # Re-raise the exception to halt the study\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.create_new","title":"<code>create_new()</code>","text":"<p>Creates a new empty project in memory, deleting any existing file. The project is not saved to disk until save() is explicitly called.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def create_new(self):\n    \"\"\"\n    Creates a new empty project in memory, deleting any existing file.\n    The project is not saved to disk until save() is explicitly called.\n    \"\"\"\n    # Close any currently open document to release file locks before deleting\n    if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n        self._log(\"Closing existing document before creating a new one to release file lock.\", log_type='info')\n        self.document.Close()\n\n    if self.project_path and os.path.exists(self.project_path):\n        self._log(f\"Deleting existing project file at {self.project_path}\", log_type='warning')\n        os.remove(self.project_path)\n\n        # Also delete associated cache files\n        project_dir = os.path.dirname(self.project_path)\n        project_filename_base = os.path.basename(self.project_path)\n        for item in os.listdir(project_dir):\n            # Cache files start with the project name (or a dot) and are not the project itself.\n            # e.g. &lt;project&gt;.smash.s4l_cache or .&lt;project&gt;.smash.s4l_lock\n            is_cache_file = item.startswith(f\".{project_filename_base}\") or \\\n                            (item.startswith(project_filename_base) and item != project_filename_base)\n\n            if is_cache_file:\n                item_path = os.path.join(project_dir, item)\n                if os.path.isfile(item_path):\n                    self._log(f\"Deleting cache file: {item_path}\", log_type='info')\n                    try:\n                        os.remove(item_path)\n                    except OSError as e:\n                        self._log(f\"Error deleting cache file {item_path}: {e}\", log_type='error')\n\n    self._log(\"Creating a new empty project in memory.\", log_type='info')\n    self.document.New()\n\n    # In S4L v9.0, the modeling environment must be initialized by creating a\n    # geometric entity after creating a new document. The project should not\n    # be saved until it is populated by the setup script.\n    self._log(\"Initializing model by creating and deleting a dummy block...\", log_type='verbose')\n    dummy_block = s4l_model.CreateSolidBlock(s4l_model.Vec3(0,0,0), s4l_model.Vec3(1,1,1))\n    dummy_block.Delete()\n    self._log(\"Model initialized, ready for population.\", log_type='verbose')\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.open","title":"<code>open()</code>","text":"<p>Opens an existing project after validating it.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def open(self):\n    \"\"\"\n    Opens an existing project after validating it.\n    \"\"\"\n    self._log(f\"Validating project file: {self.project_path}\", log_type='info')\n    if not self._is_valid_smash_file():\n        self._log(f\"ERROR: Project file {self.project_path} is corrupted or locked.\", log_type='fatal')\n        raise ProjectCorruptionError(f\"File is not a valid or accessible HDF5 file: {self.project_path}\")\n\n    self._log(f\"Opening project with Sim4Life: {self.project_path}\", log_type='info')\n    try:\n        # The open_project utility calls s4l_v1.document.Open()\n        open_project(self.project_path)\n    except Exception as e:\n        # Catching a broad exception because the underlying Sim4Life error is not specific\n        self._log(f\"ERROR: Sim4Life failed to open project file, it is likely corrupted: {e}\", log_type='fatal')\n        # Close the document if it was partially opened, to release locks\n        if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n            self.document.Close()\n        raise ProjectCorruptionError(f\"Sim4Life could not open corrupted file: {self.project_path}\")\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.save","title":"<code>save()</code>","text":"<p>Saves the project to its file path using SaveAs.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves the project to its file path using SaveAs.\n    \"\"\"\n    if not self.project_path:\n        raise ValueError(\"Project path is not set. Cannot save.\")\n\n    self._log(f\"Saving project to {self.project_path}...\", log_type='info')\n    self.document.SaveAs(self.project_path)\n    self._log(\"Project saved.\", log_type='success')\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.close","title":"<code>close()</code>","text":"<p>Closes the Sim4Life document.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes the Sim4Life document.\n    \"\"\"\n    self._log(\"Closing project document...\", log_type='info')\n    self.document.Close()\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.cleanup","title":"<code>cleanup()</code>","text":"<p>Placeholder for any additional cleanup tasks. Currently just closes the project.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Placeholder for any additional cleanup tasks.\n    Currently just closes the project.\n    \"\"\"\n    if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n        self.close()\n</code></pre>"},{"location":"api/#src.project_manager.ProjectManager.reload_project","title":"<code>reload_project()</code>","text":"<p>Saves, closes, and re-opens the project to ensure results are loaded.</p> Source code in <code>src\\project_manager.py</code> <pre><code>def reload_project(self):\n    \"\"\"Saves, closes, and re-opens the project to ensure results are loaded.\"\"\"\n    if self.document and hasattr(self.document, 'IsOpen') and self.document.IsOpen():\n        self._log(\"Saving and reloading project to load results...\", log_type='info')\n        self.save()\n        self.close()\n\n    self.open()\n    self._log(\"Project reloaded.\", log_type='success')\n</code></pre>"},{"location":"api/#src.results_extractor","title":"<code>results_extractor</code>","text":""},{"location":"api/#src.results_extractor.ResultsExtractor","title":"<code>ResultsExtractor</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Handles all post-processing and data extraction tasks.</p> Source code in <code>src\\results_extractor.py</code> <pre><code>class ResultsExtractor(LoggingMixin):\n    \"\"\"\n    Handles all post-processing and data extraction tasks.\n    \"\"\"\n    def __init__(self, config, simulation, phantom_name, frequency_mhz, placement_name, study_type, verbose_logger, progress_logger, free_space=False, gui=None, study=None):\n        self.config = config\n        self.simulation = simulation\n        self.phantom_name = phantom_name\n        self.frequency_mhz = frequency_mhz\n        self.placement_name = placement_name\n        self.study_type = study_type\n        self.verbose_logger = verbose_logger\n        self.progress_logger = progress_logger\n        self.free_space = free_space\n        self.gui = gui\n        self.study = study\n\n        import s4l_v1.document\n        import s4l_v1.analysis\n        import s4l_v1.units as units\n        self.document = s4l_v1.document\n        self.analysis = s4l_v1.analysis\n        self.units = units\n\n\n    def extract(self):\n        \"\"\"\n        Extracts and saves the simulation results.\n        \"\"\"\n        self._log(\"Extracting results...\", log_type='progress')\n        if not self.simulation:\n            self._log(\"  - ERROR: Simulation object not found. Skipping result extraction.\", log_type='error')\n            return\n\n        results_data = {}\n        simulation_extractor = self.simulation.Results()\n\n        # Extract Input Power\n        if self.gui:\n            self.gui.update_stage_progress(\"Extracting Power\", 50, 100)\n        self._extract_input_power(simulation_extractor, results_data)\n\n        # Extract SAR Statistics\n        if not self.free_space:\n            if self.gui:\n                self.gui.update_stage_progress(\"Extracting SAR\", 100, 100)\n            self._extract_sar_statistics(simulation_extractor, results_data)\n            self._extract_power_balance(simulation_extractor, results_data)\n        else:\n            self._log(\"  - Skipping SAR statistics for free-space simulation.\", log_type='info')\n\n        # Extract Point Sensor Data if any\n        if self.config.get_setting(\"simulation_parameters.number_of_point_sensors\", 0) &gt; 0:\n            if self.gui:\n                self.gui.update_stage_progress(\"Extracting Point Sensors\", 75, 100)\n            self._extract_point_sensor_data(simulation_extractor, results_data)\n\n        # Save reports and final results\n        if not self.free_space and '_temp_sar_df' in results_data:\n            self._save_reports(\n                results_data.pop('_temp_sar_df'),\n                results_data.pop('_temp_tissue_groups'),\n                results_data.pop('_temp_group_sar_stats'),\n                results_data\n            )\n\n        results_dir = os.path.join(self.config.base_dir, 'results', self.study_type, self.phantom_name, f\"{self.frequency_mhz}MHz\", self.placement_name)\n        os.makedirs(results_dir, exist_ok=True)\n        results_filepath = os.path.join(results_dir, 'sar_results.json')\n\n        # Clean up any remaining temporary keys before final json dump\n        final_results_data = {k: v for k, v in results_data.items() if not k.startswith('_temp')}\n\n        with open(results_filepath, 'w') as f:\n            json.dump(final_results_data, f, indent=4)\n        self._log(f\"  - SAR results saved to: {results_filepath}\", log_type='info')\n\n    def _extract_input_power(self, simulation_extractor, results_data):\n        \"\"\"Extracts the input power from the simulation.\"\"\"\n        self._log(\"  - Extracting input power...\", log_type='progress')\n        with self.study.subtask(\"extract_input_power\"):\n            try:\n                # For far-field, we use a theoretical model\n                if self.study_type == 'far_field':\n                    self._log(\"  - Far-field study: using theoretical model for input power.\", log_type='info')\n\n                    # Find the simulation bounding box entity\n                    import s4l_v1.model\n                    try:\n                        bbox_entity = next((e for e in s4l_v1.model.AllEntities() if hasattr(e, 'Name') and e.Name == \"far_field_simulation_bbox\"), None)\n                        if not bbox_entity:\n                            raise RuntimeError(\"Could not find 'far_field_simulation_bbox' entity in the project.\")\n                        sim_bbox = s4l_v1.model.GetBoundingBox([bbox_entity])\n                    except RuntimeError as e:\n                        self._log(f\"  - WARNING: Could not calculate theoretical input power. {e}\", log_type='warning')\n                        return\n                    sim_min = np.array(sim_bbox[0])\n                    sim_max = np.array(sim_bbox[1])\n                    self._log(f\"  - Bounding Box Min: {sim_min}, Max: {sim_max}\", log_type='verbose')\n\n                    # Add padding from config\n                    padding_bottom = np.array(self.config.get_setting('gridding_parameters.padding.manual_bottom_padding_mm', [0,0,0]))\n                    padding_top = np.array(self.config.get_setting('gridding_parameters.padding.manual_top_padding_mm', [0,0,0]))\n\n                    total_min = sim_min - padding_bottom\n                    total_max = sim_max + padding_top\n\n                    # E-field is 1 V/m\n                    e_field_v_m = 1.0\n\n                    # Impedance of free space\n                    z0 = 377.0\n\n                    # Power density (W/m^2)\n                    power_density_w_m2 = (e_field_v_m**2) / (2 * z0)\n\n                    # The incident direction determines the area\n                    direction = self.placement_name.split('_')[1] # e.g., from \"environmental_x_pos_theta\"\n\n                    dims = total_max - total_min\n                    # area is in mm^2, convert to m^2\n                    if direction == 'x':\n                        area_m2 = (dims[1] * dims[2]) / 1e6\n                    elif direction == 'y':\n                        area_m2 = (dims[0] * dims[2]) / 1e6\n                    else: # z\n                        area_m2 = (dims[0] * dims[1]) / 1e6\n                    self._log(f\"  - Calculated Area: {area_m2} m^2\", log_type='verbose')\n\n                    total_input_power = power_density_w_m2 * area_m2\n\n                    results_data['input_power_W'] = total_input_power\n                    results_data['input_power_frequency_MHz'] = float(self.frequency_mhz)\n                    self._log(f\"  - Calculated theoretical input power: {total_input_power:.4e} W\", log_type='highlight')\n                    return\n\n                # For near-field, extract from the simulation\n                input_power_extractor = simulation_extractor[\"Input Power\"]\n                self.document.AllAlgorithms.Add(input_power_extractor)\n                input_power_extractor.Update()\n\n                if hasattr(input_power_extractor, 'GetPower'):\n                    self._log(\"  - Using GetPower() to extract input power.\", log_type='info')\n                    power_w, _ = input_power_extractor.GetPower(0)\n                    results_data['input_power_W'] = float(power_w)\n                    results_data['input_power_frequency_MHz'] = float(self.frequency_mhz)\n                else:\n                    self._log(\"  - GetPower() not available, falling back to manual extraction.\", log_type='warning')\n                    input_power_output = input_power_extractor.Outputs[\"EM Input Power(f)\"]\n                    input_power_output.Update()\n\n                    if hasattr(input_power_output, 'GetHarmonicData'):\n                        self._log(\"  - Harmonic data detected. Extracting single power value.\", log_type='info')\n                        power_complex = input_power_output.GetHarmonicData(0)\n                        input_power_w = abs(power_complex)\n                        frequency_at_value_mhz = self.frequency_mhz\n                        results_data['input_power_W'] = float(input_power_w)\n                        results_data['input_power_frequency_MHz'] = float(frequency_at_value_mhz)\n                    else:\n                        power_data = input_power_output.Data.GetComponent(0)\n                        if power_data is not None and hasattr(power_data, 'size') and power_data.size &gt; 0:\n                            if power_data.size == 1:\n                                input_power_w = power_data.item()\n                                frequency_at_value_mhz = self.frequency_mhz\n                            else:\n                                center_frequency_hz = self.frequency_mhz * 1e6\n                                axis = input_power_output.Data.Axis\n                                min_diff = float('inf')\n                                target_index = -1\n                                for i, freq_hz in enumerate(axis):\n                                    diff = abs(freq_hz - center_frequency_hz)\n                                    if diff &lt; min_diff:\n                                        min_diff = diff\n                                        target_index = i\n                                if target_index != -1:\n                                    input_power_w = power_data[target_index]\n                                    frequency_at_value_mhz = axis[target_index] / 1e6\n                                else:\n                                    input_power_w = -1\n                                    frequency_at_value_mhz = -1\n                            results_data['input_power_W'] = float(input_power_w)\n                            results_data['input_power_frequency_MHz'] = float(frequency_at_value_mhz)\n                        else:\n                            self._log(\"  - WARNING: Could not extract input power values.\", log_type='warning')\n            except Exception as e:\n                self._log(f\"  - ERROR: An exception occurred during input power extraction: {e}\", level='progress', log_type='error')\n                traceback.print_exc()\n\n    def _extract_sar_statistics(self, simulation_extractor, results_data):\n        \"\"\"Extracts SAR statistics for all tissues.\"\"\"\n        self._log(\"  - Extracting SAR statistics for all tissues...\", log_type='progress')\n        with self.study.subtask(\"extract_sar_statistics\"):\n            try:\n                em_sensor_extractor = simulation_extractor[\"Overall Field\"]\n                em_sensor_extractor.FrequencySettings.ExtractedFrequency = u\"All\"\n                self.document.AllAlgorithms.Add(em_sensor_extractor)\n                em_sensor_extractor.Update()\n\n                inputs = [em_sensor_extractor.Outputs[\"EM E(x,y,z,f0)\"]]\n                sar_stats_evaluator = self.analysis.em_evaluators.SarStatisticsEvaluator(inputs=inputs)\n                sar_stats_evaluator.PeakSpatialAverageSAR = True\n                sar_stats_evaluator.PeakSAR.TargetMass = 10.0, self.units.Unit(\"g\")\n                sar_stats_evaluator.UpdateAttributes()\n                self.document.AllAlgorithms.Add(sar_stats_evaluator)\n                sar_stats_evaluator.Update()\n\n                stats_output = sar_stats_evaluator.Outputs\n\n                results = None\n                if len(stats_output) &gt; 0:\n                    # The results are in the first output port of the evaluator\n                    results_wrapper = stats_output.item_at(0)\n                    if hasattr(results_wrapper, 'Data'):\n                        results = results_wrapper.Data\n                    else:\n                        self._log(\"  - ERROR: AlgorithmOutput object does not have a .Data attribute.\", log_type='error')\n                else:\n                    self._log(\"  - ERROR: No output ports found in the SAR Statistics Evaluator.\", log_type='error')\n\n                # IMPORTANT: Clean up the algorithm to avoid issues when processing multiple simulations in the same project\n                self.document.AllAlgorithms.Remove(sar_stats_evaluator)\n\n                if not (results and hasattr(results, 'NumberOfRows') and results.NumberOfRows() &gt; 0 and results.NumberOfColumns() &gt; 0):\n                    self._log(\"  - WARNING: No SAR statistics data found.\", log_type='warning')\n                    return\n\n                columns = [\"Tissue\"] + [cap for cap in results.ColumnMainCaptions]\n                data = []\n                for i in range(results.NumberOfRows()):\n                    row_caption = results.RowCaptions[i]\n                    clean_caption = re.sub(r'\\s*\\(.*\\)\\s*$', '', row_caption).strip().replace(')', '')\n                    row_data = [clean_caption] + [results.Value(i, j) for j in range(results.NumberOfColumns())]\n                    data.append(row_data)\n\n                df = pd.DataFrame(data, columns=columns)\n                numeric_cols = [col for col in df.columns if col != 'Tissue']\n                df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n\n                tissue_groups = self._define_tissue_groups(df['Tissue'].tolist())\n                group_sar_stats = self._calculate_group_sar(df, tissue_groups)\n\n                for group, data in group_sar_stats.items():\n                    results_data[f\"{group}_weighted_avg_sar\"] = data['weighted_avg_sar']\n                    results_data[f\"{group}_peak_sar\"] = data['peak_sar']\n\n                all_regions_row = df[df['Tissue'] == 'All Regions']\n                if not all_regions_row.empty:\n                    mass_averaged_sar = all_regions_row['Mass-Averaged SAR'].iloc[0]\n                    if self.study_type == 'near_field':\n                        sar_key = 'head_SAR' if self.placement_name.lower() in ['front_of_eyes', 'by_cheek'] else 'trunk_SAR'\n                        results_data[sar_key] = float(mass_averaged_sar)\n                    else:\n                        results_data['whole_body_sar'] = float(mass_averaged_sar)\n\n                    peak_sar_col_name = 'Peak Spatial-Average SAR[IEEE/IEC62704-1] (10g)'\n                    if peak_sar_col_name in all_regions_row.columns:\n                        results_data['peak_sar_10g_W_kg'] = float(all_regions_row[peak_sar_col_name].iloc[0])\n\n                self._extract_peak_sar_details(em_sensor_extractor, results_data)\n                # Save the dataframes for later reporting\n                results_data['_temp_sar_df'] = df\n                results_data['_temp_tissue_groups'] = tissue_groups\n                results_data['_temp_group_sar_stats'] = group_sar_stats\n\n            except Exception as e:\n                self._log(f\"  - ERROR: An unexpected error during all-tissue SAR statistics extraction: {e}\", level='progress', log_type='error')\n                traceback.print_exc()\n\n    def _define_tissue_groups(self, available_tissues):\n        \"\"\"Defines tissue groups from the material mapping file or falls back to keywords.\"\"\"\n\n        material_mapping = self.config.get_material_mapping(self.phantom_name)\n\n        if \"_tissue_groups\" in material_mapping:\n            self._log(f\"  - Loading tissue groups for '{self.phantom_name}' from material_name_mapping.json\", log_type='info')\n\n            phantom_groups = material_mapping[\"_tissue_groups\"]\n            tissue_groups = {}\n            for group_name, tissue_list in phantom_groups.items():\n                # The tissues in the JSON are the S4L names, but 'available_tissues' are the mapped names.\n                # We need to get the S4L names for the available tissues to find the intersection.\n\n                # Create a reverse mapping from mapped name to S4L name for the current phantom\n                reverse_mapping = {v: k for k, v in material_mapping.items() if not k.startswith('_')}\n\n                # Find which of the available tissues (mapped names) correspond to the S4L names in our group list\n                s4l_names_in_group = set(tissue_list)\n\n                final_tissue_list = []\n                for tissue in available_tissues:\n                    s4l_name = reverse_mapping.get(tissue)\n                    if s4l_name in s4l_names_in_group:\n                        final_tissue_list.append(tissue)\n\n                tissue_groups[group_name] = final_tissue_list\n            return tissue_groups\n\n        # Fallback to old method for backward compatibility\n        self._log(\"  - WARNING: '_tissue_groups' not found in material mapping. Falling back to keyword-based tissue grouping.\", log_type='warning')\n        groups = {\n            \"eyes_group\": [\"eye\", \"cornea\", \"sclera\", \"lens\", \"vitreous\"],\n            \"skin_group\": [\"skin\"],\n            \"brain_group\": [\"brain\", \"commissura\", \"midbrain\", \"pineal\", \"hypophysis\", \"medulla_oblongata\", \"pons\", \"thalamus\", \"hippocampus\", \"cerebellum\"]\n        }\n        tissue_groups = {group: [t for t in available_tissues if any(k in t.lower() for k in keywords)] for group, keywords in groups.items()}\n        return tissue_groups\n\n    def _calculate_group_sar(self, df, tissue_groups):\n        \"\"\"Calculates weighted average and peak SAR for tissue groups.\"\"\"\n        group_sar_data = {}\n        peak_sar_col = 'Peak Spatial-Average SAR[IEEE/IEC62704-1] (10g)'\n        for group_name, tissues in tissue_groups.items():\n            group_df = df[df['Tissue'].isin(tissues)]\n            if not group_df.empty:\n                total_mass = group_df['Total Mass'].sum()\n                weighted_avg_sar = (group_df['Total Mass'] * group_df['Mass-Averaged SAR']).sum() / total_mass if total_mass &gt; 0 else 0\n                peak_sar = group_df[peak_sar_col].max() if peak_sar_col in group_df.columns else -1.0\n                group_sar_data[group_name] = {'weighted_avg_sar': weighted_avg_sar, 'peak_sar': peak_sar}\n        return group_sar_data\n\n    def _save_reports(self, df, tissue_groups, group_sar_stats, results_data):\n        \"\"\"Saves detailed reports in Pickle and HTML format.\"\"\"\n        results_dir = os.path.join(self.config.base_dir, 'results', self.study_type, self.phantom_name, f\"{self.frequency_mhz}MHz\", self.placement_name)\n        os.makedirs(results_dir, exist_ok=True)\n\n        # Make sure point sensor data is included in the pickle\n        pickle_data = {\n            'detailed_sar_stats': df,\n            'tissue_group_composition': tissue_groups,\n            'grouped_sar_stats': group_sar_stats,\n            'summary_results': results_data,\n            'peak_sar_details': results_data.get('peak_sar_details', {}),\n            'point_sensor_data': results_data.get('point_sensor_data', {})\n        }\n        pickle_filepath = os.path.join(results_dir, 'sar_stats_all_tissues.pkl')\n        with open(pickle_filepath, 'wb') as f:\n            pickle.dump(pickle_data, f)\n        self._log(f\"  - Pickle report saved to: {pickle_filepath}\", log_type='info')\n\n        html_content = df.to_html(index=False, border=1)\n        html_content += \"&lt;h2&gt;Tissue Group Composition&lt;/h2&gt;\"\n        html_content += pd.DataFrame.from_dict(tissue_groups, orient='index').to_html()\n        html_content += \"&lt;h2&gt;Grouped SAR Statistics&lt;/h2&gt;\"\n        html_content += pd.DataFrame.from_dict(group_sar_stats, orient='index').to_html()\n\n        html_content += \"&lt;h2&gt;Peak SAR Details&lt;/h2&gt;\"\n        peak_sar_df = pd.DataFrame.from_dict(results_data.get('peak_sar_details', {}), orient='index', columns=['Value'])\n        peak_sar_df.index.name = 'Parameter'\n        html_content += peak_sar_df.to_html()\n\n        html_filepath = os.path.join(results_dir, 'sar_stats_all_tissues.html')\n        with open(html_filepath, 'w', encoding='utf-8') as f:\n            f.write(html_content)\n        self._log(f\"  - HTML report saved to: {html_filepath}\", log_type='info')\n\n    def _extract_peak_sar_details(self, em_sensor_extractor, results_data):\n        \"\"\"Extracts peak SAR details like location and cell.\"\"\"\n        self._log(\"  - Extracting peak SAR details...\", log_type='progress')\n        try:\n            inputs = [em_sensor_extractor.Outputs[\"SAR(x,y,z,f0)\"]]\n            average_sar_field_evaluator = self.analysis.em_evaluators.AverageSarFieldEvaluator(inputs=inputs)\n            average_sar_field_evaluator.TargetMass = 10.0, self.units.Unit(\"g\")\n            average_sar_field_evaluator.UpdateAttributes()\n            self.document.AllAlgorithms.Add(average_sar_field_evaluator)\n            average_sar_field_evaluator.Update()\n\n            peak_sar_output = average_sar_field_evaluator.Outputs[\"Peak Spatial SAR (psSAR) Results\"]\n            peak_sar_output.Update()\n\n            data_collection = peak_sar_output.Data.DataSimpleDataCollection\n\n            peak_sar_details = {}\n            if data_collection:\n                keys = data_collection.Keys()\n                for key in keys:\n                    value = data_collection.FieldValue(key, 0)\n                    peak_sar_details[key] = value\n                    self._log(f\"    - {key}: {value}\", log_type='verbose')\n                results_data['peak_sar_details'] = peak_sar_details\n            else:\n                self._log(\"  - WARNING: Could not extract peak SAR details.\", log_type='warning')\n\n            self.document.AllAlgorithms.Remove(average_sar_field_evaluator)\n\n        except Exception as e:\n            self._log(f\"  - ERROR: An exception occurred during peak SAR detail extraction: {e}\", log_type='error')\n            traceback.print_exc()\n\n    def _extract_power_balance(self, simulation_extractor, results_data):\n        \"\"\"Extracts the power balance from the simulation.\"\"\"\n        self._log(\"  - Extracting power balance...\", log_type='progress')\n        try:\n            em_sensor_extractor = simulation_extractor[\"Overall Field\"]\n            power_balance_extractor = em_sensor_extractor.Outputs[\"Power Balance\"]\n            power_balance_extractor.Update()\n\n            power_balance_data = {}\n            keys = power_balance_extractor.Data.DataSimpleDataCollection.Keys()\n            for key in keys:\n                # Skip logging the initial 'Balance' since we will recalculate it.\n                if key == 'Balance':\n                    continue\n                value = power_balance_extractor.Data.DataSimpleDataCollection.FieldValue(key, 0)\n                power_balance_data[key] = value\n                self._log(f\"    - {key}: {value:.4e}\", log_type='verbose')\n\n            # For far-field, the input power is theoretical, so we overwrite the 'Pin'\n            if self.study_type == 'far_field' and 'input_power_W' in results_data:\n                power_balance_data['Pin'] = results_data['input_power_W']\n                self._log(f\"    - Overwriting Pin with theoretical value: {power_balance_data['Pin']:.4e} W\", log_type='info')\n\n            pin = power_balance_data.get('Pin', 0.0)\n            diel_loss = power_balance_data.get('DielLoss', 0.0)\n            rad_power = power_balance_data.get('RadPower', 0.0)\n\n            if pin &gt; 1e-9:\n                p_out = diel_loss + rad_power\n                balance = 100 * (p_out / pin)\n            else:\n                balance = float('nan')\n\n            power_balance_data['Balance'] = balance\n            self._log(f\"    - Final Balance: {balance:.2f}%\", log_type='highlight')\n\n            results_data['power_balance'] = power_balance_data\n\n        except Exception as e:\n            self._log(f\"  - WARNING: Could not extract power balance: {e}\", log_type='warning')\n            traceback.print_exc()\n\n    def _extract_point_sensor_data(self, simulation_extractor, results_data):\n        \"\"\"Extracts point sensor data, plots it, and saves the raw data.\"\"\"\n        self._log(\"  - Extracting point sensor data...\", log_type='progress')\n        with self.study.subtask(\"extract_point_sensor_data\"):\n            try:\n                num_sensors = self.config.get_setting(\"simulation_parameters.number_of_point_sensors\", 0)\n                if num_sensors == 0:\n                    return\n\n                plt.ioff()\n                plt.style.use('science')\n                plt.rcParams.update({\"text.usetex\": False})\n                fig, ax = plt.subplots()\n                ax.grid(True, which='major', axis='y', linestyle='--')\n\n                point_source_order = self.config.get_setting(\"simulation_parameters.point_source_order\", [])\n\n                point_sensor_results = {}\n\n                for i in range(num_sensors):\n                    if i &gt;= len(point_source_order):\n                        self._log(f\"    - WARNING: Not enough entries in 'point_source_order' config for sensor {i+1}. Skipping.\", log_type='warning')\n                        continue\n\n                    corner_name = point_source_order[i]\n                    sensor_name = f\"Point Sensor Entity {i+1}\"\n                    full_sensor_name = f\"{sensor_name} ({corner_name})\"\n\n                    try:\n                        # In recent S4L versions, point sensors are directly under the main extractor\n                        em_sensor_extractor = simulation_extractor[full_sensor_name]\n                        if not em_sensor_extractor:\n                            self._log(f\"    - WARNING: Could not find sensor extractor for '{full_sensor_name}'\", log_type='warning')\n                            continue\n                    except Exception as e:\n                        self._log(f\"    - WARNING: Could not retrieve sensor '{full_sensor_name}'. Error: {e}\", log_type='warning')\n                        continue\n\n                    self.document.AllAlgorithms.Add(em_sensor_extractor)\n\n                    if \"EM E(t)\" not in em_sensor_extractor.Outputs:\n                        self._log(f\"    - WARNING: 'EM E(t)' output not found for sensor '{full_sensor_name}'\", log_type='warning')\n                        self.document.AllAlgorithms.Remove(em_sensor_extractor)\n                        continue\n\n                    em_output = em_sensor_extractor.Outputs[\"EM E(t)\"]\n                    em_output.Update()\n\n                    time_axis = em_output.Data.Axis\n                    ex_data = em_output.Data.GetComponent(0)\n                    ey_data = em_output.Data.GetComponent(1)\n                    ez_data = em_output.Data.GetComponent(2)\n\n                    label = corner_name.replace('_', ' ').title()\n\n                    if time_axis is not None and time_axis.size &gt; 0:\n                        # For plotting, let's plot the magnitude\n                        e_mag = np.sqrt(ex_data**2 + ey_data**2 + ez_data**2)\n                        ax.plot(time_axis, e_mag, label=label)\n                        self._log(f\"    - Plotted data for sensor: {label}\", log_type='verbose')\n\n                        point_sensor_results[label] = {\n                            'time_s': time_axis.tolist(),\n                            'Ex_V_m': ex_data.tolist(),\n                            'Ey_V_m': ey_data.tolist(),\n                            'Ez_V_m': ez_data.tolist(),\n                            'E_mag_V_m': e_mag.tolist()\n                        }\n                        self._log(f\"    - Saved raw data for sensor: {label}\", log_type='verbose')\n                    else:\n                        self._log(f\"    - WARNING: No data found for sensor '{full_sensor_name}'\", log_type='warning')\n\n                    self.document.AllAlgorithms.Remove(em_sensor_extractor)\n\n                if point_sensor_results:\n                    results_data['point_sensor_data'] = point_sensor_results\n\n                ax.set_xlabel(\"Time (s)\")\n                ax.set_ylabel(\"|E-Field| (V/m)\")\n                ax.set_title(f\"Point Sensor E-Field Magnitude vs. Time ({self.frequency_mhz} MHz)\")\n                ax.legend()\n\n                results_dir = os.path.join(self.config.base_dir, 'results', self.study_type, self.phantom_name, f\"{self.frequency_mhz}MHz\", self.placement_name)\n                plot_filepath = os.path.join(results_dir, 'point_sensor_data.png')\n                plt.savefig(plot_filepath, dpi=300)\n                plt.close(fig)\n                self._log(f\"  - Point sensor plot saved to: {plot_filepath}\", log_type='info')\n\n            except Exception as e:\n                self._log(f\"  - ERROR: An exception occurred during point sensor data extraction: {e}\", level='progress', log_type='error')\n                traceback.print_exc()\n</code></pre>"},{"location":"api/#src.results_extractor.ResultsExtractor.extract","title":"<code>extract()</code>","text":"<p>Extracts and saves the simulation results.</p> Source code in <code>src\\results_extractor.py</code> <pre><code>def extract(self):\n    \"\"\"\n    Extracts and saves the simulation results.\n    \"\"\"\n    self._log(\"Extracting results...\", log_type='progress')\n    if not self.simulation:\n        self._log(\"  - ERROR: Simulation object not found. Skipping result extraction.\", log_type='error')\n        return\n\n    results_data = {}\n    simulation_extractor = self.simulation.Results()\n\n    # Extract Input Power\n    if self.gui:\n        self.gui.update_stage_progress(\"Extracting Power\", 50, 100)\n    self._extract_input_power(simulation_extractor, results_data)\n\n    # Extract SAR Statistics\n    if not self.free_space:\n        if self.gui:\n            self.gui.update_stage_progress(\"Extracting SAR\", 100, 100)\n        self._extract_sar_statistics(simulation_extractor, results_data)\n        self._extract_power_balance(simulation_extractor, results_data)\n    else:\n        self._log(\"  - Skipping SAR statistics for free-space simulation.\", log_type='info')\n\n    # Extract Point Sensor Data if any\n    if self.config.get_setting(\"simulation_parameters.number_of_point_sensors\", 0) &gt; 0:\n        if self.gui:\n            self.gui.update_stage_progress(\"Extracting Point Sensors\", 75, 100)\n        self._extract_point_sensor_data(simulation_extractor, results_data)\n\n    # Save reports and final results\n    if not self.free_space and '_temp_sar_df' in results_data:\n        self._save_reports(\n            results_data.pop('_temp_sar_df'),\n            results_data.pop('_temp_tissue_groups'),\n            results_data.pop('_temp_group_sar_stats'),\n            results_data\n        )\n\n    results_dir = os.path.join(self.config.base_dir, 'results', self.study_type, self.phantom_name, f\"{self.frequency_mhz}MHz\", self.placement_name)\n    os.makedirs(results_dir, exist_ok=True)\n    results_filepath = os.path.join(results_dir, 'sar_results.json')\n\n    # Clean up any remaining temporary keys before final json dump\n    final_results_data = {k: v for k, v in results_data.items() if not k.startswith('_temp')}\n\n    with open(results_filepath, 'w') as f:\n        json.dump(final_results_data, f, indent=4)\n    self._log(f\"  - SAR results saved to: {results_filepath}\", log_type='info')\n</code></pre>"},{"location":"api/#src.setups","title":"<code>setups</code>","text":""},{"location":"api/#src.setups.base_setup","title":"<code>base_setup</code>","text":""},{"location":"api/#src.setups.base_setup.BaseSetup","title":"<code>BaseSetup</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Abstract base class for all simulation setups (Near-Field, Far-Field).</p> Source code in <code>src\\setups\\base_setup.py</code> <pre><code>class BaseSetup(LoggingMixin):\n    \"\"\"\n    Abstract base class for all simulation setups (Near-Field, Far-Field).\n    \"\"\"\n    def __init__(self, config, verbose_logger, progress_logger):\n        \"\"\"\n        Initializes the base setup.\n\n        Args:\n            config (Config): The configuration object for the study.\n            verbose_logger (logging.Logger): Logger for verbose output.\n            progress_logger (logging.Logger): Logger for progress updates.\n        \"\"\"\n        self.config = config\n        self.verbose_logger = verbose_logger\n        self.progress_logger = progress_logger\n        import s4l_v1\n        self.s4l_v1 = s4l_v1\n        self.emfdtd = self.s4l_v1.simulation.emfdtd\n        self.model = self.s4l_v1.model\n\n    def _apply_simulation_time_and_termination(self, simulation, sim_bbox_entity, frequency_mhz):\n        \"\"\"\n        Calculates and applies simulation time and termination settings.\n        This is a shared method for both Near-Field and Far-Field setups.\n        \"\"\"\n        sim_params = self.config.get_simulation_parameters()\n\n        # Time Calculation\n        time_multiplier = sim_params.get(\"simulation_time_multiplier\", 5)\n        self._log(f\"  - Using simulation time multiplier: {time_multiplier}\", log_type='info')\n\n        bbox = self.model.GetBoundingBox([sim_bbox_entity])\n        if not bbox or len(bbox) &lt; 2:\n            self._log(f\"  - ERROR: Could not get a valid bounding box for entity '{sim_bbox_entity.Name}'. Skipping time calculation.\", log_type='error')\n            return\n        bbox_min, bbox_max = bbox\n\n        diagonal_length_m = np.linalg.norm(np.array(bbox_max) - np.array(bbox_min)) / 1000.0\n\n        time_to_travel_s = (time_multiplier * diagonal_length_m) / 299792458\n        sim_time_periods = time_to_travel_s / (1 / (frequency_mhz * 1e6))\n        simulation.SetupSettings.SimulationTime = sim_time_periods, self.s4l_v1.units.Periods\n        self._log(f\"  - Simulation time set to {sim_time_periods:.2f} periods.\", log_type='info')\n\n        # Termination Criteria\n        term_level = sim_params.get(\"global_auto_termination\", \"GlobalAutoTerminationWeak\")\n        self._log(f\"  - Setting termination criteria to: {term_level}\", log_type='info')\n        term_options = simulation.SetupSettings.GlobalAutoTermination.enum\n        if hasattr(term_options, term_level):\n            simulation.SetupSettings.GlobalAutoTermination = getattr(term_options, term_level)\n\n        if term_level == \"GlobalAutoTerminationUserDefined\":\n            convergence_db = sim_params.get(\"convergence_level_dB\", -30)\n            simulation.SetupSettings.ConvergenceLevel = convergence_db\n            self._log(f\"    - Convergence level set to: {convergence_db} dB\", log_type='info')\n\n    def _setup_solver_settings(self, simulation):\n        \"\"\" Configures solver settings, including kernel. \"\"\"\n        self._log(\"  - Configuring solver settings...\", log_type='progress')\n        solver_settings = self.config.get_solver_settings()\n        if not solver_settings:\n            return\n\n        solver = simulation.SolverSettings\n\n        # Setup Kernel\n        kernel_type = solver_settings.get(\"kernel\", \"Software\").lower()\n\n        if kernel_type == \"acceleware\":\n            solver.Kernel = solver.Kernel.enum.AXware\n            self._log(f\"    - Solver kernel set to: Acceleware (AXware)\", log_type='info')\n        elif kernel_type == \"cuda\":\n            solver.Kernel = solver.Kernel.enum.Cuda\n            self._log(f\"    - Solver kernel set to: Cuda\", log_type='info')\n        else:\n            solver.Kernel = solver.Kernel.enum.Software\n            self._log(f\"    - Solver kernel set to: Software\", log_type='info')\n\n    def run_full_setup(self, project_manager):\n        \"\"\"\n        This method must be implemented by subclasses to prepare the simulation scene.\n\n        Args:\n            project_manager (ProjectManager): The project manager to handle file operations.\n\n        Returns:\n            The main simulation object.\n        \"\"\"\n        raise NotImplementedError(\"The 'run_full_setup' method must be implemented by a subclass.\")\n\n    def _add_point_sensors(self, simulation, sim_bbox_entity_name):\n        \"\"\"Adds point sensors at the corners of the simulation bounding box.\"\"\"\n        num_points = self.config.get_setting(\"simulation_parameters.number_of_point_sensors\", 0)\n        if num_points == 0:\n            self._log(\"  - Skipping point sensor creation (0 points requested).\", log_type='info')\n            return\n\n        sim_bbox_entity = next((e for e in self.model.AllEntities() if sim_bbox_entity_name in e.Name), None)\n        if not sim_bbox_entity:\n            self._log(f\"  - WARNING: Could not find simulation bounding box '{sim_bbox_entity_name}' to add point sensors.\", log_type='warning')\n            return\n\n        bbox_min, bbox_max = self.model.GetBoundingBox([sim_bbox_entity])\n\n        corner_map = {\n            \"lower_left_bottom\": (bbox_min, bbox_min, bbox_min),\n            \"lower_right_bottom\": (bbox_max, bbox_min, bbox_min),\n            \"lower_left_up\": (bbox_min, bbox_max, bbox_min),\n            \"lower_right_up\": (bbox_max, bbox_max, bbox_min),\n            \"top_left_bottom\": (bbox_min, bbox_min, bbox_max),\n            \"top_right_bottom\": (bbox_max, bbox_min, bbox_max),\n            \"top_left_up\": (bbox_min, bbox_max, bbox_max),\n            \"top_right_up\": (bbox_max, bbox_max, bbox_max)\n        }\n\n        point_source_order = self.config.get_setting(\"simulation_parameters.point_source_order\", list(corner_map.keys()))\n\n        for i in range(num_points):\n            corner_name = point_source_order[i]\n            corner_coords = corner_map.get(corner_name)\n            if corner_coords is None:\n                self._log(f\"  - WARNING: Invalid corner name '{corner_name}' in point_source_order. Skipping.\", log_type='warning')\n                continue\n\n            point_entity_name = f\"Point Sensor Entity {i+1} ({corner_name})\"\n\n            existing_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and e.Name == point_entity_name), None)\n\n            if existing_entity:\n                self._log(f\"  - Point sensor '{point_entity_name}' already exists. Skipping creation.\", log_type='info')\n                continue\n\n            point_entity = self.model.CreatePoint(self.model.Vec3(corner_coords[0][0], corner_coords[1][1], corner_coords[2][2]))\n            point_entity.Name = point_entity_name\n            point_sensor = self.emfdtd.PointSensorSettings()\n            point_sensor.Name = f\"Point Sensor {i+1}\"\n            simulation.Add(point_sensor, [point_entity])\n            self._log(f\"  - Added point sensor at {corner_coords} ({corner_name})\", log_type='info')\n\n    @profile\n    def _finalize_setup(self, project_manager, simulation, all_simulation_parts, frequency_mhz):\n        \"\"\"\n        Performs the final voxelization and grid update for a simulation.\n        This is a shared method for both Near-Field and Far-Field setups.\n        \"\"\"\n        self._log(\"    - Finalizing setup...\", log_type='progress')\n\n        voxeler_settings = self.emfdtd.AutomaticVoxelerSettings()\n        simulation.Add(voxeler_settings, all_simulation_parts)\n\n        import XCore\n        old_log_level = XCore.SetLogLevel(XCore.eLogCategory.Error)\n        simulation.UpdateAllMaterials()\n        XCore.SetLogLevel(old_log_level)\n\n        if self.config.get_setting('export_material_properties'):\n            self._log(\"--- Extracting Material Properties ---\", level='progress', log_type='header')\n            material_properties = []\n            for settings in simulation.AllSettings:\n                if isinstance(settings, self.emfdtd.MaterialSettings):\n                    try:\n                        self._log(f\"  - Material: '{settings.Name}'\", log_type='info')\n                        self._log(f\"    - Relative Permittivity: {settings.ElectricProps.RelativePermittivity:.4f}\", log_type='verbose')\n                        self._log(f\"    - Electric Conductivity (S/m): {settings.ElectricProps.Conductivity:.4f}\", log_type='verbose')\n                        self._log(f\"    - Mass Density (kg/m^3): {settings.MassDensity:.2f}\", log_type='verbose')\n                        material_properties.append({\n                            'Name': settings.Name,\n                            'RelativePermittivity': settings.ElectricProps.RelativePermittivity,\n                            'Conductivity': settings.ElectricProps.Conductivity,\n                            'MassDensity': settings.MassDensity\n                        })\n                    except Exception as e:\n                        self._log(f\"    - Could not extract properties for '{settings.Name}': {e}\", log_type='warning')\n            self._log(\"--- Finished Extracting Material Properties ---\", level='progress', log_type='header')\n\n            output_dir = \"analysis/cpw/data\"\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n            output_path = os.path.join(output_dir, f\"material_properties_{frequency_mhz}.pkl\")\n            with open(output_path, 'wb') as f:\n                pickle.dump(material_properties, f)\n            self._log(f\"--- Exported Material Properties to {output_path} ---\", level='progress', log_type='success')\n\n        simulation.UpdateGrid()\n        project_manager.save()\n        simulation.CreateVoxels()\n        self._log(\"    - Finalizing setup complete.\", log_type='success')\n</code></pre>"},{"location":"api/#src.setups.base_setup.BaseSetup.__init__","title":"<code>__init__(config, verbose_logger, progress_logger)</code>","text":"<p>Initializes the base setup.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The configuration object for the study.</p> required <code>verbose_logger</code> <code>Logger</code> <p>Logger for verbose output.</p> required <code>progress_logger</code> <code>Logger</code> <p>Logger for progress updates.</p> required Source code in <code>src\\setups\\base_setup.py</code> <pre><code>def __init__(self, config, verbose_logger, progress_logger):\n    \"\"\"\n    Initializes the base setup.\n\n    Args:\n        config (Config): The configuration object for the study.\n        verbose_logger (logging.Logger): Logger for verbose output.\n        progress_logger (logging.Logger): Logger for progress updates.\n    \"\"\"\n    self.config = config\n    self.verbose_logger = verbose_logger\n    self.progress_logger = progress_logger\n    import s4l_v1\n    self.s4l_v1 = s4l_v1\n    self.emfdtd = self.s4l_v1.simulation.emfdtd\n    self.model = self.s4l_v1.model\n</code></pre>"},{"location":"api/#src.setups.base_setup.BaseSetup.run_full_setup","title":"<code>run_full_setup(project_manager)</code>","text":"<p>This method must be implemented by subclasses to prepare the simulation scene.</p> <p>Parameters:</p> Name Type Description Default <code>project_manager</code> <code>ProjectManager</code> <p>The project manager to handle file operations.</p> required <p>Returns:</p> Type Description <p>The main simulation object.</p> Source code in <code>src\\setups\\base_setup.py</code> <pre><code>def run_full_setup(self, project_manager):\n    \"\"\"\n    This method must be implemented by subclasses to prepare the simulation scene.\n\n    Args:\n        project_manager (ProjectManager): The project manager to handle file operations.\n\n    Returns:\n        The main simulation object.\n    \"\"\"\n    raise NotImplementedError(\"The 'run_full_setup' method must be implemented by a subclass.\")\n</code></pre>"},{"location":"api/#src.setups.boundary_setup","title":"<code>boundary_setup</code>","text":""},{"location":"api/#src.setups.boundary_setup.BoundarySetup","title":"<code>BoundarySetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> <p>Configures the boundary conditions for the simulation.</p> Source code in <code>src\\setups\\boundary_setup.py</code> <pre><code>class BoundarySetup(BaseSetup):\n    \"\"\"\n    Configures the boundary conditions for the simulation.\n    \"\"\"\n    def __init__(self, config, simulation, verbose_logger, progress_logger):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.simulation = simulation\n\n    def setup_boundary_conditions(self):\n        \"\"\"\n        Sets up the boundary conditions based on the configuration.\n        \"\"\"\n        self._log(\"Setting up boundary conditions...\", log_type='progress')\n        solver_settings = self.config.get_solver_settings()\n        boundary_config = solver_settings.get(\"boundary_conditions\", {})\n\n        # Set Boundary Type (e.g., UpmlCpml)\n        bc_type = boundary_config.get(\"type\", \"UpmlCpml\")\n        self._log(f\"  - Setting global boundary conditions to: {bc_type}\", log_type='info')\n\n        global_boundaries = self.simulation.GlobalBoundarySettings\n        if global_boundaries:\n            bc_enum = global_boundaries.GlobalBoundaryType.enum\n            if hasattr(bc_enum, bc_type):\n                global_boundaries.GlobalBoundaryType = getattr(bc_enum, bc_type)\n                self._log(f\"    - Successfully set GlobalBoundaryType to {bc_type}\", log_type='verbose')\n            else:\n                self._log(f\"    - Warning: Invalid boundary condition type '{bc_type}'. Using default.\", log_type='warning')\n        else:\n            self._log(\"    - Warning: 'GlobalBoundarySettings' not found on simulation object.\", log_type='warning')\n\n        # Set PML Strength\n        strength = boundary_config.get(\"strength\", \"Medium\").capitalize()\n        self._log(f\"  - Setting PML strength to: {strength}\", log_type='info')\n\n        boundary_settings_list = [x for x in self.simulation.AllSettings if isinstance(x, self.emfdtd.BoundarySettings)]\n        if not boundary_settings_list:\n            self._log(\"  - No BoundarySettings found in simulation. Cannot set PML strength.\", log_type='warning')\n            return\n\n        boundary_settings = boundary_settings_list[0]\n\n        strength_enum = boundary_settings.PmlStrength.enum\n        if hasattr(strength_enum, strength):\n            boundary_settings.PmlStrength = getattr(strength_enum, strength)\n            self._log(f\"    - Successfully set PmlStrength to {strength}\", log_type='verbose')\n        else:\n            self._log(f\"    - Warning: Invalid PML strength '{strength}'. Using default (Medium).\", log_type='warning')\n            boundary_settings.PmlStrength = strength_enum.Medium\n</code></pre>"},{"location":"api/#src.setups.boundary_setup.BoundarySetup.setup_boundary_conditions","title":"<code>setup_boundary_conditions()</code>","text":"<p>Sets up the boundary conditions based on the configuration.</p> Source code in <code>src\\setups\\boundary_setup.py</code> <pre><code>def setup_boundary_conditions(self):\n    \"\"\"\n    Sets up the boundary conditions based on the configuration.\n    \"\"\"\n    self._log(\"Setting up boundary conditions...\", log_type='progress')\n    solver_settings = self.config.get_solver_settings()\n    boundary_config = solver_settings.get(\"boundary_conditions\", {})\n\n    # Set Boundary Type (e.g., UpmlCpml)\n    bc_type = boundary_config.get(\"type\", \"UpmlCpml\")\n    self._log(f\"  - Setting global boundary conditions to: {bc_type}\", log_type='info')\n\n    global_boundaries = self.simulation.GlobalBoundarySettings\n    if global_boundaries:\n        bc_enum = global_boundaries.GlobalBoundaryType.enum\n        if hasattr(bc_enum, bc_type):\n            global_boundaries.GlobalBoundaryType = getattr(bc_enum, bc_type)\n            self._log(f\"    - Successfully set GlobalBoundaryType to {bc_type}\", log_type='verbose')\n        else:\n            self._log(f\"    - Warning: Invalid boundary condition type '{bc_type}'. Using default.\", log_type='warning')\n    else:\n        self._log(\"    - Warning: 'GlobalBoundarySettings' not found on simulation object.\", log_type='warning')\n\n    # Set PML Strength\n    strength = boundary_config.get(\"strength\", \"Medium\").capitalize()\n    self._log(f\"  - Setting PML strength to: {strength}\", log_type='info')\n\n    boundary_settings_list = [x for x in self.simulation.AllSettings if isinstance(x, self.emfdtd.BoundarySettings)]\n    if not boundary_settings_list:\n        self._log(\"  - No BoundarySettings found in simulation. Cannot set PML strength.\", log_type='warning')\n        return\n\n    boundary_settings = boundary_settings_list[0]\n\n    strength_enum = boundary_settings.PmlStrength.enum\n    if hasattr(strength_enum, strength):\n        boundary_settings.PmlStrength = getattr(strength_enum, strength)\n        self._log(f\"    - Successfully set PmlStrength to {strength}\", log_type='verbose')\n    else:\n        self._log(f\"    - Warning: Invalid PML strength '{strength}'. Using default (Medium).\", log_type='warning')\n        boundary_settings.PmlStrength = strength_enum.Medium\n</code></pre>"},{"location":"api/#src.setups.far_field_setup","title":"<code>far_field_setup</code>","text":""},{"location":"api/#src.setups.far_field_setup.FarFieldSetup","title":"<code>FarFieldSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> <p>Configures a single far-field simulation for a specific direction and polarization.</p> Source code in <code>src\\setups\\far_field_setup.py</code> <pre><code>class FarFieldSetup(BaseSetup):\n    \"\"\"\n    Configures a single far-field simulation for a specific direction and polarization.\n    \"\"\"\n    def __init__(self, config, phantom_name, frequency_mhz, direction_name, polarization_name, project_manager, verbose_logger, progress_logger):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.phantom_name = phantom_name\n        self.frequency_mhz = frequency_mhz\n        self.direction_name = direction_name\n        self.polarization_name = polarization_name\n        self.project_manager = project_manager\n        self.simulation_type = self.config.get_setting('far_field_setup/type', 'environmental')\n        self.document = self.s4l_v1.document\n\n    def run_full_setup(self, phantom_setup):\n        \"\"\"\n        Executes the full sequence of setup steps for a single far-field simulation.\n        \"\"\"\n        self._log(f\"--- Setting up single Far-Field sim ---\", log_type='header')\n\n        # The phantom is now loaded once per project in the study.\n        # This setup will just add a simulation to the currently open project.\n        bbox_entity = self._create_or_get_simulation_bbox()\n\n        simulation = self._create_simulation_entity(bbox_entity)\n\n        self._apply_common_settings(simulation)\n\n        return simulation\n\n    def _create_simulation_entity(self, bbox_entity):\n        \"\"\"\n        Creates the EM FDTD simulation entity and its plane wave source.\n        \"\"\"\n        sim_name = f\"EM_FDTD_{self.phantom_name}_{self.frequency_mhz}MHz_{self.direction_name}_{self.polarization_name}\"\n        self._log(f\"  - Creating simulation: {sim_name}\", log_type='info')\n\n        simulation = self.emfdtd.Simulation()\n        simulation.Name = sim_name\n\n        # Set frequency for the entire simulation FIRST. This influences material properties.\n        simulation.Frequency = self.frequency_mhz, self.s4l_v1.units.MHz\n\n        plane_wave_source = self.emfdtd.PlaneWaveSourceSettings()\n        plane_wave_source.CenterFrequency = self.frequency_mhz, self.s4l_v1.units.MHz\n\n        # Calculate direction vector (k) and angles\n        direction_map = {\n            \"x_pos\": (0, 90), \"x_neg\": (180, 90),\n            \"y_pos\": (90, 90), \"y_neg\": (270, 90),\n            \"z_pos\": (0, 0),   \"z_neg\": (0, 180)\n        }\n        phi_deg, theta_deg = direction_map[self.direction_name]\n\n        plane_wave_source.Theta = theta_deg\n        plane_wave_source.Phi = phi_deg\n\n        # Set polarization (psi angle)\n        if self.polarization_name == 'theta':\n            plane_wave_source.Psi = 0\n        elif self.polarization_name == 'phi':\n            plane_wave_source.Psi = 90\n\n        simulation.Add(plane_wave_source, [bbox_entity])\n        self.document.AllSimulations.Add(simulation)\n\n        self._apply_simulation_time_and_termination(simulation, bbox_entity, self.frequency_mhz)\n\n        return simulation\n\n    def _create_or_get_simulation_bbox(self):\n        \"\"\"\n        Creates the simulation bounding box if it doesn't exist, otherwise returns the existing one.\n        \"\"\"\n        bbox_name = \"far_field_simulation_bbox\"\n        existing_bbox = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and e.Name == bbox_name), None)\n        if existing_bbox:\n            self._log(\"Found existing simulation bounding box.\", log_type='verbose')\n            return existing_bbox\n\n        self._log(\"Creating simulation bounding box for far-field...\", log_type='progress')\n        import XCoreModeling\n        phantom_entities = [e for e in self.model.AllEntities() if isinstance(e, XCoreModeling.TriangleMesh)]\n        if not phantom_entities:\n            raise RuntimeError(\"No phantom found to create bounding box.\")\n\n        bbox_min, bbox_max = self.model.GetBoundingBox(phantom_entities)\n\n        padding_mm = self.config.get_setting(\"simulation_parameters.bbox_padding_mm\", 50)\n\n        sim_bbox_min = np.array(bbox_min) - padding_mm\n        sim_bbox_max = np.array(bbox_max) + padding_mm\n\n        sim_bbox = XCoreModeling.CreateWireBlock(self.model.Vec3(sim_bbox_min), self.model.Vec3(sim_bbox_max))\n        sim_bbox.Name = bbox_name\n        self._log(f\"  - Created far-field simulation BBox with {padding_mm}mm padding.\", log_type='info')\n        return sim_bbox\n\n    def _apply_common_settings(self, simulation):\n        \"\"\"\n        Applies common material, gridding, and solver settings to the simulation.\n        \"\"\"\n        self._log(f\"Applying common settings to {simulation.Name}...\", log_type='progress')\n\n        material_setup = MaterialSetup(self.config, simulation, None, self.phantom_name, self.verbose_logger, self.progress_logger, free_space=False)\n        material_setup.assign_materials(phantom_only=True)\n\n        gridding_setup = GriddingSetup(self.config, simulation, None, None, self.verbose_logger, self.progress_logger, frequency_mhz=self.frequency_mhz)\n        gridding_setup.setup_gridding()\n\n        boundary_setup = BoundarySetup(self.config, simulation, self.verbose_logger, self.progress_logger)\n        boundary_setup.setup_boundary_conditions()\n\n        self._add_point_sensors(simulation, \"far_field_simulation_bbox\")\n\n        self._setup_solver_settings(simulation)\n\n        self._finalize_setup(self.project_manager, simulation, self.frequency_mhz)\n        self._log(\"Common settings applied.\", log_type='success')\n\n    def _finalize_setup(self, project_manager, simulation, frequency_mhz):\n        \"\"\"\n        Gathers all necessary entities for a far-field simulation and calls the shared\n        finalization method from the base class.\n        \"\"\"\n        bbox_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and e.Name == \"far_field_simulation_bbox\"), None)\n        if not bbox_entity:\n            raise RuntimeError(\"Could not find 'far_field_simulation_bbox' for voxelization.\")\n\n        import XCoreModeling\n        phantom_entities = [e for e in self.model.AllEntities() if isinstance(e, XCoreModeling.TriangleMesh)]\n        point_sensor_entities = [e for e in self.model.AllEntities() if \"Point Sensor Entity\" in e.Name]\n\n        all_simulation_parts = phantom_entities + [bbox_entity] + point_sensor_entities\n\n        super()._finalize_setup(project_manager, simulation, all_simulation_parts, frequency_mhz)\n</code></pre>"},{"location":"api/#src.setups.far_field_setup.FarFieldSetup.run_full_setup","title":"<code>run_full_setup(phantom_setup)</code>","text":"<p>Executes the full sequence of setup steps for a single far-field simulation.</p> Source code in <code>src\\setups\\far_field_setup.py</code> <pre><code>def run_full_setup(self, phantom_setup):\n    \"\"\"\n    Executes the full sequence of setup steps for a single far-field simulation.\n    \"\"\"\n    self._log(f\"--- Setting up single Far-Field sim ---\", log_type='header')\n\n    # The phantom is now loaded once per project in the study.\n    # This setup will just add a simulation to the currently open project.\n    bbox_entity = self._create_or_get_simulation_bbox()\n\n    simulation = self._create_simulation_entity(bbox_entity)\n\n    self._apply_common_settings(simulation)\n\n    return simulation\n</code></pre>"},{"location":"api/#src.setups.gridding_setup","title":"<code>gridding_setup</code>","text":""},{"location":"api/#src.setups.gridding_setup.GriddingSetup","title":"<code>GriddingSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> Source code in <code>src\\setups\\gridding_setup.py</code> <pre><code>class GriddingSetup(BaseSetup):\n    def __init__(self, config, simulation, placement_name, antenna, verbose_logger, progress_logger, frequency_mhz=None):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.simulation = simulation\n        self.placement_name = placement_name\n        self.antenna = antenna\n        self.frequency_mhz = frequency_mhz\n\n        import s4l_v1.units\n        self.units = s4l_v1.units\n\n    def setup_gridding(self, antenna_components=None):\n        \"\"\"\n        Sets up the gridding for the simulation. If antenna_components are provided,\n        it will also set up the subgrids for the antenna.\n        \"\"\"\n        self._log(\"Setting up gridding...\", log_type='progress')\n        self._setup_main_grid()\n        if antenna_components:\n            self._setup_subgrids(antenna_components)\n        else:\n            self._log(\"  - No antenna components provided, skipping subgridding.\", log_type='info')\n\n    def _setup_main_grid(self):\n        \"\"\"\n        Sets up the main grid based on the overall simulation bounding box,\n        including global gridding mode, padding, and resolution.\n        \"\"\"\n        gridding_params = self.config.get_gridding_parameters()\n        global_gridding_params = gridding_params.get(\"global_gridding\", {})\n        gridding_mode = global_gridding_params.get(\"grid_mode\", \"automatic\")\n\n        # Determine the name of the simulation bounding box\n        if self.simulation.Name.endswith(\"_freespace\"):\n            sim_bbox_name = \"freespace_simulation_bbox\"\n        elif self.placement_name:\n            sim_bbox_name = f\"{self.placement_name.lower()}_simulation_bbox\"\n        else:  # Far-field case\n            sim_bbox_name = \"far_field_simulation_bbox\"\n\n        # Find the bounding box entity\n        self._log(f\"  - Looking for global grid bounding box: '{sim_bbox_name}'\", log_type='verbose')\n        sim_bbox_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and e.Name == sim_bbox_name), None)\n        if not sim_bbox_entity:\n            raise RuntimeError(f\"Could not find simulation bounding box: '{sim_bbox_name}'\")\n\n        # Apply global grid settings\n        self.simulation.GlobalGridSettings.BoundingBox = self.model.GetBoundingBox([sim_bbox_entity])\n\n        if gridding_mode == \"automatic\":\n            self._log(\"  - Using automatic gridding.\", log_type='info')\n            self.simulation.GlobalGridSettings.DiscretizationMode = 'Automatic'\n\n            # Add the required grid object for the simulation box\n            added_grid_settings = self.simulation.AddAutomaticGridSettings([sim_bbox_entity])\n\n            # Map user-friendly refinement names to Sim4Life enums\n            refinement_mapping = {\n                \"Very Fine\": \"AutoRefinementVeryFine\",\n                \"Fine\": \"AutoRefinementFine\",\n                \"Default\": \"AutoRefinementDefault\",\n                \"Coarse\": \"AutoRefinementCoarse\",\n                \"Very Coarse\": \"AutoRefinementVeryCoarse\"\n            }\n\n            # Set refinement based on config, with a default value\n            user_refinement_level = global_gridding_params.get(\"refinement\", \"Default\")\n            s4l_refinement_level = refinement_mapping.get(user_refinement_level, \"AutoRefinementDefault\")\n\n            # Apply the same setting to both the global and the added grid\n            self.simulation.GlobalGridSettings.AutoRefinement = s4l_refinement_level\n            added_grid_settings.AutoRefinement = s4l_refinement_level\n            self._log(f\"  - Global and added automatic grid set with refinement level: {user_refinement_level} ({s4l_refinement_level})\", log_type='verbose')\n\n        elif gridding_mode == \"manual\":\n            self._log(\"  - Using manual gridding.\", log_type='info')\n            self.simulation.GlobalGridSettings.DiscretizationMode = 'Manual'\n\n            # Add the required grid object for the simulation box\n            added_manual_grid = self.simulation.AddManualGridSettings([sim_bbox_entity])\n\n            global_grid_res_mm = None\n            log_source = \"default\"\n\n            # Try to get per-frequency gridding first\n            if self.frequency_mhz:\n                per_freq_gridding = gridding_params.get(\"global_gridding_per_frequency\")\n                if per_freq_gridding and isinstance(per_freq_gridding, dict):\n                    freq_key = str(int(self.frequency_mhz))\n                    if freq_key in per_freq_gridding:\n                        global_grid_res_mm = per_freq_gridding[freq_key]\n                        log_source = f\"frequency-specific ({self.frequency_mhz}MHz)\"\n\n            # Fallback to global gridding if per-frequency is not found\n            if global_grid_res_mm is None:\n                global_grid_res_mm = global_gridding_params.get(\"manual_fallback_max_step_mm\", 5.0)\n                log_source = \"global\"\n\n            max_step_setting = (np.array([global_grid_res_mm] * 3), self.units.MilliMeters)\n\n            # Apply the same setting to both the global and the added grid\n            self.simulation.GlobalGridSettings.MaxStep = max_step_setting\n            added_manual_grid.MaxStep = max_step_setting\n            self._log(f\"  - Global and added manual grid set with {log_source} resolution: {global_grid_res_mm} mm.\", log_type='verbose')\n        else:\n            raise ValueError(f\"Unsupported gridding_mode: {gridding_mode}\")\n\n        # Setup Padding\n        padding_params = gridding_params.get(\"padding\", {})\n        padding_mode = padding_params.get(\"padding_mode\", \"automatic\")\n        global_grid_settings = self.simulation.GlobalGridSettings\n\n        if padding_mode == \"manual\":\n            self._log(\"  - Using manual padding.\", log_type='info')\n            global_grid_settings.PaddingMode = global_grid_settings.PaddingMode.enum.Manual\n\n            bottom_padding = np.array(padding_params.get(\"manual_bottom_padding_mm\", [0,0,0]))\n            top_padding = np.array(padding_params.get(\"manual_top_padding_mm\", [0,0,0]))\n\n            global_grid_settings.BottomPadding = bottom_padding, self.units.MilliMeters\n            global_grid_settings.TopPadding = top_padding, self.units.MilliMeters\n            self._log(f\"    - Manual padding set: Bottom={bottom_padding}mm, Top={top_padding}mm\", log_type='verbose')\n        else:\n            self._log(\"  - Using automatic padding.\", log_type='info')\n            global_grid_settings.PaddingMode = global_grid_settings.PaddingMode.enum.Automatic\n\n    def _setup_subgrids(self, antenna_components):\n        # Antenna-specific gridding\n        if not self.antenna:\n            self._log(\"  - No antenna provided, skipping subgridding.\", log_type='info')\n            return\n\n        antenna_config = self.antenna.get_config_for_frequency()\n        gridding_config = antenna_config.get(\"gridding\")\n        if gridding_config:\n            # Automatic settings\n            automatic_components = [antenna_components[name] for name in gridding_config.get(\"automatic\", []) if name in antenna_components]\n            if automatic_components:\n                self.simulation.AddAutomaticGridSettings(automatic_components)\n\n            # Manual settings\n            manual_grid_step = gridding_config.get(\"manual_grid_step\", {})\n            resolution = gridding_config.get(\"resolution\", {})\n\n            for grid_type, comp_names in gridding_config.get(\"manual\", {}).items():\n                components_to_grid = [antenna_components[name] for name in comp_names if name in antenna_components]\n\n                if components_to_grid:\n                    if grid_type in manual_grid_step:\n                        res = manual_grid_step[grid_type]\n                    else:\n                        continue\n\n                    geom_res = resolution.get(grid_type, [1.0, 1.0, 1.0])\n\n                    if self.placement_name == \"by_cheek\":\n                        oriented_res = [res, res, res]\n                        oriented_geom_res = [geom_res, geom_res, geom_res]\n                    else:\n                        oriented_res = res\n                        oriented_geom_res = geom_res\n\n                    manual_grid = self.simulation.AddManualGridSettings(components_to_grid)\n                    manual_grid.MaxStep = np.array(oriented_res), self.units.MilliMeters\n                    manual_grid.Resolution = np.array(oriented_geom_res), self.units.MilliMeters\n        else:\n            source_entity = antenna_components[self.antenna.get_source_entity_name()]\n            self.simulation.AddAutomaticGridSettings([source_entity])\n</code></pre>"},{"location":"api/#src.setups.gridding_setup.GriddingSetup.setup_gridding","title":"<code>setup_gridding(antenna_components=None)</code>","text":"<p>Sets up the gridding for the simulation. If antenna_components are provided, it will also set up the subgrids for the antenna.</p> Source code in <code>src\\setups\\gridding_setup.py</code> <pre><code>def setup_gridding(self, antenna_components=None):\n    \"\"\"\n    Sets up the gridding for the simulation. If antenna_components are provided,\n    it will also set up the subgrids for the antenna.\n    \"\"\"\n    self._log(\"Setting up gridding...\", log_type='progress')\n    self._setup_main_grid()\n    if antenna_components:\n        self._setup_subgrids(antenna_components)\n    else:\n        self._log(\"  - No antenna components provided, skipping subgridding.\", log_type='info')\n</code></pre>"},{"location":"api/#src.setups.material_setup","title":"<code>material_setup</code>","text":""},{"location":"api/#src.setups.material_setup.MaterialSetup","title":"<code>MaterialSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> Source code in <code>src\\setups\\material_setup.py</code> <pre><code>class MaterialSetup(BaseSetup):\n    def __init__(self, config, simulation, antenna, phantom_name, verbose_logger, progress_logger, free_space=False):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.simulation = simulation\n        self.antenna = antenna\n        self.phantom_name = phantom_name\n        self.free_space = free_space\n\n        import s4l_v1.materials.database\n        import XCoreModeling\n\n        self.database = s4l_v1.materials.database\n        self.XCoreModeling = XCoreModeling\n\n    def assign_materials(self, antenna_components=None, phantom_only=False):\n        \"\"\"\n        Assigns materials to the simulation entities.\n        \"\"\"\n        self._log(\"Assigning materials...\", log_type='progress')\n\n        # Background material\n        background_settings = self.simulation.raw.BackgroundMaterialSettings()\n        air_material = self.database[\"Generic 1.1\"][\"Air\"]\n        self.simulation.raw.AssignMaterial(background_settings, air_material)\n\n        # Phantom materials\n        if not self.free_space:\n            self._assign_phantom_materials()\n\n        # Antenna materials\n        if not phantom_only:\n            if not antenna_components:\n                raise ValueError(\"antenna_components must be provided when not in phantom_only mode.\")\n            self._assign_antenna_materials(antenna_components)\n\n    def _assign_phantom_materials(self):\n        lock_file_path = os.path.join(self.config.base_dir, 'material_db.lock')\n\n        # Acquire lock\n        while True:\n            try:\n                # Use exclusive creation mode to atomically create the file\n                with open(lock_file_path, 'x'):\n                    break  # Lock acquired\n            except FileExistsError:\n                time.sleep(0.5)  # Wait before retrying\n\n        try:\n            all_entities = self.model.AllEntities()\n            phantom_parts = [e for e in all_entities if isinstance(e, self.XCoreModeling.TriangleMesh)]\n\n            name_mapping = self.config.get_material_mapping(self.phantom_name)\n\n            material_groups = {}\n            for part in phantom_parts:\n                base_name = part.Name.split('(')[0].strip()\n                material_name = name_mapping.get(base_name, base_name.replace('_', ' '))\n                material_groups.setdefault(material_name, []).append(part)\n\n            for material_name, entities in material_groups.items():\n                try:\n                    mat = self.database[\"IT'IS 4.2\"][material_name]\n                    material_settings = self.emfdtd.MaterialSettings()\n                    self.simulation.LinkMaterialWithDatabase(material_settings, mat)\n                    self.simulation.Add(material_settings, entities)\n                except KeyError:\n                    self._log(f\"    - Warning: Could not find material '{material_name}' in IT'IS 4.2 database.\", log_type='warning')\n\n        finally:\n            # Release lock\n            if os.path.exists(lock_file_path):\n                os.remove(lock_file_path)\n\n    def _assign_antenna_materials(self, antenna_components):\n        antenna_config = self.antenna.get_config_for_frequency()\n        material_mappings = antenna_config.get(\"materials\", {})\n\n        for comp_name, mat_name in material_mappings.items():\n            if comp_name in antenna_components:\n                entity = antenna_components[comp_name]\n                material_settings = self.emfdtd.MaterialSettings()\n\n                excitation_type = self.config.get_excitation_type()\n                if \"Copper\" in mat_name and self.free_space and excitation_type.lower() == 'gaussian':\n                    material_settings.Type = \"PEC\"\n                    self.simulation.Add(material_settings, [entity])\n                    self._log(\"\\n\" + \"=\"*80, log_type='warning')\n                    self._log(f\"  WARNING: Forcing material for '{comp_name}' to PEC.\", log_type='warning')\n                    self._log(\"           This is a required workaround because Sim4Life does not yet support\", log_type='warning')\n                    self._log(\"           Gaussian excitation with dispersive materials like Copper.\", log_type='warning')\n                    self._log(\"=\"*80 + \"\\n\", log_type='warning')\n                elif mat_name.lower() == 'pec':\n                    material_settings.Type = \"PEC\"\n                    self.simulation.Add(material_settings, [entity])\n                    self._log(f\"  - Assigned 'PEC' to '{comp_name}'.\", log_type='info')\n                else:\n                    try:\n                        db_name = \"IT'IS 4.2\" if \"Rogers\" in mat_name else \"Generic 1.1\"\n                        mat = self.database[db_name][mat_name]\n                        self.simulation.LinkMaterialWithDatabase(material_settings, mat)\n                        self.simulation.Add(material_settings, [entity])\n                        self._log(f\"  - Assigned '{mat_name}' to '{comp_name}'.\", log_type='info')\n                    except KeyError:\n                        self._log(f\"    - Warning: Could not find material '{mat_name}' in database.\", log_type='warning')\n            else:\n                self._log(f\"    - Warning: Could not find component '{comp_name}' to assign material.\", log_type='warning')\n</code></pre>"},{"location":"api/#src.setups.material_setup.MaterialSetup.assign_materials","title":"<code>assign_materials(antenna_components=None, phantom_only=False)</code>","text":"<p>Assigns materials to the simulation entities.</p> Source code in <code>src\\setups\\material_setup.py</code> <pre><code>def assign_materials(self, antenna_components=None, phantom_only=False):\n    \"\"\"\n    Assigns materials to the simulation entities.\n    \"\"\"\n    self._log(\"Assigning materials...\", log_type='progress')\n\n    # Background material\n    background_settings = self.simulation.raw.BackgroundMaterialSettings()\n    air_material = self.database[\"Generic 1.1\"][\"Air\"]\n    self.simulation.raw.AssignMaterial(background_settings, air_material)\n\n    # Phantom materials\n    if not self.free_space:\n        self._assign_phantom_materials()\n\n    # Antenna materials\n    if not phantom_only:\n        if not antenna_components:\n            raise ValueError(\"antenna_components must be provided when not in phantom_only mode.\")\n        self._assign_antenna_materials(antenna_components)\n</code></pre>"},{"location":"api/#src.setups.near_field_setup","title":"<code>near_field_setup</code>","text":""},{"location":"api/#src.setups.near_field_setup.NearFieldSetup","title":"<code>NearFieldSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> <p>Configures the entire simulation environment within Sim4Life by coordinating specialized setup modules.</p> Source code in <code>src\\setups\\near_field_setup.py</code> <pre><code>class NearFieldSetup(BaseSetup):\n    \"\"\"\n    Configures the entire simulation environment within Sim4Life by coordinating\n    specialized setup modules.\n    \"\"\"\n    def __init__(self, config, phantom_name, frequency_mhz, placement_name, antenna, verbose_logger, progress_logger, free_space=False):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.phantom_name = phantom_name\n        self.frequency_mhz = frequency_mhz\n        self.placement_name = placement_name\n        self.antenna = antenna\n        self.free_space = free_space\n\n        if placement_name.startswith('front_of_eyes'):\n            self.base_placement_name = 'front_of_eyes'\n        elif placement_name.startswith('by_cheek'):\n            self.base_placement_name = 'by_cheek'\n        elif placement_name.startswith('by_belly'):\n            self.base_placement_name = 'by_belly'\n        else:\n            self.base_placement_name = placement_name\n\n        # S4L modules\n        import XCoreModeling\n        self.document = self.s4l_v1.document\n        self.XCoreModeling = XCoreModeling\n\n    def run_full_setup(self, project_manager, lock=None):\n        \"\"\"\n        Executes the full sequence of setup steps.\n        \"\"\"\n        self._log(\"Running full simulation setup...\", log_type='progress')\n\n        # Create or open the project file. This is the first step.\n        project_manager.create_or_open_project(self.phantom_name, self.frequency_mhz, self.placement_name)\n\n        if not self.free_space:\n            phantom_setup = PhantomSetup(self.config, self.phantom_name, self.verbose_logger, self.progress_logger)\n            phantom_setup.ensure_phantom_is_loaded()\n            self._setup_bounding_boxes()\n\n        placement_setup = PlacementSetup(self.config, self.phantom_name, self.frequency_mhz, self.placement_name, self.antenna, self.verbose_logger, self.progress_logger, self.free_space)\n        placement_setup.place_antenna()\n\n        self._create_simulation_bbox()\n\n        simulation = self._setup_simulation_entity()\n\n        antenna_components = self._get_antenna_components()\n\n        material_setup = MaterialSetup(self.config, simulation, self.antenna, self.phantom_name, self.verbose_logger, self.progress_logger, self.free_space)\n        material_setup.assign_materials(antenna_components)\n\n        gridding_setup = GriddingSetup(self.config, simulation, self.placement_name, self.antenna, self.verbose_logger, self.progress_logger, frequency_mhz=self.frequency_mhz)\n        gridding_setup.setup_gridding(antenna_components)\n\n        boundary_setup = BoundarySetup(self.config, simulation, self.verbose_logger, self.progress_logger)\n        boundary_setup.setup_boundary_conditions()\n\n        source_setup = SourceSetup(self.config, simulation, self.frequency_mhz, self.antenna, self.verbose_logger, self.progress_logger, self.free_space)\n        source_setup.setup_source_and_sensors(antenna_components)\n\n        sim_bbox_name = f\"{self.placement_name.lower()}_simulation_bbox\"\n        self._add_point_sensors(simulation, sim_bbox_name)\n\n        self._finalize_setup(project_manager, simulation, antenna_components)\n\n        self._log(\"Full simulation setup complete.\", log_type='success')\n        return simulation\n\n    def _get_antenna_components(self):\n        # The antenna group name changes during placement. We need to find it regardless of its state.\n        # Initial name: \"Antenna {freq} MHz\"\n        # Placed name:  \"Antenna {freq} MHz (Placement)\"\n\n        placed_name = f\"Antenna {self.frequency_mhz} MHz ({self.placement_name})\"\n        initial_name = f\"Antenna {self.frequency_mhz} MHz\"\n\n        all_entities = self.model.AllEntities()\n\n        antenna_group = next((e for e in all_entities if hasattr(e, 'Name') and e.Name == placed_name), None)\n        if not antenna_group:\n            antenna_group = next((e for e in all_entities if hasattr(e, 'Name') and e.Name == initial_name), None)\n\n        if not antenna_group:\n            raise RuntimeError(f\"Could not find antenna group. Looked for '{placed_name}' and '{initial_name}'.\")\n\n        flat_component_list = []\n        for entity in antenna_group.Entities:\n            if hasattr(entity, 'History') and \"Union\" in entity.History:\n                flat_component_list.extend(entity.GetChildren())\n            else:\n                flat_component_list.append(entity)\n\n        return {e.Name: e for e in flat_component_list}\n\n    def _setup_bounding_boxes(self):\n        \"\"\"\n        Creates the head and trunk bounding boxes.\n        \"\"\"\n        self._log(\"Setting up bounding boxes...\", log_type='progress')\n        all_entities = self.model.AllEntities()\n\n        phantom_config = self.config.get_phantom_config(self.phantom_name.lower())\n        if not phantom_config:\n            raise ValueError(f\"Configuration for '{self.phantom_name.lower()}' not found.\")\n\n        head_bbox_name = f\"{self.phantom_name.lower()}_Head_BBox\"\n        trunk_bbox_name = f\"{self.phantom_name.lower()}_Trunk_BBox\"\n\n        entities_to_delete = [e for e in all_entities if hasattr(e, 'Name') and e.Name in [head_bbox_name, trunk_bbox_name]]\n        for entity in entities_to_delete:\n            self._log(f\"  - Deleting existing entity: {entity.Name}\", log_type='verbose')\n            entity.Delete()\n\n        all_entities = self.model.AllEntities()\n        tissue_entities = [e for e in all_entities if isinstance(e, self.XCoreModeling.TriangleMesh)]\n        bbox_min, bbox_max = self.model.GetBoundingBox(tissue_entities)\n\n        # Head BBox\n        ear_skin_entity = next((e for e in all_entities if hasattr(e, 'Name') and e.Name == \"Ear_skin\"), None)\n        if not ear_skin_entity:\n            head_x_min, head_x_max = bbox_min[0], bbox_max[0]\n        else:\n            ear_bbox_min, ear_bbox_max = self.model.GetBoundingBox([ear_skin_entity])\n            head_x_min, head_x_max = ear_bbox_min[0], ear_bbox_max[0]\n\n        head_y_sep = phantom_config['head_y_separation']\n        back_of_head_y = phantom_config.get('back_of_head', bbox_min[1])\n        head_bbox_min_vec = self.model.Vec3(head_x_min, back_of_head_y, head_y_sep)\n        head_bbox_max_vec = self.model.Vec3(head_x_max, bbox_max[1], bbox_max[2])\n        head_bbox = self.XCoreModeling.CreateWireBlock(head_bbox_min_vec, head_bbox_max_vec)\n        head_bbox.Name = head_bbox_name\n        self._log(\"  - Head BBox created.\", log_type='info')\n\n        # Trunk BBox\n        trunk_z_sep = phantom_config['trunk_z_separation']\n        chest_y_ext = phantom_config['chest_extension']\n        trunk_bbox_min_vec = self.model.Vec3(bbox_min[0], bbox_min[1], trunk_z_sep)\n        trunk_bbox_max_vec = self.model.Vec3(bbox_max[0], chest_y_ext, head_y_sep)\n        trunk_bbox = self.XCoreModeling.CreateWireBlock(trunk_bbox_min_vec, trunk_bbox_max_vec)\n        trunk_bbox.Name = trunk_bbox_name\n        self._log(\"  - Trunk BBox created.\", log_type='info')\n\n    def _create_simulation_bbox(self):\n        if self.free_space:\n            antenna_bbox_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and \"Antenna bounding box\" in e.Name), None)\n            antenna_bbox_min, antenna_bbox_max = self.model.GetBoundingBox([antenna_bbox_entity])\n            expansion = self.config.get_freespace_expansion()\n            sim_bbox_min = np.array(antenna_bbox_min) - np.array(expansion)\n            sim_bbox_max = np.array(antenna_bbox_max) + np.array(expansion)\n            sim_bbox = self.XCoreModeling.CreateWireBlock(self.model.Vec3(sim_bbox_min), self.model.Vec3(sim_bbox_max))\n            sim_bbox.Name = \"freespace_simulation_bbox\"\n            self._log(f\"  - Created free-space simulation BBox.\", log_type='info')\n        else:\n            antenna_bbox_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and \"Antenna bounding box\" in e.Name), None)\n            if self.base_placement_name in ['front_of_eyes', 'by_cheek']:\n                bbox_to_combine_name = f\"{self.phantom_name.lower()}_Head_BBox\"\n            else:\n                bbox_to_combine_name = f\"{self.phantom_name.lower()}_Trunk_BBox\"\n\n            bbox_to_combine = self.model.AllEntities()[bbox_to_combine_name]\n\n            combined_bbox_min, combined_bbox_max = self.model.GetBoundingBox([bbox_to_combine, antenna_bbox_entity])\n            sim_bbox = self.XCoreModeling.CreateWireBlock(combined_bbox_min, combined_bbox_max)\n            sim_bbox.Name = f\"{self.placement_name.lower()}_simulation_bbox\"\n            self._log(f\"  - Combined BBox created for {self.placement_name}.\", log_type='info')\n\n    def _setup_simulation_entity(self):\n        \"\"\"\n        Creates and configures the base EM-FDTD simulation entity.\n        \"\"\"\n        self._log(\"Setting up simulation entity...\", log_type='progress')\n\n        if self.document.AllSimulations:\n            for sim in list(self.document.AllSimulations):\n                self.document.AllSimulations.Remove(sim)\n\n        sim_name = f\"EM_FDTD_{self.phantom_name}_{self.frequency_mhz}MHz_{self.placement_name}\"\n        if self.free_space:\n            sim_name += \"_freespace\"\n        simulation = self.emfdtd.Simulation()\n        simulation.Name = sim_name\n\n        import s4l_v1.units\n        simulation.Frequency = self.frequency_mhz, s4l_v1.units.MHz\n\n        self.document.AllSimulations.Add(simulation)\n\n        self._setup_solver_settings(simulation)\n\n        if self.free_space:\n            sim_bbox_name = \"freespace_simulation_bbox\"\n        else:\n            sim_bbox_name = f\"{self.placement_name.lower()}_simulation_bbox\"\n\n        sim_bbox_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and e.Name == sim_bbox_name), None)\n        if not sim_bbox_entity:\n            raise RuntimeError(f\"Could not find simulation bounding box: '{sim_bbox_name}'\")\n\n        self._apply_simulation_time_and_termination(simulation, sim_bbox_entity, self.frequency_mhz)\n\n        return simulation\n\n    def _finalize_setup(self, project_manager, simulation, antenna_components):\n        all_antenna_parts = list(antenna_components.values())\n        point_sensor_entities = [e for e in self.model.AllEntities() if \"Point Sensor Entity\" in e.Name]\n\n        if self.free_space:\n            sim_bbox_name = \"freespace_simulation_bbox\"\n        else:\n            sim_bbox_name = f\"{self.placement_name.lower()}_simulation_bbox\"\n\n        sim_bbox_entity = next((e for e in self.model.AllEntities() if hasattr(e, 'Name') and e.Name == sim_bbox_name), None)\n        if not sim_bbox_entity:\n            raise RuntimeError(f\"Could not find simulation bounding box: '{sim_bbox_name}' for voxelization.\")\n\n        phantom_entities = [e for e in self.model.AllEntities() if isinstance(e, self.XCoreModeling.TriangleMesh)]\n        all_simulation_parts = phantom_entities + all_antenna_parts + point_sensor_entities + [sim_bbox_entity]\n\n        super()._finalize_setup(project_manager, simulation, all_simulation_parts, self.frequency_mhz)\n</code></pre>"},{"location":"api/#src.setups.near_field_setup.NearFieldSetup.run_full_setup","title":"<code>run_full_setup(project_manager, lock=None)</code>","text":"<p>Executes the full sequence of setup steps.</p> Source code in <code>src\\setups\\near_field_setup.py</code> <pre><code>def run_full_setup(self, project_manager, lock=None):\n    \"\"\"\n    Executes the full sequence of setup steps.\n    \"\"\"\n    self._log(\"Running full simulation setup...\", log_type='progress')\n\n    # Create or open the project file. This is the first step.\n    project_manager.create_or_open_project(self.phantom_name, self.frequency_mhz, self.placement_name)\n\n    if not self.free_space:\n        phantom_setup = PhantomSetup(self.config, self.phantom_name, self.verbose_logger, self.progress_logger)\n        phantom_setup.ensure_phantom_is_loaded()\n        self._setup_bounding_boxes()\n\n    placement_setup = PlacementSetup(self.config, self.phantom_name, self.frequency_mhz, self.placement_name, self.antenna, self.verbose_logger, self.progress_logger, self.free_space)\n    placement_setup.place_antenna()\n\n    self._create_simulation_bbox()\n\n    simulation = self._setup_simulation_entity()\n\n    antenna_components = self._get_antenna_components()\n\n    material_setup = MaterialSetup(self.config, simulation, self.antenna, self.phantom_name, self.verbose_logger, self.progress_logger, self.free_space)\n    material_setup.assign_materials(antenna_components)\n\n    gridding_setup = GriddingSetup(self.config, simulation, self.placement_name, self.antenna, self.verbose_logger, self.progress_logger, frequency_mhz=self.frequency_mhz)\n    gridding_setup.setup_gridding(antenna_components)\n\n    boundary_setup = BoundarySetup(self.config, simulation, self.verbose_logger, self.progress_logger)\n    boundary_setup.setup_boundary_conditions()\n\n    source_setup = SourceSetup(self.config, simulation, self.frequency_mhz, self.antenna, self.verbose_logger, self.progress_logger, self.free_space)\n    source_setup.setup_source_and_sensors(antenna_components)\n\n    sim_bbox_name = f\"{self.placement_name.lower()}_simulation_bbox\"\n    self._add_point_sensors(simulation, sim_bbox_name)\n\n    self._finalize_setup(project_manager, simulation, antenna_components)\n\n    self._log(\"Full simulation setup complete.\", log_type='success')\n    return simulation\n</code></pre>"},{"location":"api/#src.setups.phantom_setup","title":"<code>phantom_setup</code>","text":""},{"location":"api/#src.setups.phantom_setup.PhantomSetup","title":"<code>PhantomSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> Source code in <code>src\\setups\\phantom_setup.py</code> <pre><code>class PhantomSetup(BaseSetup):\n    def __init__(self, config, phantom_name, verbose_logger, progress_logger):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.phantom_name = phantom_name\n\n        import s4l_v1.model\n        import s4l_v1.data\n        import XCoreModeling\n\n        self.model = s4l_v1.model\n        self.data = s4l_v1.data\n        self.XCoreModeling = XCoreModeling\n\n    def _log(self, message, log_type='default'):\n        self.verbose_logger.info(message, extra={'log_type': log_type})\n\n    def ensure_phantom_is_loaded(self):\n        \"\"\"\n        Ensures the phantom model is loaded into the current document.\n        \"\"\"\n        self._log(\"--- Running Phantom Check ---\", log_type='header')\n        all_entities = self.model.AllEntities()\n        self._log(f\"Found {len(all_entities)} total entities in the project.\", log_type='info')\n\n        is_loaded = False\n        for i, entity in enumerate(all_entities):\n            if hasattr(entity, 'Name'):\n                entity_name_lower = entity.Name.lower()\n                phantom_name_lower = self.phantom_name.lower()\n                if phantom_name_lower in entity_name_lower:\n                    is_loaded = True\n                    break\n            else:\n                self._log(f\"  - Entity {i}: (No 'Name' attribute)\", log_type='verbose')\n\n        if is_loaded:\n            self._log(\"--- Phantom Check Result: Phantom model is already present. ---\", log_type='success')\n            return True\n        else:\n            self._log(\"--- Phantom Check Result: Phantom not found in project. ---\", log_type='warning')\n\n        study_type = self.config.get_setting('study_type')\n        if study_type == 'near_field' or study_type == 'far_field':\n            sab_path = os.path.join(self.config.base_dir, 'data', 'phantoms', f\"{self.phantom_name}.sab\")\n            if os.path.exists(sab_path):\n                self._log(f\"Phantom not found in document. Importing from '{sab_path}'...\", log_type='info')\n                self.XCoreModeling.Import(sab_path)\n                self._log(\"Phantom imported successfully.\", log_type='success')\n                return True\n\n            self._log(f\"Local .sab file not found. Attempting to download '{self.phantom_name}'...\", log_type='info')\n            available_downloads = self.data.GetAvailableDownloads()\n            phantom_to_download = next((item for item in available_downloads if self.phantom_name.lower() in item.Name.lower()), None)\n\n            if not phantom_to_download:\n                raise FileNotFoundError(f\"Phantom '{self.phantom_name}' not found for download or in local files.\")\n\n            self._log(f\"Found '{phantom_to_download.Name}'. Downloading...\", log_type='info')\n            download_email = self.config.get_setting('download_email', 'example@example.com')\n            self.data.DownloadModel(phantom_to_download, email=download_email, directory=os.path.join(self.config.base_dir, 'data', 'phantoms'))\n            self._log(\"Phantom downloaded successfully. Please re-run the script to import the new .sab file.\", log_type='success')\n            return False\n        else:\n            raise ValueError(f\"Unknown study type: {study_type}\")\n</code></pre>"},{"location":"api/#src.setups.phantom_setup.PhantomSetup.ensure_phantom_is_loaded","title":"<code>ensure_phantom_is_loaded()</code>","text":"<p>Ensures the phantom model is loaded into the current document.</p> Source code in <code>src\\setups\\phantom_setup.py</code> <pre><code>def ensure_phantom_is_loaded(self):\n    \"\"\"\n    Ensures the phantom model is loaded into the current document.\n    \"\"\"\n    self._log(\"--- Running Phantom Check ---\", log_type='header')\n    all_entities = self.model.AllEntities()\n    self._log(f\"Found {len(all_entities)} total entities in the project.\", log_type='info')\n\n    is_loaded = False\n    for i, entity in enumerate(all_entities):\n        if hasattr(entity, 'Name'):\n            entity_name_lower = entity.Name.lower()\n            phantom_name_lower = self.phantom_name.lower()\n            if phantom_name_lower in entity_name_lower:\n                is_loaded = True\n                break\n        else:\n            self._log(f\"  - Entity {i}: (No 'Name' attribute)\", log_type='verbose')\n\n    if is_loaded:\n        self._log(\"--- Phantom Check Result: Phantom model is already present. ---\", log_type='success')\n        return True\n    else:\n        self._log(\"--- Phantom Check Result: Phantom not found in project. ---\", log_type='warning')\n\n    study_type = self.config.get_setting('study_type')\n    if study_type == 'near_field' or study_type == 'far_field':\n        sab_path = os.path.join(self.config.base_dir, 'data', 'phantoms', f\"{self.phantom_name}.sab\")\n        if os.path.exists(sab_path):\n            self._log(f\"Phantom not found in document. Importing from '{sab_path}'...\", log_type='info')\n            self.XCoreModeling.Import(sab_path)\n            self._log(\"Phantom imported successfully.\", log_type='success')\n            return True\n\n        self._log(f\"Local .sab file not found. Attempting to download '{self.phantom_name}'...\", log_type='info')\n        available_downloads = self.data.GetAvailableDownloads()\n        phantom_to_download = next((item for item in available_downloads if self.phantom_name.lower() in item.Name.lower()), None)\n\n        if not phantom_to_download:\n            raise FileNotFoundError(f\"Phantom '{self.phantom_name}' not found for download or in local files.\")\n\n        self._log(f\"Found '{phantom_to_download.Name}'. Downloading...\", log_type='info')\n        download_email = self.config.get_setting('download_email', 'example@example.com')\n        self.data.DownloadModel(phantom_to_download, email=download_email, directory=os.path.join(self.config.base_dir, 'data', 'phantoms'))\n        self._log(\"Phantom downloaded successfully. Please re-run the script to import the new .sab file.\", log_type='success')\n        return False\n    else:\n        raise ValueError(f\"Unknown study type: {study_type}\")\n</code></pre>"},{"location":"api/#src.setups.placement_setup","title":"<code>placement_setup</code>","text":""},{"location":"api/#src.setups.placement_setup.PlacementSetup","title":"<code>PlacementSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> Source code in <code>src\\setups\\placement_setup.py</code> <pre><code>class PlacementSetup(BaseSetup):\n    def __init__(self, config, phantom_name, frequency_mhz, placement_name, antenna, verbose_logger, progress_logger, free_space=False):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.phantom_name = phantom_name\n        self.frequency_mhz = frequency_mhz\n        self.placement_name = placement_name\n        self.antenna = antenna\n        self.free_space = free_space\n\n        # Parse the detailed placement name, e.g., \"front_of_eyes_center_vertical\"\n        try:\n            parts = placement_name.split('_')\n            if placement_name.startswith('front_of_eyes'):\n                self.base_placement_name = 'front_of_eyes'\n                self.position_name = parts[3]\n                self.orientation_name = parts[4]\n            elif placement_name.startswith('by_cheek'):\n                self.base_placement_name = 'by_cheek'\n                self.position_name = parts[2]\n                self.orientation_name = parts[3]\n            elif placement_name.startswith('by_belly'):\n                self.base_placement_name = 'by_belly'\n                self.position_name = parts[2]\n                self.orientation_name = parts[3]\n            else:\n                # Fallback for old single placement names or unknown format\n                self.base_placement_name = placement_name\n                self.position_name = \"default\"\n                self.orientation_name = \"default\"\n\n        except IndexError:\n            # Fallback for old single placement names\n            self.base_placement_name = placement_name\n            self.position_name = \"default\"\n            self.orientation_name = \"default\"\n\n        import XCoreMath\n        self.XCoreMath = XCoreMath\n\n    def place_antenna(self):\n        \"\"\"\n        Places and orients the antenna by composing a single transformation matrix\n        and applying it once.\n        \"\"\"\n        self._log(f\"--- Starting Placement: {self.base_placement_name} - {self.position_name} - {self.orientation_name} ---\", log_type='header')\n\n        placements_config = self.config.get_phantom_placements(self.phantom_name.lower())\n        if not self.free_space and not placements_config.get(f\"do_{self.base_placement_name}\"):\n            self._log(f\"Placement '{self.base_placement_name}' is disabled in the configuration.\", log_type='info')\n            return\n\n        base_target_point, orientation_rotations, position_offset = self._get_placement_details()\n\n        # Import antenna model\n        antenna_path = self.antenna.get_centered_antenna_path(os.path.join(self.config.base_dir, 'data', 'antennas', 'centered'))\n        imported_entities = list(self.model.Import(antenna_path))\n\n        antenna_group = next((e for e in imported_entities if \"Antenna\" in e.Name and \"bounding box\" not in e.Name), None)\n        bbox_entity = next((e for e in imported_entities if \"bounding box\" in e.Name), None)\n\n        if not antenna_group:\n            raise RuntimeError(\"Could not find imported antenna group.\")\n\n        # Rename the entities to include the placement name for uniqueness\n        antenna_group.Name = f\"{antenna_group.Name} ({self.placement_name})\"\n        if bbox_entity:\n            bbox_entity.Name = f\"{bbox_entity.Name} ({self.placement_name})\"\n\n        entities_to_transform = [antenna_group, bbox_entity] if bbox_entity else [antenna_group]\n\n        # --- Transformation Composition ---\n        self._log(\"Composing final transformation...\", log_type='progress')\n\n        # Start with an identity transform\n        final_transform = self.XCoreMath.Transform()\n\n        # 1. Stand-up Rotation\n        rot_stand_up = self.XCoreMath.Rotation(self.XCoreMath.Vec3(1, 0, 0), np.deg2rad(90))\n        final_transform = rot_stand_up * final_transform\n\n        # Special rotation for 'by_cheek' to align with YZ plane\n        if self.base_placement_name.startswith('by_cheek'):\n            self._log(\"Applying 'by_cheek' specific Z-rotation.\", log_type='info')\n            rot_z_cheek = self.XCoreMath.Rotation(self.XCoreMath.Vec3(0, 0, 1), np.deg2rad(-90))\n            final_transform = rot_z_cheek * final_transform\n\n        # 2. Orientation Twist\n        if orientation_rotations:\n            for rot in orientation_rotations:\n                axis_map = {'X': self.XCoreMath.Vec3(1,0,0), 'Y': self.XCoreMath.Vec3(0,1,0), 'Z': self.XCoreMath.Vec3(0,0,1)}\n                rot_twist = self.XCoreMath.Rotation(axis_map[rot['axis'].upper()], np.deg2rad(rot['angle_deg']))\n                final_transform = rot_twist * final_transform\n\n        # 3. Final Translation\n        final_target_point = self.model.Vec3(\n            base_target_point[0] + position_offset[0],\n            base_target_point[1] + position_offset[1],\n            base_target_point[2] + position_offset[2]\n        )\n        translation_transform = self.XCoreMath.Translation(final_target_point)\n        final_transform = translation_transform * final_transform\n\n        # --- Apply the single, composed transform ---\n        self._log(\"Applying final composed transform.\", log_type='progress')\n        for entity in entities_to_transform:\n            entity.ApplyTransform(final_transform)\n\n        self._log(\"--- Transformation Sequence Complete ---\", log_type='success')\n\n    def _get_placement_details(self):\n        \"\"\"\n        Calculates the base target point, position offset, and orientation rotations.\n        \"\"\"\n        if self.free_space:\n            return self.model.Vec3(0, 0, 0), [], [0, 0, 0]\n\n        scenario = self.config.get_placement_scenario(self.base_placement_name)\n        if not scenario:\n            raise ValueError(f\"Placement scenario '{self.base_placement_name}' not defined.\")\n\n        if self.base_placement_name.startswith('by_cheek'):\n            position_offset = [0, 0, 0]\n        else:\n            position_offset = scenario['positions'].get(self.position_name, [0, 0, 0])\n        orientation_rotations = scenario['orientations'].get(self.orientation_name, [])\n\n        all_entities = self.model.AllEntities()\n        placements_config = self.config.get_phantom_placements(self.phantom_name.lower())\n        base_target_point = self.model.Vec3(0, 0, 0)\n\n        if self.base_placement_name == 'front_of_eyes':\n            eye_entities = [e for e in all_entities if 'Eye' in e.Name or 'Cornea' in e.Name]\n            if not eye_entities:\n                raise ValueError(\"No eye or cornea entities found for 'Eyes' placement.\")\n            eye_bbox_min, eye_bbox_max = self.model.GetBoundingBox(eye_entities)\n            distance = placements_config.get('distance_from_eye', 200)\n            base_target_point[0] = (eye_bbox_min[0] + eye_bbox_max[0]) / 2.0\n            base_target_point[1] = eye_bbox_max[1] + distance\n            base_target_point[2] = (eye_bbox_min[2] + eye_bbox_max[2]) / 2.0\n\n        elif self.base_placement_name.startswith('by_cheek'):\n            # Find ear and mouth entities\n            ear_skin = next((e for e in all_entities if hasattr(e, 'Name') and e.Name == \"Ear_skin\"), None)\n            mouth_entity = next((e for e in all_entities if hasattr(e, 'Name') and e.Name == \"Tongue\"), None)\n            if not ear_skin or not mouth_entity:\n                raise ValueError(\"Could not find 'Ear_skin' or 'Tongue' entities for 'Cheek' placement.\")\n\n            # Get center points\n            ear_bbox_min, ear_bbox_max = self.model.GetBoundingBox([ear_skin])\n            mouth_bbox_min, mouth_bbox_max = self.model.GetBoundingBox([mouth_entity])\n\n            ear_center = self.XCoreMath.Vec3(\n                (ear_bbox_min[0] + ear_bbox_max[0]) / 2.0,\n                (ear_bbox_min[1] + ear_bbox_max[1]) / 2.0,\n                (ear_bbox_min[2] + ear_bbox_max[2]) / 2.0\n            )\n            mouth_center = self.XCoreMath.Vec3(\n                (mouth_bbox_min[0] + mouth_bbox_max[0]) / 2.0,\n                (mouth_bbox_min[1] + mouth_bbox_max[1]) / 2.0,\n                (mouth_bbox_min[2] + mouth_bbox_max[2]) / 2.0\n            )\n\n            # Calculate angle for base alignment\n            direction_vector = mouth_center - ear_center\n            # Angle in YZ plane is rotation around X-axis\n            angle_rad = np.arctan2(direction_vector[2], direction_vector[1])\n            angle_deg = np.rad2deg(angle_rad)\n\n            # The phone should be perpendicular to this line, so add 90 degrees\n            base_rotation_deg = angle_deg + 90\n            self._log(f\"Calculated base rotation for cheek alignment: {base_rotation_deg:.2f} degrees around X-axis.\", log_type='info')\n\n            # Add this as a new base rotation\n            base_rotation = {\"axis\": \"X\", \"angle_deg\": base_rotation_deg}\n            orientation_rotations.insert(0, base_rotation)\n\n            # Set the target point based on the ear\n            distance = placements_config.get('distance_from_cheek', 8)\n            base_target_point[0] = ear_bbox_max[0] + distance\n            base_target_point[1] = ear_center[1]\n            base_target_point[2] = ear_center[2]\n\n        elif self.base_placement_name == 'by_belly':\n            trunk_bbox_name = f\"{self.phantom_name.lower()}_Trunk_BBox\"\n            trunk_bbox = next((e for e in all_entities if hasattr(e, 'Name') and e.Name == trunk_bbox_name), None)\n            if not trunk_bbox:\n                raise ValueError(f\"Could not find '{trunk_bbox_name}' entity for 'Belly' placement.\")\n\n            belly_bbox_min, belly_bbox_max = self.model.GetBoundingBox([trunk_bbox])\n            distance = placements_config.get('distance_from_belly', 50)\n\n            base_target_point[0] = (belly_bbox_min[0] + belly_bbox_max[0]) / 2.0\n            base_target_point[1] = belly_bbox_max[1] + distance\n            base_target_point[2] = (belly_bbox_min[2] + belly_bbox_max[2]) / 2.0\n\n        else:\n            raise ValueError(f\"Invalid base placement name: {self.base_placement_name}\")\n\n        return base_target_point, orientation_rotations, position_offset\n</code></pre>"},{"location":"api/#src.setups.placement_setup.PlacementSetup.place_antenna","title":"<code>place_antenna()</code>","text":"<p>Places and orients the antenna by composing a single transformation matrix and applying it once.</p> Source code in <code>src\\setups\\placement_setup.py</code> <pre><code>def place_antenna(self):\n    \"\"\"\n    Places and orients the antenna by composing a single transformation matrix\n    and applying it once.\n    \"\"\"\n    self._log(f\"--- Starting Placement: {self.base_placement_name} - {self.position_name} - {self.orientation_name} ---\", log_type='header')\n\n    placements_config = self.config.get_phantom_placements(self.phantom_name.lower())\n    if not self.free_space and not placements_config.get(f\"do_{self.base_placement_name}\"):\n        self._log(f\"Placement '{self.base_placement_name}' is disabled in the configuration.\", log_type='info')\n        return\n\n    base_target_point, orientation_rotations, position_offset = self._get_placement_details()\n\n    # Import antenna model\n    antenna_path = self.antenna.get_centered_antenna_path(os.path.join(self.config.base_dir, 'data', 'antennas', 'centered'))\n    imported_entities = list(self.model.Import(antenna_path))\n\n    antenna_group = next((e for e in imported_entities if \"Antenna\" in e.Name and \"bounding box\" not in e.Name), None)\n    bbox_entity = next((e for e in imported_entities if \"bounding box\" in e.Name), None)\n\n    if not antenna_group:\n        raise RuntimeError(\"Could not find imported antenna group.\")\n\n    # Rename the entities to include the placement name for uniqueness\n    antenna_group.Name = f\"{antenna_group.Name} ({self.placement_name})\"\n    if bbox_entity:\n        bbox_entity.Name = f\"{bbox_entity.Name} ({self.placement_name})\"\n\n    entities_to_transform = [antenna_group, bbox_entity] if bbox_entity else [antenna_group]\n\n    # --- Transformation Composition ---\n    self._log(\"Composing final transformation...\", log_type='progress')\n\n    # Start with an identity transform\n    final_transform = self.XCoreMath.Transform()\n\n    # 1. Stand-up Rotation\n    rot_stand_up = self.XCoreMath.Rotation(self.XCoreMath.Vec3(1, 0, 0), np.deg2rad(90))\n    final_transform = rot_stand_up * final_transform\n\n    # Special rotation for 'by_cheek' to align with YZ plane\n    if self.base_placement_name.startswith('by_cheek'):\n        self._log(\"Applying 'by_cheek' specific Z-rotation.\", log_type='info')\n        rot_z_cheek = self.XCoreMath.Rotation(self.XCoreMath.Vec3(0, 0, 1), np.deg2rad(-90))\n        final_transform = rot_z_cheek * final_transform\n\n    # 2. Orientation Twist\n    if orientation_rotations:\n        for rot in orientation_rotations:\n            axis_map = {'X': self.XCoreMath.Vec3(1,0,0), 'Y': self.XCoreMath.Vec3(0,1,0), 'Z': self.XCoreMath.Vec3(0,0,1)}\n            rot_twist = self.XCoreMath.Rotation(axis_map[rot['axis'].upper()], np.deg2rad(rot['angle_deg']))\n            final_transform = rot_twist * final_transform\n\n    # 3. Final Translation\n    final_target_point = self.model.Vec3(\n        base_target_point[0] + position_offset[0],\n        base_target_point[1] + position_offset[1],\n        base_target_point[2] + position_offset[2]\n    )\n    translation_transform = self.XCoreMath.Translation(final_target_point)\n    final_transform = translation_transform * final_transform\n\n    # --- Apply the single, composed transform ---\n    self._log(\"Applying final composed transform.\", log_type='progress')\n    for entity in entities_to_transform:\n        entity.ApplyTransform(final_transform)\n\n    self._log(\"--- Transformation Sequence Complete ---\", log_type='success')\n</code></pre>"},{"location":"api/#src.setups.source_setup","title":"<code>source_setup</code>","text":""},{"location":"api/#src.setups.source_setup.SourceSetup","title":"<code>SourceSetup</code>","text":"<p>               Bases: <code>BaseSetup</code></p> Source code in <code>src\\setups\\source_setup.py</code> <pre><code>class SourceSetup(BaseSetup):\n    def __init__(self, config, simulation, frequency_mhz, antenna, verbose_logger, progress_logger, free_space=False):\n        super().__init__(config, verbose_logger, progress_logger)\n        self.simulation = simulation\n        self.frequency_mhz = frequency_mhz\n        self.antenna = antenna\n        self.free_space = free_space\n\n        import s4l_v1.units\n        self.units = s4l_v1.units\n\n    def setup_source_and_sensors(self, antenna_components):\n        \"\"\"\n        Sets up the excitation source and sensors.\n        \"\"\"\n        self._log(\"Setting up source and sensors...\", log_type='progress')\n\n        source_name = self.antenna.get_source_entity_name()\n        if source_name not in antenna_components:\n             raise RuntimeError(f\"Could not find source entity '{source_name}' in antenna group.\")\n        source_entity = antenna_components[source_name]\n\n        # Source setup\n        edge_source_settings = self.emfdtd.EdgeSourceSettings()\n\n        # Get the enum for ExcitationType\n        excitation_enum = edge_source_settings.ExcitationType.enum\n\n        if self.free_space:\n            self._log(\"  - Using Gaussian source for free-space simulation.\", log_type='info')\n            excitation_type = 'gaussian'\n            edge_source_settings.ExcitationType = excitation_enum.Gaussian\n            edge_source_settings.CenterFrequency = self.frequency_mhz, self.units.MHz\n            edge_source_settings.Bandwidth = 50.0, self.units.MHz\n        else:\n            self._log(\"  - Using Harmonic source for phantom simulation.\", log_type='info')\n            excitation_type = 'harmonic'\n            edge_source_settings.ExcitationType = excitation_enum.Harmonic\n            edge_source_settings.Frequency = self.frequency_mhz, self.units.MHz\n            edge_source_settings.CenterFrequency = self.frequency_mhz, self.units.MHz\n\n        self.simulation.Add(edge_source_settings, [source_entity])\n\n        # Sensor setup\n        edge_sensor_settings = self.emfdtd.EdgeSensorSettings()\n        self.simulation.Add(edge_sensor_settings, [source_entity])\n\n        if self.free_space:\n            far_field_sensor_settings = self.simulation.AddFarFieldSensorSettings()\n\n            # Configure extracted frequencies for Gaussian source\n            if excitation_type.lower() == 'gaussian':\n                center_freq_hz = self.frequency_mhz * 1e6\n                bandwidth_hz = 50.0 * 1e6\n                start_freq_hz = center_freq_hz - (bandwidth_hz / 2)\n                end_freq_hz = center_freq_hz + (bandwidth_hz / 2)\n\n                # Create a list of 21 frequencies, including the center frequency\n                num_samples = 21\n                extracted_frequencies_hz = [start_freq_hz + i * (bandwidth_hz / (num_samples - 1)) for i in range(num_samples)]\n\n                far_field_sensor_settings.ExtractedFrequencies = extracted_frequencies_hz, self.units.Hz\n                self._log(f\"  - Set extracted frequencies from {start_freq_hz/1e6} MHz to {end_freq_hz/1e6} MHz.\", log_type='info')\n</code></pre>"},{"location":"api/#src.setups.source_setup.SourceSetup.setup_source_and_sensors","title":"<code>setup_source_and_sensors(antenna_components)</code>","text":"<p>Sets up the excitation source and sensors.</p> Source code in <code>src\\setups\\source_setup.py</code> <pre><code>def setup_source_and_sensors(self, antenna_components):\n    \"\"\"\n    Sets up the excitation source and sensors.\n    \"\"\"\n    self._log(\"Setting up source and sensors...\", log_type='progress')\n\n    source_name = self.antenna.get_source_entity_name()\n    if source_name not in antenna_components:\n         raise RuntimeError(f\"Could not find source entity '{source_name}' in antenna group.\")\n    source_entity = antenna_components[source_name]\n\n    # Source setup\n    edge_source_settings = self.emfdtd.EdgeSourceSettings()\n\n    # Get the enum for ExcitationType\n    excitation_enum = edge_source_settings.ExcitationType.enum\n\n    if self.free_space:\n        self._log(\"  - Using Gaussian source for free-space simulation.\", log_type='info')\n        excitation_type = 'gaussian'\n        edge_source_settings.ExcitationType = excitation_enum.Gaussian\n        edge_source_settings.CenterFrequency = self.frequency_mhz, self.units.MHz\n        edge_source_settings.Bandwidth = 50.0, self.units.MHz\n    else:\n        self._log(\"  - Using Harmonic source for phantom simulation.\", log_type='info')\n        excitation_type = 'harmonic'\n        edge_source_settings.ExcitationType = excitation_enum.Harmonic\n        edge_source_settings.Frequency = self.frequency_mhz, self.units.MHz\n        edge_source_settings.CenterFrequency = self.frequency_mhz, self.units.MHz\n\n    self.simulation.Add(edge_source_settings, [source_entity])\n\n    # Sensor setup\n    edge_sensor_settings = self.emfdtd.EdgeSensorSettings()\n    self.simulation.Add(edge_sensor_settings, [source_entity])\n\n    if self.free_space:\n        far_field_sensor_settings = self.simulation.AddFarFieldSensorSettings()\n\n        # Configure extracted frequencies for Gaussian source\n        if excitation_type.lower() == 'gaussian':\n            center_freq_hz = self.frequency_mhz * 1e6\n            bandwidth_hz = 50.0 * 1e6\n            start_freq_hz = center_freq_hz - (bandwidth_hz / 2)\n            end_freq_hz = center_freq_hz + (bandwidth_hz / 2)\n\n            # Create a list of 21 frequencies, including the center frequency\n            num_samples = 21\n            extracted_frequencies_hz = [start_freq_hz + i * (bandwidth_hz / (num_samples - 1)) for i in range(num_samples)]\n\n            far_field_sensor_settings.ExtractedFrequencies = extracted_frequencies_hz, self.units.Hz\n            self._log(f\"  - Set extracted frequencies from {start_freq_hz/1e6} MHz to {end_freq_hz/1e6} MHz.\", log_type='info')\n</code></pre>"},{"location":"api/#src.simulation_runner","title":"<code>simulation_runner</code>","text":""},{"location":"api/#src.simulation_runner.SimulationRunner","title":"<code>SimulationRunner</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Manages the execution of the simulation, either through the Sim4Life API or by calling the iSolve.exe solver manually.</p> Source code in <code>src\\simulation_runner.py</code> <pre><code>class SimulationRunner(LoggingMixin):\n    \"\"\"\n    Manages the execution of the simulation, either through the Sim4Life API\n    or by calling the iSolve.exe solver manually.\n    \"\"\"\n    def __init__(self, config, project_path, simulations, verbose_logger, progress_logger, gui=None, study=None):\n        self.config = config\n        self.project_path = project_path\n        self.simulations = simulations if isinstance(simulations, list) else [simulations]\n        self.verbose_logger = verbose_logger\n        self.progress_logger = progress_logger\n        self.gui = gui\n        self.study = study\n        import s4l_v1.document\n        self.document = s4l_v1.document\n\n\n    def run_all(self):\n        \"\"\"\n        Runs all simulations in the list, managing GUI animations.\n        \"\"\"\n        total_sims = len(self.simulations)\n        if self.gui:\n            self.gui.update_stage_progress(\"Running Simulation\", 0, total_sims)\n\n        for i, sim in enumerate(self.simulations):\n            self._log(f\"\\n--- Running simulation {i+1}/{total_sims}: {sim.Name} ---\", level='progress', log_type='header')\n\n            # Start animation before the run\n            if self.gui:\n                self.gui.start_stage_animation(\"run_simulation_total\", i + 1)\n\n            self.run(sim)\n\n            # End animation and update progress after the run\n            if self.gui and self.study:\n                self.gui.end_stage_animation()\n                progress = self.study.profiler.get_weighted_progress(\"run\", (i + 1) / total_sims)\n                self.gui.update_overall_progress(int(progress), 100)\n                self.gui.update_stage_progress(\"Running Simulation\", i + 1, total_sims)\n\n        self._log(\"\\n--- All simulations finished ---\", level='progress', log_type='success')\n\n    def run(self, simulation):\n        \"\"\"\n        Runs a single simulation, wrapping the entire process in a single subtask\n        for more accurate time estimation.\n        \"\"\"\n        self._log(f\"Running simulation: {simulation.Name}\", log_type='verbose')\n        if not simulation:\n            self._log(f\"ERROR: Simulation object not found.\", level='progress', log_type='error')\n            return\n\n        with self.study.subtask(\"run_simulation_total\"):\n            server_name = self.config.get_server()\n\n            try:\n                if hasattr(simulation, \"WriteInputFile\"):\n                    with self.study.subtask(\"run_write_input_file\"):\n                        self._log(\"Writing solver input file...\", level='progress', log_type='progress')\n                        simulation.WriteInputFile()\n                        self.document.SaveAs(self.project_path) # Force a save to flush files\n\n                # Stop here if we only want to write the input file\n                if self.config.get_only_write_input_file():\n                    self._log(\"'only_write_input_file' is true, skipping simulation run.\", level='progress', log_type='info')\n                    return\n\n                if self.config.get_manual_isolve():\n                    self._run_isolve_manual(simulation)\n                elif server_name and server_name != 'localhost':\n                    self._run_osparc_direct(simulation, server_name)\n                else:\n                    # Fallback to localhost if no server or localhost is specified\n                    simulation.RunSimulation(wait=True, server_id=None)\n                    self._log(\"Simulation finished on localhost.\", level='progress', log_type='success')\n\n            except Exception as e:\n                self._log(f\"An error occurred during simulation run: {e}\", level='progress', log_type='error')\n                # Check if a cloud server was intended for the run\n                if self.config.get_server() and self.config.get_server() != 'localhost':\n                    self._log(\"If you are running on the cloud, please ensure you are logged into Sim4Life via the GUI and your API credentials are correct.\", level='progress', log_type='warning')\n                traceback.print_exc()\n\n        return simulation\n\n    def _run_isolve_manual(self, simulation):\n        \"\"\"\n        Finds iSolve.exe, runs it in a non-blocking way, and logs its output in real-time.\n        \"\"\"\n        # --- 1. Setup: Find paths and prepare command ---\n        # The input file is now written in the run() method before this is called.\n\n        python_path = sys.executable\n        s4l_root = os.path.dirname(os.path.dirname(python_path))\n        isolve_path = os.path.join(s4l_root, \"Solvers\", \"iSolve.exe\")\n        if not os.path.exists(isolve_path):\n            raise FileNotFoundError(f\"iSolve.exe not found at the expected path: {isolve_path}\")\n\n        if not hasattr(simulation, 'GetInputFileName'):\n            raise RuntimeError(\"Could not get input file name from simulation object.\")\n\n        relative_path = simulation.GetInputFileName()\n        project_dir = os.path.dirname(self.project_path)\n        input_file_path = os.path.join(project_dir, relative_path)\n        if not os.path.exists(input_file_path):\n            raise FileNotFoundError(f\"Solver input file not found at: {input_file_path}\")\n\n        solver_kernel = self.config.get_solver_settings().get('kernel', 'Software')\n        log_msg = f\"Running iSolve with {solver_kernel} on {os.path.basename(input_file_path)}\"\n        self._log(log_msg, level='progress', log_type='info')\n\n        command = [isolve_path, \"-i\", input_file_path]\n        show_output = self.config.get_solver_settings().get('show_solver_output', True)\n\n        # --- 2. Non-blocking reader thread setup ---\n        def reader_thread(pipe, queue):\n            \"\"\"Reads lines from a pipe and puts them on a queue.\"\"\"\n            try:\n                for line in iter(pipe.readline, ''):\n                    queue.put(line)\n            finally:\n                pipe.close()\n\n        try:\n            with self.study.subtask(\"run_isolve_execution\"):\n                process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, creationflags=subprocess.CREATE_NO_WINDOW)\n\n                output_queue = Queue()\n                thread = threading.Thread(target=reader_thread, args=(process.stdout, output_queue))\n                thread.daemon = True\n                thread.start()\n\n                # --- 3. Main loop: Monitor process and log output without blocking ---\n                while process.poll() is None:\n                    try:\n                        # Read all available lines from the queue\n                        while True:\n                            line = output_queue.get_nowait()\n                            self.verbose_logger.info(line.strip())\n                    except Empty:\n                        # No new output, sleep briefly to prevent a busy-wait\n                        time.sleep(0.1)\n\n                # Process has finished, get the return code\n                return_code = process.returncode\n                # Make sure the reader thread has finished and read all remaining output\n                thread.join()\n                while not output_queue.empty():\n                    line = output_queue.get_nowait()\n                    self.verbose_logger.info(line.strip())\n\n                if return_code != 0:\n                    error_message = f\"iSolve.exe failed with return code {return_code}.\"\n                    self._log(error_message, level='progress', log_type='error')\n                    raise RuntimeError(error_message)\n\n            # --- 4. Post-simulation steps ---\n            with self.study.subtask(\"run_wait_for_results\"):\n                self._log(\"Waiting for 5 seconds to ensure results are written to disk...\", level='progress', log_type='info')\n                non_blocking_sleep(5)\n\n            with self.study.subtask(\"run_reload_project\"):\n                self._log(\"Re-opening project to load results...\", level='progress', log_type='progress')\n                self.document.Close()\n                open_project(self.project_path)\n\n            sim_name = simulation.Name\n            simulation = next((s for s in self.document.AllSimulations if s.Name == sim_name), None)\n            if not simulation:\n                raise RuntimeError(f\"Could not find simulation '{sim_name}' after re-opening project.\")\n            self._log(\"Project reloaded and results are available.\", level='progress', log_type='success')\n\n        except Exception as e:\n            self._log(f\"An unexpected error occurred while running iSolve.exe: {e}\", level='progress', log_type='error')\n            traceback.print_exc()\n            raise\n\n    def _run_osparc_direct(self, simulation, server_name):\n        \"\"\"\n        Submits a job directly to the oSPARC platform, bypassing server discovery.\n        \"\"\"\n        try:\n            import XOsparcApiClient  # Only available in Sim4Life v8.2.2 and later.\n        except ImportError as e:\n            self._log(\"Failed to import XOsparcApiClient. This module is required for direct oSPARC integration.\", level='progress', log_type='error')\n            self._log(\"Please ensure you are using Sim4Life version 8.2.2 or higher.\", level='progress', log_type='info')\n            self._log(f\"Original error: {e}\", log_type='verbose')\n            traceback.print_exc()\n            raise RuntimeError(\"Could not import XOsparcApiClient module, which is necessary for oSPARC runs.\")\n\n        self._log(\"--- Running simulation directly on oSPARC ---\", level='progress', log_type='header')\n\n        # 1. Get Credentials and Initialize Client\n        creds = self.config.get_osparc_credentials()\n        if not all(k in creds for k in ['api_key', 'api_secret', 'api_server']):\n            raise ValueError(\"Missing oSPARC credentials in configuration.\")\n\n        client = XOsparcApiClient.OsparcApiClient(\n            api_key=creds['api_key'],\n            api_secret=creds['api_secret'],\n            api_server=creds['api_server'],\n            api_version=creds.get('api_version', 'v0')\n        )\n        self._log(\"oSPARC client initialized.\", log_type='verbose')\n\n        # 2. Prepare Job Submission Data\n        input_file_path = os.path.join(os.path.dirname(self.project_path), simulation.GetInputFileName())\n        if not os.path.exists(input_file_path):\n            raise FileNotFoundError(f\"Solver input file not found at: {input_file_path}\")\n\n        job_data = XOsparcApiClient.JobSubmissionData()\n        job_data.InputFilePath = input_file_path\n        job_data.ResourceName = server_name\n        job_data.SolverKey = \"sim4life-isolve\"\n        job_data.SolverVersion = \"\" # Let the API choose the default version\n\n        # 3. Create and Start the Job\n        self._log(f\"Creating job for input file: {os.path.basename(input_file_path)}\", level='progress', log_type='info')\n        create_response = client.CreateJob(job_data)\n        if not create_response.Success:\n            raise RuntimeError(f\"Failed to create oSPARC job: {create_response.Content}\")\n\n        job_id = create_response.Content.get(\"id\")\n        if not job_id:\n            raise RuntimeError(\"oSPARC API did not return a job ID after creation.\")\n\n        self._log(f\"Job created with ID: {job_id}. Starting job...\", level='progress', log_type='info')\n        start_response = client.StartJob(job_data, job_id)\n        if not start_response.Success:\n            raise RuntimeError(f\"Failed to start oSPARC job {job_id}: {start_response.Content}\")\n\n        # 4. Poll for Job Completion\n        self._log(\"Job started. Polling for status...\", level='progress', log_type='progress')\n        while True:\n            status_response = client.GetJobStatus(job_data.SolverKey, job_data.SolverVersion, job_id)\n            if not status_response.Success:\n                self._log(f\"Warning: Could not get job status for {job_id}.\", level='progress', log_type='warning')\n                time.sleep(10)\n                continue\n\n            status = status_response.Content.get(\"state\")\n            self._log(f\"  - Job '{job_id}' status: {status}\", log_type='verbose')\n\n            if status in [\"SUCCESS\", \"FAILED\", \"ABORTED\"]:\n                log_type = 'success' if status == \"SUCCESS\" else 'error'\n                self._log(f\"Job {job_id} finished with status: {status}\", level='progress', log_type=log_type)\n                if status != \"SUCCESS\":\n                    raise RuntimeError(f\"oSPARC job {job_id} failed with status: {status}\")\n                break\n\n            time.sleep(30)\n\n        # 5. Post-simulation steps (similar to _run_isolve_manual)\n        with self.study.subtask(\"run_wait_for_results\"):\n            self._log(\"Waiting for 5 seconds to ensure results are written to disk...\", level='progress', log_type='info')\n            non_blocking_sleep(5)\n\n        with self.study.subtask(\"run_reload_project\"):\n            self._log(\"Re-opening project to load results...\", level='progress', log_type='progress')\n            self.document.Close()\n            open_project(self.project_path)\n\n        sim_name = simulation.Name\n        simulation = next((s for s in self.document.AllSimulations if s.Name == sim_name), None)\n        if not simulation:\n            raise RuntimeError(f\"Could not find simulation '{sim_name}' after re-opening project.\")\n        self._log(\"Project reloaded and results are available.\", level='progress', log_type='success')\n</code></pre>"},{"location":"api/#src.simulation_runner.SimulationRunner.run_all","title":"<code>run_all()</code>","text":"<p>Runs all simulations in the list, managing GUI animations.</p> Source code in <code>src\\simulation_runner.py</code> <pre><code>def run_all(self):\n    \"\"\"\n    Runs all simulations in the list, managing GUI animations.\n    \"\"\"\n    total_sims = len(self.simulations)\n    if self.gui:\n        self.gui.update_stage_progress(\"Running Simulation\", 0, total_sims)\n\n    for i, sim in enumerate(self.simulations):\n        self._log(f\"\\n--- Running simulation {i+1}/{total_sims}: {sim.Name} ---\", level='progress', log_type='header')\n\n        # Start animation before the run\n        if self.gui:\n            self.gui.start_stage_animation(\"run_simulation_total\", i + 1)\n\n        self.run(sim)\n\n        # End animation and update progress after the run\n        if self.gui and self.study:\n            self.gui.end_stage_animation()\n            progress = self.study.profiler.get_weighted_progress(\"run\", (i + 1) / total_sims)\n            self.gui.update_overall_progress(int(progress), 100)\n            self.gui.update_stage_progress(\"Running Simulation\", i + 1, total_sims)\n\n    self._log(\"\\n--- All simulations finished ---\", level='progress', log_type='success')\n</code></pre>"},{"location":"api/#src.simulation_runner.SimulationRunner.run","title":"<code>run(simulation)</code>","text":"<p>Runs a single simulation, wrapping the entire process in a single subtask for more accurate time estimation.</p> Source code in <code>src\\simulation_runner.py</code> <pre><code>def run(self, simulation):\n    \"\"\"\n    Runs a single simulation, wrapping the entire process in a single subtask\n    for more accurate time estimation.\n    \"\"\"\n    self._log(f\"Running simulation: {simulation.Name}\", log_type='verbose')\n    if not simulation:\n        self._log(f\"ERROR: Simulation object not found.\", level='progress', log_type='error')\n        return\n\n    with self.study.subtask(\"run_simulation_total\"):\n        server_name = self.config.get_server()\n\n        try:\n            if hasattr(simulation, \"WriteInputFile\"):\n                with self.study.subtask(\"run_write_input_file\"):\n                    self._log(\"Writing solver input file...\", level='progress', log_type='progress')\n                    simulation.WriteInputFile()\n                    self.document.SaveAs(self.project_path) # Force a save to flush files\n\n            # Stop here if we only want to write the input file\n            if self.config.get_only_write_input_file():\n                self._log(\"'only_write_input_file' is true, skipping simulation run.\", level='progress', log_type='info')\n                return\n\n            if self.config.get_manual_isolve():\n                self._run_isolve_manual(simulation)\n            elif server_name and server_name != 'localhost':\n                self._run_osparc_direct(simulation, server_name)\n            else:\n                # Fallback to localhost if no server or localhost is specified\n                simulation.RunSimulation(wait=True, server_id=None)\n                self._log(\"Simulation finished on localhost.\", level='progress', log_type='success')\n\n        except Exception as e:\n            self._log(f\"An error occurred during simulation run: {e}\", level='progress', log_type='error')\n            # Check if a cloud server was intended for the run\n            if self.config.get_server() and self.config.get_server() != 'localhost':\n                self._log(\"If you are running on the cloud, please ensure you are logged into Sim4Life via the GUI and your API credentials are correct.\", level='progress', log_type='warning')\n            traceback.print_exc()\n\n    return simulation\n</code></pre>"},{"location":"api/#src.studies","title":"<code>studies</code>","text":""},{"location":"api/#src.studies.base_study","title":"<code>base_study</code>","text":""},{"location":"api/#src.studies.base_study.BaseStudy","title":"<code>BaseStudy</code>","text":"<p>               Bases: <code>LoggingMixin</code></p> <p>Abstract base class for all studies (Near-Field, Far-Field).</p> Source code in <code>src\\studies\\base_study.py</code> <pre><code>class BaseStudy(LoggingMixin):\n    \"\"\"\n    Abstract base class for all studies (Near-Field, Far-Field).\n    \"\"\"\n    def __init__(self, study_type, config_filename=None, gui=None, profiler=None):\n        self.study_type = study_type\n        self.gui = gui\n        self.verbose_logger = logging.getLogger('verbose')\n        self.progress_logger = logging.getLogger('progress')\n\n        self.base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n        self.config = Config(self.base_dir, config_filename if config_filename else f\"{self.study_type}_config.json\")\n\n        # Get study-specific profiling config\n        profiling_config = self.config.get_profiling_config(self.study_type)\n        execution_control = self.config.get_setting('execution_control', {'do_setup': True, 'do_run': True, 'do_extract': True})\n\n        self.profiler = Profiler(execution_control, profiling_config, self.study_type, self.config.profiling_config_path)\n        self.line_profiler = None\n\n        self.project_manager = ProjectManager(self.config, self.verbose_logger, self.progress_logger, self.gui)\n\n    def _check_for_stop_signal(self):\n        \"\"\"Checks if the GUI has requested a stop.\"\"\"\n        if self.gui and self.gui.is_stopped():\n            raise StudyCancelledError(\"Study cancelled by user.\")\n\n    def subtask(self, task_name, instance_to_profile=None):\n        \"\"\"\n        Returns a context manager that profiles a subtask, handling timing,\n        GUI animations, and optional line-profiling.\n        \"\"\"\n        return profile_subtask(self, task_name, instance_to_profile)\n\n    def start_stage_animation(self, task_name, end_value):\n        \"\"\"Starts the GUI animation for a stage.\"\"\"\n        if self.gui:\n            self.gui.start_stage_animation(task_name, end_value)\n\n    def end_stage_animation(self):\n        \"\"\"Ends the GUI animation for a stage.\"\"\"\n        if self.gui:\n            self.gui.end_stage_animation()\n\n    def run(self):\n        \"\"\"Main method to run the study.\"\"\"\n        ensure_s4l_running()\n        try:\n            self._run_study()\n        except StudyCancelledError:\n            self._log(\"--- Study execution cancelled by user. ---\", level='progress', log_type='warning')\n        except Exception as e:\n            self._log(f\"--- FATAL ERROR in study: {e} ---\", level='progress', log_type='fatal')\n            self.verbose_logger.error(traceback.format_exc())\n        finally:\n            self._log(f\"\\n--- {self.__class__.__name__} Finished ---\", level='progress', log_type='success')\n            self.profiler.save_estimates()\n            self.project_manager.cleanup()\n            if self.gui:\n                self.gui.update_profiler() # Send final profiler state\n\n    def _run_study(self):\n        \"\"\"\n        This method must be implemented by subclasses to execute the specific study.\n        \"\"\"\n        raise NotImplementedError(\"The '_run_study' method must be implemented by a subclass.\")\n\n    def _setup_line_profiler(self, subtask_name, instance):\n        \"\"\"\n        Sets up the line profiler for a specific subtask if configured.\n        \"\"\"\n        line_profiling_config = self.config.get_line_profiling_config()\n\n        if not line_profiling_config.get(\"enabled\", False) or subtask_name not in line_profiling_config.get(\"subtasks\", {}):\n            return None, lambda func: func\n\n        self._log(f\"  - Setting up line profiler for subtask: {subtask_name}\", level='verbose', log_type='verbose')\n\n        lp = LineProfiler()\n        functions_to_profile = line_profiling_config[\"subtasks\"][subtask_name]\n\n        for func_path in functions_to_profile:\n            try:\n                module_path, class_name, func_name = func_path.rsplit('.', 2)\n\n                # Dynamically import the module and get the class\n                module = importlib.import_module(module_path)\n                class_obj = getattr(module, class_name)\n\n                # Get the function object from the class\n                func_to_add = getattr(class_obj, func_name)\n\n                self._log(f\"    - Adding function to profiler: {class_name}.{func_name} from {module_path}\", log_type='verbose')\n                lp.add_function(func_to_add)\n\n            except (ImportError, AttributeError, ValueError) as e:\n                self._log(f\"  - WARNING: Could not find or parse function '{func_path}' for line profiling. Error: {e}\", level='progress', log_type='warning')\n\n        return lp, lp.wrap_function\n</code></pre>"},{"location":"api/#src.studies.base_study.BaseStudy.subtask","title":"<code>subtask(task_name, instance_to_profile=None)</code>","text":"<p>Returns a context manager that profiles a subtask, handling timing, GUI animations, and optional line-profiling.</p> Source code in <code>src\\studies\\base_study.py</code> <pre><code>def subtask(self, task_name, instance_to_profile=None):\n    \"\"\"\n    Returns a context manager that profiles a subtask, handling timing,\n    GUI animations, and optional line-profiling.\n    \"\"\"\n    return profile_subtask(self, task_name, instance_to_profile)\n</code></pre>"},{"location":"api/#src.studies.base_study.BaseStudy.start_stage_animation","title":"<code>start_stage_animation(task_name, end_value)</code>","text":"<p>Starts the GUI animation for a stage.</p> Source code in <code>src\\studies\\base_study.py</code> <pre><code>def start_stage_animation(self, task_name, end_value):\n    \"\"\"Starts the GUI animation for a stage.\"\"\"\n    if self.gui:\n        self.gui.start_stage_animation(task_name, end_value)\n</code></pre>"},{"location":"api/#src.studies.base_study.BaseStudy.end_stage_animation","title":"<code>end_stage_animation()</code>","text":"<p>Ends the GUI animation for a stage.</p> Source code in <code>src\\studies\\base_study.py</code> <pre><code>def end_stage_animation(self):\n    \"\"\"Ends the GUI animation for a stage.\"\"\"\n    if self.gui:\n        self.gui.end_stage_animation()\n</code></pre>"},{"location":"api/#src.studies.base_study.BaseStudy.run","title":"<code>run()</code>","text":"<p>Main method to run the study.</p> Source code in <code>src\\studies\\base_study.py</code> <pre><code>def run(self):\n    \"\"\"Main method to run the study.\"\"\"\n    ensure_s4l_running()\n    try:\n        self._run_study()\n    except StudyCancelledError:\n        self._log(\"--- Study execution cancelled by user. ---\", level='progress', log_type='warning')\n    except Exception as e:\n        self._log(f\"--- FATAL ERROR in study: {e} ---\", level='progress', log_type='fatal')\n        self.verbose_logger.error(traceback.format_exc())\n    finally:\n        self._log(f\"\\n--- {self.__class__.__name__} Finished ---\", level='progress', log_type='success')\n        self.profiler.save_estimates()\n        self.project_manager.cleanup()\n        if self.gui:\n            self.gui.update_profiler() # Send final profiler state\n</code></pre>"},{"location":"api/#src.studies.far_field_study","title":"<code>far_field_study</code>","text":""},{"location":"api/#src.studies.far_field_study.FarFieldStudy","title":"<code>FarFieldStudy</code>","text":"<p>               Bases: <code>BaseStudy</code></p> <p>Manages a far-field simulation study, including setup, execution, and results extraction.</p> Source code in <code>src\\studies\\far_field_study.py</code> <pre><code>class FarFieldStudy(BaseStudy):\n    \"\"\"\n    Manages a far-field simulation study, including setup, execution, and results extraction.\n    \"\"\"\n    def __init__(self, config_filename=\"far_field_config.json\", gui=None):\n        super().__init__('far_field', config_filename, gui)\n\n    def _run_study(self):\n        \"\"\"\n        Executes the entire far-field study by iterating through each simulation case,\n        creating a separate project for each.\n        \"\"\"\n        self._log(f\"--- Starting Far-Field Study: {self.config.get_setting('study_name')} ---\", level='progress', log_type='header')\n\n        do_setup = self.config.get_setting('execution_control.do_setup', True)\n        do_run = self.config.get_setting('execution_control.do_run', True)\n        do_extract = self.config.get_setting('execution_control.do_extract', True)\n\n        if not do_setup and not do_run and not do_extract:\n            self._log(\"All execution phases (setup, run, extract) are disabled in the config. Nothing to do.\", log_type='warning')\n            return\n\n        if not do_setup and do_run:\n            self._log(\"WARNING: Running simulations without setup is not a standard workflow and might lead to issues.\", log_type='warning')\n\n        phantoms = self.config.get_setting('phantoms', [])\n        frequencies = self.config.get_setting('frequencies_mhz', [])\n        far_field_params = self.config.get_setting('far_field_setup.environmental', {})\n        incident_directions = far_field_params.get('incident_directions', [])\n        polarizations = far_field_params.get('polarizations', [])\n\n        total_projects = len(phantoms) * len(frequencies)\n        sims_per_project = len(incident_directions) * len(polarizations)\n        total_simulations = total_projects * sims_per_project\n\n        # Inform the profiler about the total number of simulations and projects for accurate ETA\n        self.profiler.set_total_simulations(total_simulations)\n        self.profiler.set_project_scope(total_projects)\n\n        # Give the profiler a hint about the first phase to avoid an initial \"N/A\" for ETA\n        if do_setup:\n            self.profiler.current_phase = 'setup'\n        elif do_run:\n            self.profiler.current_phase = 'run'\n        elif do_extract:\n            self.profiler.current_phase = 'extract'\n\n        if self.gui:\n            self.gui.update_overall_progress(0, 100) # Initialize GUI progress\n\n        current_sim_count = 0\n\n        for i, phantom_name in enumerate(phantoms):\n            try:\n                # One phantom setup object can be reused for all frequencies\n                phantom_setup = PhantomSetup(self.config, phantom_name, self.verbose_logger, self.progress_logger)\n\n                for j, freq in enumerate(frequencies):\n                    try:\n                        # Check for a stop signal from the GUI at the start of each major iteration.\n                        self._check_for_stop_signal()\n\n                        # Create a new, clean project for each frequency to avoid performance degradation.\n                        self.project_manager.create_or_open_project(phantom_name, freq)\n\n                        # The phantom must be loaded into each new project.\n                        phantom_setup.ensure_phantom_is_loaded()\n\n                        project_index = i * len(frequencies) + j + 1\n                        self.profiler.set_current_project(project_index)\n                        self.profiler.completed_phases.clear() # Reset for the new project\n\n                        self._log(f\"\\n--- Processing Frequency {j+1}/{len(frequencies)}: {freq}MHz for Phantom '{phantom_name}' ---\", level='progress', log_type='header')\n\n                        all_simulations = []\n                        # 1. Setup Phase\n                        if do_setup:\n                            with profile(self, \"setup\"):\n                                total_setups = len(incident_directions) * len(polarizations)\n                                if self.gui:\n                                    self.gui.update_stage_progress(\"Setup\", 0, total_setups)\n\n                                for k, direction_name in enumerate(incident_directions):\n                                    for l, polarization_name in enumerate(polarizations):\n                                        self._check_for_stop_signal()\n                                        setup_index = k * len(polarizations) + l + 1\n                                        self._log(f\"    - Setting up simulation {setup_index}/{total_setups}: {direction_name}_{polarization_name}\", level='progress', log_type='progress')\n\n                                        setup = FarFieldSetup(self.config, phantom_name, freq, direction_name, polarization_name, self.project_manager, self.verbose_logger, self.progress_logger)\n\n                                        # The context manager handles timing, GUI animations, and optional line profiling.\n                                        with self.subtask(\"setup_simulation\", instance_to_profile=setup) as wrapper:\n                                            simulation = wrapper(setup.run_full_setup)(phantom_setup)\n\n                                        if simulation:\n                                            all_simulations.append((simulation, direction_name, polarization_name))\n                                        else:\n                                            self._log(f\"    - Setup failed for {direction_name}_{polarization_name}\", level='progress', log_type='error')\n\n                                        if self.gui:\n                                            progress = self.profiler.get_weighted_progress(\"setup\", setup_index / total_setups)\n                                            self.gui.update_overall_progress(int(progress), 100)\n                                            self.gui.update_stage_progress(\"Setup\", setup_index, total_setups)\n\n                                # Save the project after the entire setup phase for this project is complete.\n                                self._log(\"  - Saving project after setup phase...\", level='progress', log_type='progress')\n                                self.project_manager.save()\n                        else:\n                            # If not setting up, filter simulations from the existing project based on the config\n                            import s4l_v1.document\n                            all_sims_in_doc = list(s4l_v1.document.AllSimulations)\n\n                            # Get the desired simulation names from the config\n                            config_directions = self.config.get_setting('far_field_setup.environmental.incident_directions', [])\n                            config_polarizations = self.config.get_setting('far_field_setup.environmental.polarizations', [])\n\n                            # Reconstruct the expected simulation names from the config\n                            expected_sim_names_thelonious = [f\"EM_FDTD_{phantom_name}_{freq}MHz_{d}_{p}\" for d in config_directions for p in config_polarizations]\n                            expected_sim_names_thelonius = [f\"EM_FDTD_{phantom_name.replace('thelonious', 'thelonius')}_{freq}MHz_{d}_{p}\" for d in config_directions for p in config_polarizations]\n                            expected_sim_names = expected_sim_names_thelonious + expected_sim_names_thelonius\n\n                            # Filter the simulations from the document\n                            sims_to_process = [sim for sim in all_sims_in_doc if sim.Name in expected_sim_names]\n\n                            # Recreate the all_simulations tuple structure\n                            all_simulations = []\n                            for sim in sims_to_process:\n                                parts = sim.Name.split('_')\n                                direction_name = \"_\".join(parts[4:-1])\n                                polarization_name = parts[-1]\n                                all_simulations.append((sim, direction_name, polarization_name))\n\n                        if not all_simulations:\n                            self._log(\"  ERROR: No matching simulations found in the project for the current configuration. Skipping.\", level='progress', log_type='error')\n                            continue\n\n                        # 2. Run Phase\n                        if do_run:\n                            with profile(self, \"run\"):\n                                self.profiler.start_stage('run', total_stages=len(all_simulations))\n                                sim_objects_only = [s[0] for s in all_simulations]\n                                runner = SimulationRunner(self.config, self.project_manager.project_path, sim_objects_only, self.verbose_logger, self.progress_logger, self.gui, study=self)\n                                runner.run_all()\n                                # Manually complete the run phase after all its stages are done\n                                self.profiler.complete_run_phase()\n\n                        # 3. Extraction Phase\n                        if do_extract:\n                            with profile(self, \"extract\"):\n                                if self.gui:\n                                    self.gui.update_stage_progress(\"Extracting Results\", 0, len(all_simulations))\n                                self.project_manager.reload_project()\n                                self._extract_results_for_project(phantom_name, freq, all_simulations)\n                                if self.gui:\n                                    progress = self.profiler.get_weighted_progress(\"extract\", 1.0)\n                                    self.gui.update_overall_progress(int(progress), 100)\n                                    self.gui.update_stage_progress(\"Extracting Results\", len(all_simulations), len(all_simulations))\n                    except Exception as e:\n                        error_msg = f\"  ERROR: An error occurred while processing frequency {freq}MHz for phantom '{phantom_name}': {e}\"\n                        self._log(error_msg, log_type='error')\n                        self.verbose_logger.error(traceback.format_exc())\n                        # Ensure the project is closed even if an error occurs within the frequency loop\n                        if self.project_manager and hasattr(self.project_manager.document, 'IsOpen') and self.project_manager.document.IsOpen():\n                            self.project_manager.close()\n                        continue # Continue to the next frequency\n                    finally:\n                        # Ensure the project is closed after each frequency to release resources.\n                        if self.project_manager and hasattr(self.project_manager.document, 'IsOpen') and self.project_manager.document.IsOpen():\n                            self.project_manager.close()\n\n            except StudyCancelledError:\n                # This is not an error, but a signal to stop. Re-raise it to be caught by the outer loop.\n                raise\n            except Exception as e:\n                self._log(f\"  ERROR: An error occurred while processing phantom '{phantom_name}': {e}\", level='progress', log_type='error')\n                traceback.print_exc()\n\n    def _extract_results_for_project(self, phantom_name, freq, simulations_to_extract):\n        \"\"\"\n        Extracts results for a given list of simulations.\n        \"\"\"\n        if not simulations_to_extract:\n            self._log(\"  - No matching simulations to extract based on the current configuration.\", log_type='warning')\n            return\n\n        self._log(f\"  - Extracting results for {len(simulations_to_extract)} simulation(s) matching the configuration.\", log_type='info')\n        for sim, direction_name, polarization_name in simulations_to_extract:\n            self._check_for_stop_signal()\n            try:\n                placement_name = f\"environmental_{direction_name}_{polarization_name}\"\n\n                self._log(f\"    - Extracting from simulation: {sim.Name}\", level='progress', log_type='progress')\n                extractor = ResultsExtractor(self.config, sim, phantom_name, freq, placement_name, 'far_field', self.verbose_logger, self.progress_logger, gui=self.gui, study=self)\n                extractor.extract()\n            except Exception as e:\n                self._log(f\"    - ERROR: Failed to extract results for simulation '{sim.Name}': {e}\", level='progress', log_type='error')\n                traceback.print_exc()\n</code></pre>"},{"location":"api/#src.studies.near_field_study","title":"<code>near_field_study</code>","text":""},{"location":"api/#src.studies.near_field_study.NearFieldStudy","title":"<code>NearFieldStudy</code>","text":"<p>               Bases: <code>BaseStudy</code></p> <p>Manages and runs a full near-field simulation campaign.</p> Source code in <code>src\\studies\\near_field_study.py</code> <pre><code>class NearFieldStudy(BaseStudy):\n    \"\"\"\n    Manages and runs a full near-field simulation campaign.\n    \"\"\"\n    def __init__(self, config_filename=\"near_field_config.json\", gui=None):\n        super().__init__('near_field', config_filename, gui)\n\n    def _run_study(self):\n        \"\"\"\n        Runs the entire simulation campaign based on the configuration.\n        \"\"\"\n        self._log(f\"--- Starting Near-Field Study: {self.config.get_setting('study_name')} ---\", level='progress', log_type='header')\n\n        do_setup = self.config.get_setting('execution_control.do_setup', True)\n        do_run = self.config.get_setting('execution_control.do_run', True)\n        do_extract = self.config.get_setting('execution_control.do_extract', True)\n\n        if not do_setup and not do_run and not do_extract:\n            self._log(\"All execution phases (setup, run, extract) are disabled. Nothing to do.\", log_type='warning')\n            return\n\n        phantoms = self.config.get_setting('phantoms', [])\n        if not isinstance(phantoms, list):\n            phantoms = [phantoms]\n        frequencies = self.config.get_setting('antenna_config', {}).keys()\n        all_scenarios = self.config.get_setting('placement_scenarios', {})\n\n        # Calculate total number of simulations for the profiler\n        total_simulations = 0\n        for phantom_name in phantoms:\n            placements_config = self.config.get_phantom_placements(phantom_name)\n            if not placements_config:\n                continue\n            for scenario_name, scenario_details in all_scenarios.items():\n                if placements_config.get(f\"do_{scenario_name}\"):\n                    positions = scenario_details.get('positions', {})\n                    orientations = scenario_details.get('orientations', {})\n                    total_simulations += len(frequencies) * len(positions) * len(orientations)\n\n        self.profiler.set_total_simulations(total_simulations)\n        if self.gui:\n            self.gui.update_overall_progress(0, 100)\n\n        simulation_count = 0\n        for phantom_name in phantoms:\n            placements_config = self.config.get_phantom_placements(phantom_name)\n            if not placements_config:\n                continue\n\n            enabled_placements = []\n            for scenario_name, scenario_details in all_scenarios.items():\n                if placements_config.get(f\"do_{scenario_name}\"):\n                    positions = scenario_details.get('positions', {})\n                    orientations = scenario_details.get('orientations', {})\n                    for pos_name in positions.keys():\n                        for orient_name in orientations.keys():\n                            enabled_placements.append(f\"{scenario_name}_{pos_name}_{orient_name}\")\n\n            for freq_str in frequencies:\n                self._check_for_stop_signal()\n                freq = int(freq_str)\n                for placement_name in enabled_placements:\n                    self._check_for_stop_signal()\n                    simulation_count += 1\n                    self._log(f\"\\n--- Processing Simulation {simulation_count}/{total_simulations}: {phantom_name}, {freq}MHz, {placement_name} ---\", level='progress', log_type='header')\n                    self._run_placement(phantom_name, freq, placement_name, do_setup, do_run, do_extract)\n\n    def _run_placement(self, phantom_name, freq, placement_name, do_setup, do_run, do_extract):\n        \"\"\"\n        Runs a full placement scenario, which may include multiple positions and orientations.\n        \"\"\"\n        try:\n            simulation = None\n\n            # 1. Setup Simulation\n            if do_setup:\n                with profile(self, \"setup\"):\n                    if self.gui:\n                        self.gui.update_stage_progress(\"Setup\", 0, 1)\n\n                    antenna = Antenna(self.config, freq)\n                    setup = NearFieldSetup(self.config, phantom_name, freq, placement_name, antenna, self.verbose_logger, self.progress_logger)\n\n                    with self.subtask(\"setup_simulation\", instance_to_profile=setup) as wrapper:\n                        simulation = wrapper(setup.run_full_setup)(self.project_manager)\n\n                    if not simulation:\n                        self._log(f\"ERROR: Setup failed for {placement_name}. Cannot proceed.\", level='progress', log_type='error')\n                        return\n\n                    # The first save is now critical after setup is complete.\n                    self.project_manager.save()\n                    if self.gui:\n                        progress = self.profiler.get_weighted_progress(\"setup\", 1.0)\n                        self.gui.update_overall_progress(int(progress), 100)\n                        self.gui.update_stage_progress(\"Setup\", 1, 1)\n            else:\n                self.project_manager.create_or_open_project(phantom_name, freq, placement_name)\n\n            # ALWAYS get a fresh simulation handle from the document before run/extract\n            import s4l_v1.document\n            if s4l_v1.document.AllSimulations:\n                # First, try to find the simulation with the correct \"thelonious\" name.\n                sim_name_correct = f\"EM_FDTD_{phantom_name}_{freq}MHz_{placement_name}\"\n                simulation = next((s for s in s4l_v1.document.AllSimulations if s.Name == sim_name_correct), None)\n\n                # If not found, check for the old \"thelonius\" name.\n                if not simulation:\n                    sim_name_old = f\"EM_FDTD_{phantom_name.replace('thelonious', 'thelonius')}_{freq}MHz_{placement_name}\"\n                    simulation = next((s for s in s4l_v1.document.AllSimulations if s.Name == sim_name_old), None)\n\n            if not simulation:\n                self._log(f\"ERROR: No simulation found or created for {placement_name}. Cannot proceed.\", level='progress', log_type='error')\n                return\n\n            # 2. Run Simulation\n            if do_run:\n                with profile(self, \"run\"):\n                    self.profiler.start_stage('run', total_stages=1)\n                    runner = SimulationRunner(self.config, self.project_manager.project_path, [simulation], self.verbose_logger, self.progress_logger, self.gui, self)\n                    runner.run_all()\n                    self.profiler.complete_run_phase()\n\n            # 3. Extract Results\n            if do_extract:\n                with profile(self, \"extract\"):\n                    if self.gui:\n                        self.gui.update_stage_progress(\"Extracting Results\", 0, 1)\n                    self.project_manager.reload_project()\n\n                    import s4l_v1.document\n                    sim_name = simulation.Name\n                    reloaded_simulation = next((s for s in s4l_v1.document.AllSimulations if s.Name == sim_name), None)\n\n                    # If not found, check for the old \"thelonius\" name.\n                    if not reloaded_simulation:\n                        sim_name_old = sim_name.replace('thelonious', 'thelonius')\n                        reloaded_simulation = next((s for s in s4l_v1.document.AllSimulations if s.Name == sim_name_old), None)\n\n                    if not reloaded_simulation:\n                        raise RuntimeError(f\"Could not find simulation '{sim_name}' (or '{sim_name_old}') after reloading project.\")\n\n                    extractor = ResultsExtractor(self.config, reloaded_simulation, phantom_name, freq, placement_name, 'near_field', self.verbose_logger, self.progress_logger, gui=self.gui, study=self)\n                    extractor.extract()\n                    self.project_manager.save()\n                    if self.gui:\n                        progress = self.profiler.get_weighted_progress(\"extract\", 1.0)\n                        self.gui.update_overall_progress(int(progress), 100)\n                        self.gui.update_stage_progress(\"Extracting Results\", 1, 1)\n\n        except Exception as e:\n            self._log(f\"ERROR: An error occurred during placement '{placement_name}': {e}\", level='progress', log_type='error')\n            traceback.print_exc()\n        finally:\n            if self.project_manager and hasattr(self.project_manager.document, 'IsOpen') and self.project_manager.document.IsOpen():\n                self.project_manager.close()\n</code></pre>"},{"location":"api/#src.utils","title":"<code>utils</code>","text":""},{"location":"api/#src.utils.StudyCancelledError","title":"<code>StudyCancelledError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception to indicate that the study was cancelled by the user.</p> Source code in <code>src\\utils.py</code> <pre><code>class StudyCancelledError(Exception):\n    \"\"\"Custom exception to indicate that the study was cancelled by the user.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#src.utils.Profiler","title":"<code>Profiler</code>","text":"<p>A simple profiler to track execution time and estimate remaining time for a series of runs.</p> Source code in <code>src\\utils.py</code> <pre><code>class Profiler:\n    \"\"\"\n    A simple profiler to track execution time and estimate remaining time for a series of runs.\n    \"\"\"\n    def __init__(self, config_path, study_type='sensitivity_analysis'):\n        self.config_path = config_path\n        self.study_type = study_type\n        self.profiling_config = self._load_config()\n\n        self.start_time = time.monotonic()\n        self.run_times = []\n        self.total_runs = 0\n        self.completed_runs = 0\n        self.current_run_start_time = None\n\n    def _load_config(self):\n        \"\"\"Loads the profiling configuration for the specific study type.\"\"\"\n        try:\n            with open(self.config_path, 'r') as f:\n                full_config = json.load(f)\n            return full_config.get(self.study_type, {})\n        except (FileNotFoundError, json.JSONDecodeError):\n            return {\"average_run_time\": 60.0} # Default value\n\n    def start_study(self, total_runs):\n        \"\"\"Starts a new study, resetting counters.\"\"\"\n        self.total_runs = total_runs\n        self.completed_runs = 0\n        self.run_times = []\n        self.start_time = time.monotonic()\n\n    def start_run(self):\n        \"\"\"Marks the beginning of a single run.\"\"\"\n        self.current_run_start_time = time.monotonic()\n\n    def end_run(self):\n        \"\"\"Marks the end of a single run and records its duration.\"\"\"\n        if self.current_run_start_time:\n            duration = time.monotonic() - self.current_run_start_time\n            self.run_times.append(duration)\n            self.completed_runs += 1\n            self.current_run_start_time = None\n\n    def get_average_run_time(self):\n        \"\"\"\n        Gets the average run time, prioritizing measured times over historical estimates.\n        \"\"\"\n        if self.run_times:\n            return sum(self.run_times) / len(self.run_times)\n        return self.profiling_config.get(\"average_run_time\", 60.0)\n\n    def get_time_remaining(self):\n        \"\"\"Estimates the time remaining for the entire study.\"\"\"\n        if self.total_runs == 0:\n            return 0\n\n        avg_time = self.get_average_run_time()\n        remaining_runs = self.total_runs - self.completed_runs\n        return remaining_runs * avg_time\n\n    def save_estimates(self):\n        \"\"\"Saves the new average run time to the configuration file.\"\"\"\n        if not self.run_times:\n            return # Nothing to save\n\n        new_avg = self.get_average_run_time()\n\n        try:\n            with open(self.config_path, 'r') as f:\n                full_config = json.load(f)\n        except (FileNotFoundError, json.JSONDecodeError):\n            full_config = {}\n\n        if self.study_type not in full_config:\n            full_config[self.study_type] = {}\n\n        full_config[self.study_type]['average_run_time'] = new_avg\n\n        with open(self.config_path, 'w') as f:\n            json.dump(full_config, f, indent=4)\n\n    def get_elapsed(self):\n        return time.monotonic() - self.start_time\n\n    @contextlib.contextmanager\n    def subtask(self, name):\n        \"\"\"A context manager to time a subtask.\"\"\"\n        # This is a simplified version for the simple profiler, it does not need a stack.\n        start_time = time.monotonic()\n        try:\n            yield\n        finally:\n            elapsed = time.monotonic() - start_time\n            # We can just log the subtask time for now.\n            logging.getLogger('verbose').info(f\"Subtask '{name}' took {elapsed:.2f}s\", extra={'log_type': 'verbose'})\n</code></pre>"},{"location":"api/#src.utils.Profiler.start_study","title":"<code>start_study(total_runs)</code>","text":"<p>Starts a new study, resetting counters.</p> Source code in <code>src\\utils.py</code> <pre><code>def start_study(self, total_runs):\n    \"\"\"Starts a new study, resetting counters.\"\"\"\n    self.total_runs = total_runs\n    self.completed_runs = 0\n    self.run_times = []\n    self.start_time = time.monotonic()\n</code></pre>"},{"location":"api/#src.utils.Profiler.start_run","title":"<code>start_run()</code>","text":"<p>Marks the beginning of a single run.</p> Source code in <code>src\\utils.py</code> <pre><code>def start_run(self):\n    \"\"\"Marks the beginning of a single run.\"\"\"\n    self.current_run_start_time = time.monotonic()\n</code></pre>"},{"location":"api/#src.utils.Profiler.end_run","title":"<code>end_run()</code>","text":"<p>Marks the end of a single run and records its duration.</p> Source code in <code>src\\utils.py</code> <pre><code>def end_run(self):\n    \"\"\"Marks the end of a single run and records its duration.\"\"\"\n    if self.current_run_start_time:\n        duration = time.monotonic() - self.current_run_start_time\n        self.run_times.append(duration)\n        self.completed_runs += 1\n        self.current_run_start_time = None\n</code></pre>"},{"location":"api/#src.utils.Profiler.get_average_run_time","title":"<code>get_average_run_time()</code>","text":"<p>Gets the average run time, prioritizing measured times over historical estimates.</p> Source code in <code>src\\utils.py</code> <pre><code>def get_average_run_time(self):\n    \"\"\"\n    Gets the average run time, prioritizing measured times over historical estimates.\n    \"\"\"\n    if self.run_times:\n        return sum(self.run_times) / len(self.run_times)\n    return self.profiling_config.get(\"average_run_time\", 60.0)\n</code></pre>"},{"location":"api/#src.utils.Profiler.get_time_remaining","title":"<code>get_time_remaining()</code>","text":"<p>Estimates the time remaining for the entire study.</p> Source code in <code>src\\utils.py</code> <pre><code>def get_time_remaining(self):\n    \"\"\"Estimates the time remaining for the entire study.\"\"\"\n    if self.total_runs == 0:\n        return 0\n\n    avg_time = self.get_average_run_time()\n    remaining_runs = self.total_runs - self.completed_runs\n    return remaining_runs * avg_time\n</code></pre>"},{"location":"api/#src.utils.Profiler.save_estimates","title":"<code>save_estimates()</code>","text":"<p>Saves the new average run time to the configuration file.</p> Source code in <code>src\\utils.py</code> <pre><code>def save_estimates(self):\n    \"\"\"Saves the new average run time to the configuration file.\"\"\"\n    if not self.run_times:\n        return # Nothing to save\n\n    new_avg = self.get_average_run_time()\n\n    try:\n        with open(self.config_path, 'r') as f:\n            full_config = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        full_config = {}\n\n    if self.study_type not in full_config:\n        full_config[self.study_type] = {}\n\n    full_config[self.study_type]['average_run_time'] = new_avg\n\n    with open(self.config_path, 'w') as f:\n        json.dump(full_config, f, indent=4)\n</code></pre>"},{"location":"api/#src.utils.Profiler.subtask","title":"<code>subtask(name)</code>","text":"<p>A context manager to time a subtask.</p> Source code in <code>src\\utils.py</code> <pre><code>@contextlib.contextmanager\ndef subtask(self, name):\n    \"\"\"A context manager to time a subtask.\"\"\"\n    # This is a simplified version for the simple profiler, it does not need a stack.\n    start_time = time.monotonic()\n    try:\n        yield\n    finally:\n        elapsed = time.monotonic() - start_time\n        # We can just log the subtask time for now.\n        logging.getLogger('verbose').info(f\"Subtask '{name}' took {elapsed:.2f}s\", extra={'log_type': 'verbose'})\n</code></pre>"},{"location":"api/#src.utils.format_time","title":"<code>format_time(seconds)</code>","text":"<p>Formats seconds into a human-readable string (e.g., 1m 23s).</p> Source code in <code>src\\utils.py</code> <pre><code>def format_time(seconds):\n    \"\"\"Formats seconds into a human-readable string (e.g., 1m 23s).\"\"\"\n    seconds = int(seconds)\n    if seconds &lt; 60:\n        return f\"{seconds}s\"\n    minutes, seconds = divmod(seconds, 60)\n    if minutes &lt; 60:\n        return f\"{minutes}m {seconds}s\"\n    hours, minutes = divmod(minutes, 60)\n    return f\"{hours}h {minutes}m {seconds}s\"\n</code></pre>"},{"location":"api/#src.utils.non_blocking_sleep","title":"<code>non_blocking_sleep(seconds)</code>","text":"<p>A non-blocking sleep that processes GUI events.</p> Source code in <code>src\\utils.py</code> <pre><code>def non_blocking_sleep(seconds):\n    \"\"\"\n    A non-blocking sleep that processes GUI events.\n    \"\"\"\n    from PySide6.QtCore import QCoreApplication, QTime, QEventLoop\n\n    end_time = QTime.currentTime().addSecs(int(seconds))\n    while QTime.currentTime() &lt; end_time:\n        QCoreApplication.processEvents(QEventLoop.AllEvents, 50)\n        time.sleep(0.05)\n</code></pre>"},{"location":"api/#src.utils.profile","title":"<code>profile(study, phase_name)</code>","text":"<p>A context manager to profile a block of code (a 'phase').</p> Source code in <code>src\\utils.py</code> <pre><code>@contextlib.contextmanager\ndef profile(study, phase_name):\n    \"\"\"\n    A context manager to profile a block of code (a 'phase').\n    \"\"\"\n    # The 'run' phase is further divided into stages (simulations), so we don't start a master stage for it.\n    if phase_name != 'run':\n        study.profiler.start_stage(phase_name)\n        # Send the updated profiler to the GUI immediately so it knows the phase has started.\n        if study.gui:\n            study.gui.update_profiler()\n\n    study._log(f\"--- Starting: {phase_name} ---\", log_type='header')\n    start_time = time.monotonic()\n    try:\n        yield\n    finally:\n        elapsed = time.monotonic() - start_time\n        study._log(f\"--- Finished: {phase_name} (took {elapsed:.2f}s) ---\", log_type='header')\n\n        # For 'setup' and 'extract', ending the stage means ending the phase.\n        # For 'run', the phase is completed manually after the simulation loop.\n        if phase_name != 'run':\n            study.profiler.end_stage()\n        else:\n            # This ensures the 'run' phase duration is recorded correctly\n            study.profiler.complete_run_phase()\n\n        if study.gui:\n            study.gui.update_profiler()\n</code></pre>"},{"location":"api/#src.utils.profile_subtask","title":"<code>profile_subtask(study, task_name, instance_to_profile=None)</code>","text":"<p>A comprehensive context manager for a 'subtask'. It handles: - High-level timing via study.profiler. - GUI stage animation. - Optional, detailed line-by-line profiling if configured.</p> Source code in <code>src\\utils.py</code> <pre><code>@contextlib.contextmanager\ndef profile_subtask(study, task_name, instance_to_profile=None):\n    \"\"\"\n    A comprehensive context manager for a 'subtask'. It handles:\n    - High-level timing via study.profiler.\n    - GUI stage animation.\n    - Optional, detailed line-by-line profiling if configured.\n    \"\"\"\n    study.start_stage_animation(task_name, 1)\n    study.profiler.subtask_stack.append({'name': task_name, 'start_time': time.monotonic()})\n\n    lp = None\n    wrapper = None\n\n    # Check if line profiling is enabled for this specific subtask\n    line_profiling_config = study.config.get_line_profiling_config()\n    if (instance_to_profile and\n        line_profiling_config.get(\"enabled\", False) and\n        task_name in line_profiling_config.get(\"subtasks\", {})):\n\n        study._log(f\"  - Activating line profiler for subtask: {task_name}\", level='verbose', log_type='verbose')\n        lp, wrapper = study._setup_line_profiler(task_name, instance_to_profile)\n\n    try:\n        # If line profiler is active, yield its wrapper. Otherwise, yield a dummy function.\n        if lp and wrapper:\n            yield wrapper\n        else:\n            yield lambda func: func\n\n    finally:\n        subtask = study.profiler.subtask_stack.pop()\n        elapsed = time.monotonic() - subtask['start_time']\n        study.profiler.subtask_times[subtask['name']].append(elapsed)\n        study._log(f\"    - Subtask '{task_name}' done in {elapsed:.2f}s\", level='progress', log_type='progress')\n\n        # Update and save estimates after each subtask\n        study.profiler.update_and_save_estimates()\n\n        # If the line profiler was active, print its stats\n        if lp:\n            study._log(f\"    - Line profiler stats for '{task_name}':\", level='verbose', log_type='verbose')\n            s = io.StringIO()\n            lp.print_stats(stream=s)\n            study.verbose_logger.info(s.getvalue())\n\n        study.end_stage_animation()\n</code></pre>"},{"location":"api/#src.utils.ensure_s4l_running","title":"<code>ensure_s4l_running()</code>","text":"<p>Ensures that the Sim4Life application is running.</p> Source code in <code>src\\utils.py</code> <pre><code>def ensure_s4l_running():\n    \"\"\"\n    Ensures that the Sim4Life application is running.\n    \"\"\"\n    from s4l_v1._api import application\n\n    if application.get_app_safe() is None:\n        logging.getLogger('verbose').info(\"Starting Sim4Life application...\", extra={'log_type': 'info'})\n        application.run_application(disable_ui_plugins=True)\n        logging.getLogger('verbose').info(\"Sim4Life application started.\", extra={'log_type': 'success'})\n</code></pre>"},{"location":"api/#src.utils.open_project","title":"<code>open_project(project_path)</code>","text":"<p>Opens a Sim4Life project or creates a new one in memory.</p> Source code in <code>src\\utils.py</code> <pre><code>def open_project(project_path):\n    \"\"\"\n    Opens a Sim4Life project or creates a new one in memory.\n    \"\"\"\n    import s4l_v1.document\n    if not os.path.exists(project_path):\n        logging.getLogger('verbose').info(f\"Project file not found at {project_path}, creating a new one.\", extra={'log_type': 'warning'})\n        s4l_v1.document.New()\n    else:\n        logging.getLogger('verbose').info(f\"Opening project: {project_path}\", extra={'log_type': 'info'})\n        s4l_v1.document.Open(project_path)\n</code></pre>"},{"location":"api/#src.utils.delete_project_file","title":"<code>delete_project_file(project_path)</code>","text":"<p>Deletes the project file if it exists.</p> Source code in <code>src\\utils.py</code> <pre><code>def delete_project_file(project_path):\n    \"\"\"\n    Deletes the project file if it exists.\n    \"\"\"\n    if os.path.exists(project_path):\n        logging.getLogger('verbose').info(f\"Deleting existing project file: {project_path}\", extra={'log_type': 'warning'})\n        os.remove(project_path)\n</code></pre>"},{"location":"api/#src.utils.suppress_stdout_stderr","title":"<code>suppress_stdout_stderr()</code>","text":"<p>A context manager that redirects stdout and stderr to devnull.</p> Source code in <code>src\\utils.py</code> <pre><code>@contextlib.contextmanager\ndef suppress_stdout_stderr():\n    \"\"\"A context manager that redirects stdout and stderr to devnull.\"\"\"\n    with open(os.devnull, 'w') as fnull:\n        saved_stdout, saved_stderr = sys.stdout, sys.stderr\n        sys.stdout, sys.stderr = fnull, fnull\n        try:\n            yield\n        finally:\n            sys.stdout, sys.stderr = saved_stdout, saved_stderr\n</code></pre>"},{"location":"architecture_overview/","title":"Architecture Overview","text":"<p>This document provides a high-level overview of the GOLIAT project's architecture.</p>"},{"location":"architecture_overview/#workflow","title":"Workflow","text":"<p>The application follows a clear, modular workflow from configuration to results. The core logic is orchestrated by Study classes, which manage the entire simulation lifecycle.</p> <pre><code>graph TD\n    A[Start] --&gt; B{Load Config};\n    B --&gt; C{Select Study Type};\n    C --&gt; D[Near-Field Study];\n    C --&gt; E[Far-Field Study];\n    D --&gt; F{Run Simulation};\n    E --&gt; F;\n    F --&gt; G[Extract Results];\n    G --&gt; H[End];\n</code></pre>"},{"location":"architecture_overview/#key-components","title":"Key Components","text":"<ul> <li><code>run_study.py</code>: The main entry point of the application. It handles the GUI and launches the study in a separate process.</li> <li><code>Config</code>: A class that handles loading and validation of configuration files with inheritance.</li> <li><code>NearFieldStudy</code> / <code>FarFieldStudy</code>: These classes orchestrate the entire simulation workflow for their respective study types.</li> <li><code>ProjectManager</code>: Manages the Sim4Life project file (<code>.smash</code>).</li> <li><code>NearFieldSetup</code> / <code>FarFieldSetup</code>: These classes build the simulation scene.</li> <li><code>SimulationRunner</code>: Executes the simulation.</li> <li><code>ResultsExtractor</code>: Performs post-processing and data extraction.</li> <li><code>GuiManager</code>: Provides a real-time progress window using PySide6.</li> </ul> <p>For more detailed information, please refer to the API Reference.</p>"},{"location":"old_good_GUI-Profiling-Logger/","title":"Codebase Features: A Comprehensive Deep-Dive","text":"<p>This document details the architecture and workflow of the key operational features of the codebase, focusing on the graphical user interface (GUI), logging, session management, and the profiling/timing system.</p>"},{"location":"old_good_GUI-Profiling-Logger/#1-high-level-workflow","title":"1. High-Level Workflow","text":"<p>The application is designed to run scientific studies (e.g., Near-Field, Far-Field) which can be time-consuming. To provide user feedback and manage complexity, the system employs a multi-process architecture.</p> <ol> <li>Main Process: A lightweight PySide6 GUI (<code>ProgressGUI</code>) is launched. This GUI is responsible for displaying progress, logs, and timing information.</li> <li>Study Process: The actual study (<code>NearFieldStudy</code> or <code>FarFieldStudy</code>) is executed in a separate process using Python's <code>multiprocessing</code> module. This prevents the GUI from freezing during intensive calculations.</li> <li>Communication: The study process communicates with the GUI process through a <code>multiprocessing.Queue</code>. It sends messages containing status updates, progress information, and timing data.</li> </ol> <p>The entry point for the study process is the <code>study_process_wrapper</code> function, which sets up a special <code>QueueGUI</code> object. This object mimics the real GUI's interface but directs all its output to the shared queue.</p> <pre><code># from src/gui_manager.py\ndef study_process_wrapper(queue, study_type, config_filename, verbose, session_timestamp, execution_control):\n    \"\"\"\n    This function runs in a separate process and executes the study.\n    It communicates with the main GUI process via a queue.\n    \"\"\"\n    # ... setup ...\n    class QueueGUI:\n        def __init__(self, queue):\n            self.queue = queue\n            self.profiler = None\n\n        def log(self, message, level='verbose'):\n            if level == 'progress':\n                self.queue.put({'type': 'status', 'message': message})\n\n        def update_overall_progress(self, current_step, total_steps):\n            self.queue.put({'type': 'overall_progress', 'current': current_step, 'total': total_steps})\n        # ... other methods ...\n\n    if study_type == 'near_field':\n        study = NearFieldStudy(config_filename=config_filename, verbose=verbose, gui=QueueGUI(queue))\n    # ...\n    study.run()\n    queue.put({'type': 'finished'})\n</code></pre> <pre><code>graph TD\n    A[Main Process: ProgressGUI] -- Spawns --&gt; B[Study Process: study_process_wrapper];\n    B -- Instantiates --&gt; Study[NearFieldStudy/FarFieldStudy];\n    Study -- Uses --&gt; QueueGUI[QueueGUI object];\n    QueueGUI -- Puts messages --&gt; C{multiprocessing.Queue};\n    C -- Polled by QTimer --&gt; A;\n    A -- Updates UI --&gt; D[User];\n</code></pre>"},{"location":"old_good_GUI-Profiling-Logger/#2-gui-gui_managerpy","title":"2. GUI (<code>gui_manager.py</code>)","text":"<p>The GUI provides a real-time view of the study's progress. It runs in the main process and is designed to be responsive, even while the heavy computation happens elsewhere.</p>"},{"location":"old_good_GUI-Profiling-Logger/#message-processing","title":"Message Processing","text":"<p>The <code>ProgressGUI</code> uses a <code>QTimer</code> that fires every 100ms, calling the <code>process_queue</code> method. This method drains the queue of any pending messages from the study process and updates the UI accordingly.</p> <pre><code># from src/gui_manager.py\nclass ProgressGUI(QWidget):\n    # ...\n    def process_queue(self):\n        while not self.queue.empty():\n            try:\n                msg = self.queue.get_nowait()\n                msg_type = msg.get('type')\n\n                if msg_type == 'status':\n                    self.update_status(msg['message'])\n                elif msg_type == 'overall_progress':\n                    self.update_overall_progress(msg['current'], msg['total'])\n                elif msg_type == 'stage_progress':\n                    self.update_stage_progress(msg['name'], msg['current'], msg['total'])\n                elif msg_type == 'start_animation':\n                    self.start_stage_animation(msg['estimate'], msg['end_value'])\n                # ... other message types ...\n            except Empty:\n                break\n</code></pre>"},{"location":"old_good_GUI-Profiling-Logger/#the-animation-system-a-closer-look","title":"The Animation System: A Closer Look","text":"<p>A key feature for user experience is the smooth animation of the stage progress bar. This is used for tasks where the simulation software doesn't provide real-time progress feedback, but we have a historical estimate of how long it should take.</p> <p>How it works:</p> <ol> <li> <p>Initiation: The study process, before starting a long-running subtask (like <code>run_simulation_total</code>), gets an estimated duration from the <code>Profiler</code>. It then sends a <code>start_animation</code> message to the GUI, containing this estimated duration.     <pre><code># from src/studies/far_field_study.py (conceptual)\ndef run_simulations(self):\n    # ...\n    if self.gui:\n        # Tell the GUI to start an animation for the next step\n        self.gui.start_stage_animation(\"run_simulation_total\", i + 1)\n    self.simulation_runner.run(sim)\n</code></pre>     The <code>QueueGUI</code> object in the study process gets the estimate from its profiler instance and puts the message on the queue.     <pre><code># from src/gui_manager.py\nclass QueueGUI:\n    # ...\n    def start_stage_animation(self, task_name, end_value):\n        estimate = self.profiler.get_subtask_estimate(task_name)\n        self.queue.put({'type': 'start_animation', 'estimate': estimate, 'end_value': end_value})\n</code></pre></p> </li> <li> <p>Animation Setup: When the <code>ProgressGUI</code> receives the <code>start_animation</code> message, it sets up the animation parameters. It records the <code>start_time</code>, the <code>duration</code> (from the profiler's estimate), the progress bar's <code>start_value</code>, and the <code>end_value</code> it needs to reach.     <pre><code># from src/gui_manager.py\ndef start_stage_animation(self, estimated_duration, end_step):\n    self.animation_start_time = time.time()\n    self.animation_duration = estimated_duration\n    self.animation_start_value = self.stage_progress_bar.value()\n    # ... calculate animation_end_value based on end_step ...\n\n    self.animation_active = True\n    if not self.animation_timer.isActive():\n        self.animation_timer.start(50) # Start the animation timer (50ms interval)\n</code></pre></p> </li> <li> <p>Frame-by-Frame Update: A dedicated <code>QTimer</code> (<code>animation_timer</code>) calls the <code>update_animation</code> method every 50ms. This method calculates how much time has passed since the animation started, determines the corresponding progress percentage, and updates the progress bar's value. This creates the smooth visual effect.     <pre><code># from src/gui_manager.py\ndef update_animation(self):\n    if not self.animation_active:\n        return\n\n    elapsed = time.time() - self.animation_start_time\n\n    if self.animation_duration &gt; 0:\n        progress_ratio = min(elapsed / self.animation_duration, 1.0)\n    else:\n        progress_ratio = 1.0\n\n    value_range = self.animation_end_value - self.animation_start_value\n    current_value = self.animation_start_value + int(value_range * progress_ratio)\n\n    self.stage_progress_bar.setValue(current_value)\n</code></pre></p> </li> <li> <p>Termination: Once the actual task is complete in the study process, it sends an <code>end_animation</code> message. This stops the animation timer and sets the progress bar to its final, accurate value, correcting for any deviation between the estimate and the actual time taken.</p> </li> </ol>"},{"location":"old_good_GUI-Profiling-Logger/#3-logging-logging_managerpy","title":"3. Logging (<code>logging_manager.py</code>)","text":"<p>The system uses Python's standard <code>logging</code> module, configured to provide two distinct streams of information.</p>"},{"location":"old_good_GUI-Profiling-Logger/#loggers","title":"Loggers:","text":"<ol> <li><code>progress</code> logger: For high-level, user-facing messages. These are shown in the GUI and saved to <code>*.progress.log</code>.</li> <li><code>verbose</code> logger: For detailed, internal messages. These are saved to the main <code>*.log</code> file.</li> </ol>"},{"location":"old_good_GUI-Profiling-Logger/#implementation-details","title":"Implementation Details:","text":"<ul> <li>Log Rotation: The <code>setup_loggers</code> function checks the number of log files in the <code>logs</code> directory. If it exceeds a limit (10 pairs), it deletes the oldest pair (<code>.log</code> and <code>.progress.log</code>) to prevent the directory from growing indefinitely.</li> <li>Handler Configuration: The function creates file handlers and stream (console) handlers for each logger, ensuring messages go to the right places. <code>propagate = False</code> is used to prevent messages from being handled by parent loggers, avoiding duplicate output.</li> </ul> <pre><code># from src/logging_manager.py\ndef setup_loggers(session_timestamp=None):\n    # ... log rotation logic ...\n\n    progress_logger = logging.getLogger('progress')\n    progress_logger.setLevel(logging.INFO)\n    # Remove existing handlers to prevent duplicates\n    for handler in progress_logger.handlers[:]:\n        progress_logger.removeHandler(handler)\n\n    # File handler for progress file\n    progress_file_handler = logging.FileHandler(progress_log_filename, mode='a')\n    progress_logger.addHandler(progress_file_handler)\n\n    # Stream handler for progress (console output)\n    progress_stream_handler = logging.StreamHandler()\n    progress_logger.addHandler(progress_stream_handler)\n    progress_logger.propagate = False\n\n    # ... similar setup for verbose_logger ...\n    return progress_logger, verbose_logger, session_timestamp\n</code></pre>"},{"location":"old_good_GUI-Profiling-Logger/#4-profiling-and-timing-utilspy-profiling_configjson","title":"4. Profiling and Timing (<code>utils.py</code>, <code>profiling_config.json</code>)","text":"<p>The <code>Profiler</code> class is the engine for the timing and progress estimation system.</p>"},{"location":"old_good_GUI-Profiling-Logger/#key-concepts","title":"Key Concepts:","text":"<ul> <li>Phases and Weights: A study is divided into phases (<code>setup</code>, <code>run</code>, <code>extract</code>). <code>profiling_config.json</code> assigns a \"weight\" to each, representing its contribution to the total time.     <pre><code>// from configs/profiling_config.json\n{\n    \"phase_weights\": {\n        \"setup\": 0.299,\n        \"run\": 0.596,\n        \"extract\": 0.105\n    },\n    \"subtask_estimates\": {\n        \"setup_simulation\": 48.16,\n        \"run_simulation_total\": 107.54,\n        \"extract_sar_statistics\": 4.09\n    }\n}\n</code></pre></li> <li>Dynamic Weights: The profiler normalizes these weights based on which phases are active (controlled by <code>execution_control</code> in the config). If a user chooses to only run the <code>extract</code> phase, its weight becomes 1.0, and the progress for that phase represents 100% of the total work.</li> <li>Weighted Progress: The <code>get_weighted_progress</code> method provides a more accurate overall progress.     <pre><code># from src/utils.py\ndef get_weighted_progress(self, phase_name, phase_progress):\n    \"\"\"Calculates the overall progress based on phase weights.\"\"\"\n    total_progress = 0\n    for phase, weight in self.phase_weights.items():\n        if phase == phase_name:\n            total_progress += weight * phase_progress # Add partial progress of current phase\n        elif phase in self.completed_phases:\n            total_progress += weight # Add full weight of completed phases\n    return total_progress * 100\n</code></pre></li> <li>Time Estimation (ETA): The <code>get_time_remaining</code> method is adaptive. Initially, it relies on the <code>subtask_estimates</code>. Once one or more stages have completed, it switches to a more accurate method based on the actual average time taken per stage.</li> <li>Self-Improving Estimates: After a run, <code>save_estimates</code> calculates the average time for each timed subtask and writes these new averages back to <code>profiling_config.json</code>. This makes future estimates more accurate.</li> </ul>"},{"location":"old_good_GUI-Profiling-Logger/#5-configuration-configpy","title":"5. Configuration (<code>config.py</code>)","text":"<p>The <code>Config</code> class uses a powerful inheritance mechanism to avoid duplicating settings.</p> <ul> <li> <p>Inheritance: A config can \"extend\" a base config. The <code>_load_config_with_inheritance</code> method recursively loads the base config and merges it with the child config. The child's values override the parent's.</p> <p><pre><code># from src/config.py\ndef _load_config_with_inheritance(self, path):\n    config = self._load_json(path)\n\n    if \"extends\" in config:\n        base_config_path = self._resolve_config_path(config[\"extends\"])\n        base_config = self._load_config_with_inheritance(base_config_path)\n\n        # Merge the base configuration into the current one\n        config = deep_merge(base_config, config)\n\n    return config\n</code></pre> For example, <code>near_field_config.json</code> might only specify the settings that differ from the main <code>base_config.json</code>.</p> </li> </ul>"},{"location":"old_good_GUI-Profiling-Logger/#6-project-management","title":"6. Project Management","text":"<ul> <li><code>project_manager.py</code>: This class is critical for reliability. The underlying <code>.smash</code> project files can become corrupted or locked. The <code>_is_valid_smash_file</code> method is a key defensive measure. It first attempts to rename the file to itself (a trick to check for file locks on Windows) and then uses <code>h5py</code> to ensure the file is a valid HDF5 container before attempting to open it in the simulation software. This prevents the application from crashing on a corrupted file.</li> </ul> <p>This integrated system of GUI, logging, profiling, and configuration management provides a robust and user-friendly framework for running complex scientific simulations.</p>"},{"location":"uml/","title":"UML Diagrams","text":"<p>These diagrams are generated from the Python sources under <code>src/</code> using Pyreverse (from Pylint). The generator script is <code>scripts/generate_uml.py</code>.</p> <ul> <li>Class diagram (PlantUML): classes.puml</li> <li>Packages diagram (PlantUML): packages_GOLIAT.puml</li> </ul> <p>How to view - Use any PlantUML viewer (e.g., VS Code PlantUML extension, IntelliJ PlantUML plugin, or https://www.plantuml.com/plantuml). - Alternatively, use a browser-based renderer such as Kroki (https://kroki.io) by uploading the .puml file.</p> <p>How to regenerate - Ensure the Sim4Life Python (with pip) is on PATH as described in the README. - Then run:   - Windows (with Sim4Life Python):      - \"D:\\Program Files\\Sim4Life_8.2.2.18061\\Python\\python.exe\" scripts/generate_uml.py   - Or from a shell session where <code>python</code> resolves to the Sim4Life Python:     - source .bashrc &amp;&amp; python scripts/generate_uml.py</p> <p>Notes - The generated PlantUML files are committed to docs/ so they can be downloaded and rendered externally without requiring additional MkDocs plugins. - If you want inline rendering inside MkDocs, add a PlantUML-capable Markdown extension or serve via Kroki. For example, the mkdocs-kroki plugin can render <code>@startuml ... @enduml</code> fences at build time.</p>"}]}